{"meta":{"title":"ebxeax","subtitle":"","description":"","author":"ebx","url":"http://ebxeax.github.io","root":"/"},"pages":[],"posts":[{"title":"","slug":"中央处理器","date":"2023-08-09T03:00:41.346Z","updated":"2023-08-07T08:49:52.000Z","comments":true,"path":"2023/08/09/中央处理器/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/%E4%B8%AD%E5%A4%AE%E5%A4%84%E7%90%86%E5%99%A8/","excerpt":"","text":"中央处理器控制器：负责协调并控制计算机各部件执行程序的指令序列：取指令、分析指令、执行指令运算器：对数据加工 指令控制 操作控制 数据加工 时间控制 中断处理 基本结构运算器和控制器两大部分组成运算器： 算术逻辑单元（ALU） 暂存寄存器 累加寄存器（ACC） 通用寄存器组（AX&#x2F;BX&#x2F;CX&#x2F;DX&#x2F;SP） 程序状态字寄存器（PSW：OF&#x2F;SF&#x2F;ZF&#x2F;CF） 移位器 计数器 控制器（CU）： 程序计数器（PC） 指令寄存器（IR） 指令译码器 存储器地址寄存器 存储器数据寄存器 时序系统 微操作信号发生器 指令执行过程指令周期CPU从主存取出并执行一条指令的时间，指令周期通常使用若干机器周期表示，每个机器周期可等长或不等，一个机器周期包含若干时钟周期（节拍），每个机器周期内的时钟周期数可以不等 无条件转移指令：执行阶段不需要访存，只包含取指阶段（取指和分析）和执行阶段 间接寻址指令：为了取操作数，需要先访1次，取出有效地址，然后访存，取出操作数，所有需要包含间址周期，介于取指和执行之间 CPU采用中断方式实现主机和I&#x2F;O设备的信息交换，CPU在每条指令结束前，都要发送中断查询信号，若有中断请求，CPU进入中断响应阶段（中断周期） 取指周期 间址周期 执行周期 中断周期 只有访存的目的不同，取指周期是为了取指令，间址周期是为了取有效地址，执行周期是为了取操作数，中断周期是为了保存程序断点CPU内设置了4个标志触发器FE&#x2F;IND&#x2F;EX&#x2F;INT FE IND EX INT Fetch Index Execute Interrupt 取值 间址 执行 中断 指令周期的数据流取指周期根据PC中的内容从主存取出指令代码放入IR PC $\\to$ IR $\\to$ AddrBus $\\to$ Mem CU $\\to$ DataBus $\\to$ MDR $\\to$ Mem Mem $\\to$ DataBus $\\to$ MDR $\\to$ IR（存放指令） CU $\\stackrel{SignalControl::FE}{\\longrightarrow}$ [(PC) + 1 $\\to$ PC] 间址周期取操作数有效地址，间址为例：指令中的地址码送到MAR并送至地址总线，CU向存储器发送读命令，以获取有效地址并存在MDR Addr(IR) &#x2F; MDR $\\to$ MAR $\\to$ AddrBus $\\to$ Mem CU $\\stackrel{SignalI&#x2F;O::READ}{\\longrightarrow}$ ControlBus $\\to$ Mem（存放有效地址） Mem $\\to$ DataBus $\\to$ MDR 执行周期取操作数，根据IR的指令字的操作码通过ALU操作产生执行结果 无统一的数据流向 中断周期处理中断请求，假设程序断点存入堆栈，用SP指示栈顶地址，入栈操作是先修改栈顶指针，后存入数据 CU $\\stackrel{SignalStack::SP}{\\longrightarrow}$ [(SP) - 1 $\\to$ SP] $\\to$ MAR $\\to$ AddrBus $\\to$ Mem CU $\\stackrel{SignalI&#x2F;O::WRITE}{\\longrightarrow}$ ControlBus $\\to$ Mem PC $\\to$ MDR $\\to$ DataBus $\\to$ Mem（程序断点存入主存） CU $\\to$ PC（中断服务程序的入口地址送至PC） 指令执行方案 单指令周期 多指令周期 流水线方案 数据通路功能数据通路：数据在功能部件之间传输的路径由控制部件控制，控制部件根据每条指令功能的不同生成对数据通路的控制信号数据通路功能：实现CPU内部的运算器与寄存器以及寄存器之间的数据交换 基本结构 CPU内部单总线模式所有寄存器的输入输出端都连接在一条公共通路上，结构简单但数据传输存在较多的冲突现象，性能较低。连接各部件的总线只有一条时，称为单总线结构，CPU中有两条或更多的总线时，构成双总线结构或多总线结构 CPU内部多总线模式所有寄存器的输入输出端都连接在多条公共通路上，相比之下单总线在一个时钟内只允许传一个数据，因此指令执行效率很低，因此在多总线方式，同时在多总线上传输不同的数据，提高效率 专用数据通路方式根据指令执行过程中的数据和地址的流动方向安排连接线路，避免使用共享的总线，性能较高，但硬件量大 寄存器之间的数据传输通过内部总线完成寄存器AX的输入输出由AXout和AXin控制 (PC) $\\to$ MAR，PCout和MARin有效 主存与CPU之间的数据传输主存与CPU之间的数据传输需要借助CPU内部总线完成主存内读取 (PC) $\\to$ MAR，PCout和MARin有效 1 $\\to$ R，CU发出读命令 MEM(MAR) $\\to$ MDR，MDRin有效 (MDR) $\\to$ IR，MDRout和IRin有效 执行算术或逻辑算术由于ALU没有内部存储功能，执行加法操作，相加的两个数必须在ALU的两个输入输出端同时有效 (MDR) $\\to$ MAR，MDRout和MARin有效 1 $\\to$ R，CU读命令 MEM(MAR) $\\to$ 操作数从主存送至MDR (MDR) $\\to$ Y，MDRout和Yin有效 (ACC) + (Y) $\\to$ Z ACCout和ALUin有效 (Z) $\\to$ ACC，Zout和ACCin有效 控制器功能与原理结构和功能 运算器部件通过数据总线与内存储器、输入设备和输出设备传送数据 输入设备和输出设备通过接口电路与总线相连接 内存储器、输入设备通过接口电路与总线相连接 内存储器、输入设备和输出设备从地址总线接收地址信息，从控制总线得到控制信号，通过数据总线与其他部件传输数据 控制器部件从数据总线接受指令信息，从运算器部件接收指令转移地址，送出指令地址到地址总线，还要向系统中的部件通过运算所需的控制信号 控制器功能： 从主存取指令，并指出下一条指令在主存中的位置 对指令进行译码或测试，产生相应的操作控制信号，以便启动规定的动作 指挥并控制CPU、主存、输入输出设备之间的数据流动方向 根据控制器产生微操作控制信号的方式不同，控制器可分为 硬布线控制器 微程序控制器 两类控制器的PC和IR是相同的，但确定和表示指令执行步骤的办法以及给出控制部件各部件运算所需控制信号的方案不同 硬布线控制器 根据指令的要求、当前的时序及外部和内部的状态，按时间的顺序发送一系列微操作控制信号 由复杂的组合逻辑门电路和一些触发器构成 硬布线控制单元指令的操作码是决定控制单元发出不同操作命令（控制信号）的关键CU的输入信号来源： 经指令译码器译码产生的信息指令 时序系统产生的机器周期信号和节拍信号 来自执行单元的反馈信号（标志） 系统总线（控制总线）控制信号（中断请求、DMA请求） 硬布线控制器的时序系统及微操作 时钟周期用时钟信号控制节拍发生器，可以产生节拍，每个节拍的宽度对应一个时钟周期，每个节拍内机器可以完成一个或几个需要同时执行的操作 机器周期是所有指令执行过程的一个基准时间，访问一次存储器的时间是固定的，因此通常以存取周期作为基准时间，即内存中读取一个指令字的最短时间作为机器周期，在存储字长等于指令字长前提下，取指周期视为机器周期 指令周期 微操作命令分析控制单元具有发出各种操作命令（控制信号）序列的功能，这些命令与指令有关 执行过程，一条指令分为3个工作周期：取指周期、间址周期、执行周期 取指周期 (PC) $\\to$ MAR 1 $\\to$ R M(MAR) $\\to$ MDR (MDR) $\\to$ IR OP(IR) $\\to$ CU (PC) + 1 $\\to$ PC 间址周期 Addr(IR) $\\to$ MAR 1 $\\to$ R M(MAR) $\\to$ MDR 执行周期 非访存指令 访存指令 CPU控制方式 同步控制方式具有统一的时钟，所有控制信号均来自统一的时钟信号 异步控制方式不存在基准时标信号，各部件按自身固有的速度工作，以应答方式联络 联合控制方式大部分采用同步控制，小部分采用异步控制 微程序控制器采用存储逻辑实现,把微操作信号代码化,使每条机器指令转化为一段微程序并存入一个专门的存储器（控制存储器），微操作控制信号由微指令产生 基本概念 微操作和微命令一条机器指令可以分解为一系列微操作序列，微操作是计算机中最基本、不可再分解的操作；微程序控制的计算机中，将控制部件向执行部件发出的各种控制命令称为微命令，是构成控制系列的最小单位微命令有相容性和互斥性 微指令和微周期微指令是若干微命令的集合，存放微指令的控制存储器的单元地址称为微地址一条微地址包括：（1）操作控制字段（微操作码）：用于产生某一步操作所需的各种操作控制信号（2）顺序控制字段（微地址码）：用于控制产生下一条要执行的微指令地址微周期是执行一条微指令所需的时间，通常为一个时钟周期 主存储器和控制存储器主存储器M，用于存放程序和数据，在CPU外部，用RAM实现控制存储器CM，用于存放微程序，在CPU内部，用ROM实现 程序和微程序程序是指令的有序集合，用于完成某些特定的功能微程序是微指令的有序集合，一条指令的给你由一段微程序实现微程序和程序是两个不同的概念，微程序由微指令组成，描述机器指令，微程序实质是机器指令的实时解释器，由计算机设计者实现编制并存放于控制存储器CM中，无需知道，而程序最终由机器指令组成，由软件设计人员事先编制并存放于主存储器或辅助存储器 MAR 存放主存的读写地址 CMAR 存放控制存储器的读写微指令地址 IR 存放从主存中读出的指令 CMDR&#x2F;$\\mu$IR 存放控制存储器中读出的微指令 组成&amp;过程 控制存储器：存放各指令对应的微程序 微指令寄存器：用于存放从CM中取出的微指令，位数同微指令字长相等 微地址形成部件：产生初始微地址和后继微地址，以保证微指令的连续执行 微地址寄存器：接收微地址形成部件送来的微地址，为在CM中读取微指令作准备 在微程序控制器的控制下计算器执行机器指令的过程： 执行取微指令：自动将取指微程序的入口地址送入CMAR，从CM中读取相应的微指令送入CMDR（取指微程序的入口地址一般为CM的0号单元，当取指微程序执行完成，从主存取出的机器指令就已经存入指令寄存器中） 由机器指令的操作码字段通过微地址形成部件产生该机器指令对应的微程序入口地址，并送入CMAR 从CM中逐条读取对应的微指令并执行 执行完对应于一条机器指令的一个微程序后，又回到取指微程序的入口地址，继续第一步 微程序和机器指令：一条机器指令对应一个微程序，由于机器指令的取指令操作都是相同的，因此可将取指令操作的微命令统一编制为一个微程序，这个微程序只负责将指令从主存单元取出送入指令寄存器，也可编制对应的间址周期和中断周期的微程序控制存储器CM中的微程序个数 &#x3D; 机器指令数+取指+间址+中断 编码方式又称微指令的控制方式，如何对微指令的控制字段进行编码，以形成控制信号 直接编码（直接控制）无需进行译码，微指令的微命令字段中每位代表一个微命令，设计微指令，选用某个微命令只需将微命令对应的字段设置为1或0优：简单直观速度快缺：微指令字长过长，n个微命令要求微指令的操作字段有n位，造成控制存储器容量极大 字段直接编码将微指令的微命令字段分成若干小字段，互斥性微命令组合在同一字段，相容性微命令组合在不同字段，每个字段独立编码，每种编码代表一个微命令且各字段编码含义单独定义，与其他字段无关优:可以缩短微指令字长缺：需要通过编译电路后发出微命令，较直接编译慢 地址形成方式 直接由微指令的下地址字段指出，微指令格式中设置一个下地址字段，由微指令的下地址字段直接指出后继微指令的地址（断定方式） 根据机器指令的操作码形成，机器指令取至指令寄存器后，微指令的地址由操作码经微地址形成部件形成 增量计算器法：(CMAR) + 1 $\\to$ CMAR 根据各种标志决定微指令分支转移地址 通过测试网络形成 由硬件直接产生微程序入口地址 格式 水平型：直接编码、字段直接编码、字段间接编码、混合编码 $A_1$ $A_2$ $\\dots$ $A_{n-1}$ $A_{n}$ 判断测试字段 后继地址字段 操作控制 顺序控制 优：微程序短，执行速快缺：微指令长，编写微程序复杂 垂直型：采用类似机器指令操作码的方式，在微指令中设置微操作码字段，采用微操作码编译法，由微操作码规定微指令的功能 $\\mu$OP Rd Rs 微操作码 目的地址 源地址 优：微指令短、简单、规整，便于编写微程序缺：微程序长，执行速度慢，工作效率低 混合型在垂直型的基础上增加一些不太复杂的并行操作 水平型 垂直型 并行能力 并行能力强、灵活性高、效率高 较差 执行时间 短 长 长度 微指令字较长，微程序较短 微指令字较短，微程序较长 难易程度 难 易 动态微程序设计和毫微程序设计 动态微程序设计：根据用户的要求改变微程序，需要可写控制寄存器，使用EPROM 毫微程序设计：硬件不由微程序直接控制，通过存放在第二级控制存储器中的毫微程序来解释 微程序控制器和硬布线控制器比较 微程序控制器 硬布线控制器 工作原理 微操作控制信号以微程序的形式存放在控制存储器中，执行指令时读出即可 微操作控制信号由组合逻辑电路根据当前的指令码、状态和时序，即时产生 执行速度 慢 快 规整性 较规整 烦琐、不规整 应用场合 CISC CPU RISC CPU 易扩充性 容易 困难 异常（内中断）和中断异常是由CPU内部产生的意外事件，分为硬故障中断和程序性异常硬故障中断：是由硬连线出现异常引起（存储器校验异常、总线错误）程序性异常（软件中断）：CPU内部因执行指令而引起的（整除0、溢出、断点、单步跟踪、非法指令）按照异常发生的原因和返回方式，可进一步分为故障、自陷、终止 故障（Fault）引起故障的指令启动后、执行结束前被检测到的异常，因为无法通过异常处理程序恢复故障，因此不能回到原断点处执行，必须终止进程的执行 自陷（Trap）事先安排的一种异常事件，事先在程序中用一条特殊指令或通过某种方式设定特殊控制标志人为设置一个陷阱，当执行到被设置了陷阱的指令时，CPU在执行陷阱指令后，自动根据不同的陷阱类型进行相应的处理，然后返回到自陷程序下一条指令执行。当自陷指令是转移指令时，并不是返回到下一条指令执行，而不是返回到转移目标指令执行 终止（Abort）如果在执行指令的过程中发生了使计算机无法继续执行的硬件故障，程序将无法继续执行，只能终止，此时调出中断服务程序来重启系统，终止异常和外中断属于硬件中断 中断是来自CPU外部、与CPU执行指令无关的事件引起的中断中断的分类： 可屏蔽中断通过可屏蔽中断请求线INTR向CPU发出中断请求，CPU可通过设置相应的屏蔽字来屏蔽或不屏蔽某个中断 不可屏蔽中断通过专门的不可屏蔽中断请求NMI向CPU发出的中断请求，通常是非常紧急的硬件故障 中断和异常的不同： 缺页或溢出等异常事件是由特定指令在执行过程中产生的，而中断不和任何指令相关联，也不阻止任何指令的完成 异常的检测由CPU完成，不通过外部的某个信号通知CPU，中断CPU必须通过中断请求线获取中断源信息，才能知道哪个设备发生了何种中断 异常和中断响应过程 关中断 保存断点和程序状态 识别异常和中断并转到相应的处理程序（软件识别和硬件识别） 软件识别：CPU设置一个异常状态寄存器，用于记录异常原因，操作系统使用一个统一的异常和中断查询程序，按优先级顺序查询异常状态寄存器，以检查异常和中断类型，先查询到的先处理，然后转到内核中相应的处理程序 硬件识别（向量中断）：异常或中断处理程序的首地址称为中断向量，所有中断向量都存放于中断向量表中，每个异常或中断都被指定一个中断类型号，在中断向量表内，类型号和中断向量一一对应 指令流水线从两方面提高处理机的并行性： 时间并行：流水线技术 空间并行：超标量处理机 指令流水的定义一条指令的执行过程分解为几个阶段，每个阶段由相应的功能部件完成 取指IF 译码&#x2F;读寄存器ID 执行&#x2F;计算地址EX 访存MEM 写回WB","categories":[],"tags":[]},{"title":"","slug":"指令系统","date":"2023-08-09T03:00:41.304Z","updated":"2023-08-07T08:49:52.000Z","comments":true,"path":"2023/08/09/指令系统/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"指令系统指令系统是指令集体系结构ISA的核心ISA主要包括： 指令格式 数据类型及格式 操作数的存放方式 程序可访问的寄存器个数、位数和编号 存储空间大小和编址方式 寻址方式 指令执行过程的控制方式等 指令的基本格式一条指令包括操作码和地址码字段 操作码 地址码 操作码: 指出指令应执行的操作 识别指令 了解指令功能 区分操作数地址内容的组成和使用方法 地址码： 给出被操作的信息的地址 参加运算的一个或多个操作数所在的地址 运算结果的保存地址 程序的转移地址 被调用的子程序的入口地址等 指令长度是指一条指令中包含的二进制代码的位数指令字长取决于 操作码的长度 操作数地址码的长度 操作数地址个数 单字长指令：等于机器字长半字长指令：一半机器字长双字长指令：二倍机器字长定长指令字结构：一个指令系统所有指令的长度都是相等的 零地址指令：无显示地址 OP 不需要操作数的指令 零地址运算指令仅用于堆栈计算机，通常参与运算的两个操作数隐含的从栈顶和次栈顶弹出，送至运算器，运算结果再隐含的压入堆栈 一地址指令： OP $A_1$ OP($A_1$) $\\to$ $A_1$ 只有目的操作数，按$A_1$地址读取操作数，进行OP操作后，结果存回原地址 (ACC)OP($A_1$) $\\to$ ACC 隐含约定目的地址的双操作数指令，按指令地址$A_1$地址读取操作数，指令可隐含约定另一个操作数由ACC提供，运算结果也将存放在ACC中 若指令长度为32位，操作码占8位，1个地址码字段占24位，指令操作数直接寻址范围$2^{24}&#x3D;16M$ 二地址指令 OP $A_1$ $A_2$ ($A_1$)OP($A_2$) $\\to$ $A_1$ 常用的算术和逻辑运算指令，需要两个操作数，需要分别给出目的操作数和源操作数，其中目的操作数地址还用于存放本次运算结果 指令字长位32位，操作码占8位，两个地址码各占12位，则指令操作数的直接寻址范围$2^{12}&#x3D;4K$ 三地址指令 OP $A_1$ $A_2$ $A_3$(结果) ($A_1$)OP($A_2$) $\\to$ $A_3$ 指令字长位32位，操作码占8位，3个地址码各占8位，直接寻址范围$2^8&#x3D;256$，地址字段为主存地址，则完成一条三地址需要4次访存，取指令1次，取两个操作数2次，存放结果1次 四地址指令 OP $A_1$ $A_2$ $A_3$ $A_4$ ($A_1$)OP($A_2$) $\\to$ $A_3$ ，$A_4$ &#x3D; 下一条执行指令的地址 地址字长为32位，操作码占8位，4个地址码各占6位，直接寻址范围$2^6&#x3D;64$ 定长操作码指令格式在指令字的最高位部分分配固定的若干位（定长）表示操作码。n位操作码字段的指令系统最大能表示$2^{n}$条指令 扩展操作码指令格式 不允许短码是长码的前缀 各指令的操作码一定不能重复 0000 0001 0010 0011 0100 0101 0110 0111 1000 1001 1010 1011 1100 1101 1110 1111 操作码情况 OP $A_1$ $A_2$ $A_3$ 说明 15条三地址 0000-1110 余出16-15&#x3D;1，1*2^4&#x3D;16种 12条二地址 1111 0000-1011 余出16-12&#x3D;4，4*2^4&#x3D;64种 62条一地址 1111 (1100-1110)&#x2F;1111 (0000-1111)&#x2F;(0000-1101) 余出64-62&#x3D;2，2*2^4&#x3D;32种 32条零地址 1111 1111 1110-1111 0000-1111 指令的操作类型 数据传送 算术和逻辑运算 移位 转移 输入输出 指令寻址方式确定本条指令的数据地址以及下一条待执行指令的地址，分为： 指令寻址：寻找下条要执行的指令（1）顺序寻址通过PC+(1)，自动形成下一条指令（2）跳跃寻址通过转移指令实现，下条指令的地址不由PC自动给出，而由本条指令给出下条指令地址的计算方式。是否可跳跃受到状态寄存器和操作数的控制，跳跃的结果是当前指令修改PC值，下一条指令仍通过PC给出 数据寻址：寻找本条指令的数据通常在指令字中设一个字段指明寻址方式 操作码 寻址特征 形式地址A 常见数据寻址方式隐含寻址不明显的给出操作数，在指令中隐含操作数地址 优：有利于缩短指令字长 缺：需要增加存储操作数或隐含地址的硬件 立即（数）寻址指令的地址字段指出的不是操作数地址，而是操作数本身，又称立即数，#表示立即寻址特征，使用补码表示 优：指令在执行阶段不访问主存，指令执行时间最短 缺：A的位数限制立即数的范围 直接寻址指令中的形式地址A是操作数的真实地址EA，EA&#x3D;A 优：简单，访存1次，不需要专门计算操作数的地址 缺：A的位数决定了指令操作数的寻址范围，操作数的地址不易修改 间接寻址指令的地址字段给出的形式地址不是操作数的真正地址，而是操作数有效地址的存储单元地址，EA&#x3D;(A)，间接寻址可以迭代多次间接寻址，主存第一位表示是否为多次间址 优：可扩大寻址范围（有效地址EA的位数大于形式地址A的位数），便于编制程序（用间址寻址可方便的完成子程序返回） 缺：访问速度慢 寄存器寻址指令字中直接给出操作数所在的寄存器编号EA&#x3D; $R_i$，操作数在由$R_i$所指的寄存器内 优：指令执行阶段不访存，只访问寄存器，寄存器对应地址码长度较小，使得指令字短且因不用访存，所以执行速度快，支持向量&#x2F;矩阵运算 缺:寄存器昂贵，有限 寄存器间接寻址寄存器$R_i$中给出的不是一个操作数，而是操作数所在主存单元的地址EA&#x3D;($R_i$) 优：与一般间址寻址速度快 缺：需要访存 相对寻址PC的内容加上指令格式的形式地址A而形成操作数的有效地址EA&#x3D;(PC)+A，A是相对于当前PC的值的位移量，可正可负，用补码表示，A的位数决定寻址范围 操作数的地址是不固定的，随PC的值变化而变化，且与指令地址之间相差一个固定值，便于程序浮动，广泛用于转移指令 JMP A，CPU从存储器取出一字节，自动执行(PC)+1 $\\to$ PC，若转移指令的地址为X，且占2B，取出该指令后，PC自增2，(PC)&#x3D;X+2，执行完该指令，会自动跳转至X+2+A的地址继续执行 基址寻址将CPU的基址寄存器BR的内容加上指令格式的形式地址A形成操作数的有效地址EA&#x3D;(BR)+A，基址寄存器可采用专用寄存器也可为通用寄存器 基址寄存器面向操作系统，内容通过操作系统或管理程序确定，主要用于解决程序逻辑空间与存储器物理空间的无关性 执行过程中基址寄存器内容不变，形式地址可变（偏移量） 采用通用寄存器作为基址寄存器，用户可决定使用哪个寄存器，内容由操作系统确定 优：可扩大寻址范围（基址寄存器位数大于形式地址A的位数），用户不必考虑自己的程序存于主存哪个区域，有利于多道程序设计，可用于制成浮动程序 缺：偏移量位数较短 变址寻址有效地址EA等于指令字中的形式地址A与变址寄存器IX的内容之和，EA&#x3D;(IX)+A IX可使用专用寄存器或通用寄存器 变址寄存器面向用户，在程序执行过程，变址寄存器内容可由用户改变（作为偏移量），形式地址A不变（作为基地址） 可扩大寻址范围（变址寄存器位数大于形式地址A的位数），适合编制循环程序，偏移量的位数（IX）足以表示整个存储空间 堆栈寻址堆栈是存储器（或专用寄存器组）中一块特定的、按照后进先出（LIFO）的原则管理的存储区，存储区读写单元地址是用一个特定寄存器给出的称为堆栈指针(SP)，分为硬堆栈（不适合做大容量堆栈）和软堆栈（主存划出一段区域） 寻址方式 有效地址 访存次数 隐含寻址 程序指定 0 立即寻址 A是操作数 0 直接寻址 EA&#x3D;A 1 一次间接寻址 EA&#x3D;(A) 2 寄存器寻址 EA&#x3D; $R_i$ 0 寄存器间接一次寻址 EA &#x3D; ($R_i$) 1 相对寻址 EA&#x3D;(PC)+A 1 基址寻址 EA&#x3D;(BR)+A 1 变址寻址 EA&#x3D;(IX)+A 1","categories":[],"tags":[]},{"title":"","slug":"虚拟存储器·页式·段式·段页式","date":"2023-08-09T03:00:41.269Z","updated":"2023-08-07T08:49:53.000Z","comments":true,"path":"2023/08/09/虚拟存储器·页式·段式·段页式/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/%E8%99%9A%E6%8B%9F%E5%AD%98%E5%82%A8%E5%99%A8%C2%B7%E9%A1%B5%E5%BC%8F%C2%B7%E6%AE%B5%E5%BC%8F%C2%B7%E6%AE%B5%E9%A1%B5%E5%BC%8F/","excerpt":"","text":"虚拟存储器·页式·段式·段页式主存和辅存共同构成了虚拟存储器，在硬件和系统软件的共同管理下工作。对于程序员而言，虚拟存储器是透明的，将主存和辅存的地址空间统一编址用户编程允许涉及的地址称为虚地址或逻辑地址，虚拟地址对应的是存储空间称为虚拟空间实际的主存单元地址称为实地址或物理地址，实地址对应的存储空间称为主存地址空间或实地址空间 实地址 &#x3D; 主存页号 + 页内字地址 虚地址 &#x3D; 虚存页号 + 页内字地址 辅存地址 &#x3D; 磁盘号 + 盘面号 + 磁道号 + 扇区号 CPU使用虚地址访存，由辅助硬件找出虚地址和实地址之间的对应关系，并判断这个虚地址对应的存储单元是否已装入内存 在内存，通过地址变换，CPU可直接访问主存指示的数据单元 不在内存，则把包含这个 字的一页或一段调入主存后，在通过虚实地址变换访问 页式虚拟存储器以页为基本单位，虚拟空间与主存空间划分为同样大小的页，主存的页称为实页、页框，虚存的页称为虚页 页表页表是一张存储在主存中的虚页号和实页号对照表，记录程序的虚页调入主存时被安排在主存中的位置，页表一般长期存在于主存内 1 2 3 4 有效位 脏位 引用位 物理页 有效位（装入位）：表示对应页是否在主存 脏位（修改位）：表示页面是否被修改过，配合回写策略，判断是否需要写回磁盘 引用位（使用位）：配合替换算法进行设置 CPU运行指令时，虚地址转实地址： 页表基址寄存器存放进程的页表首地址 根据虚拟地址高位部分的虚拟页号找到相应的页表项 装入位为1，则取出物理页号和虚拟地址地位进行拼接，形成实际物理地址 装入位为0，说明缺页，需要操作系统进行缺页处理 快表(TLB)高速缓存器制成，依据程序的局部性原理，内存中的页表称之为慢表，地址转换时，先查快表，命中无需访问主存中的页表快表通常采用组相联或全相联方式 t r p 标记 有效位 实页号 具有TLB和Cache的多级存储系统三种缺失的情况 TLB Page Cache 说明 1 命中 命中 命中 TLB命中则Page一定命中，信息在主存，就可能在Cache 2 命中 命中 缺失 TLB命中则Page一定命中，信息在主存，也可能不在Cache 3 缺失 命中 命中 TLB缺失但Page可能命中，信息在主存，就可能在Cache 4 缺失 命中 缺失 TLB缺失但Page一定命中，信息在主存，也可能不在Cache 5 缺失 缺失 缺失 TLB缺失则Page也可能缺失，信息不在主存，也一定不在Cache 段式虚拟存储器按照程序的逻辑结构划分，将逻辑地址分为段号和段内地址，虚实地址转换是由段表来是实现的，段表是程序的逻辑段和在主存中存放位置的对照表 段号 段首地址 装入位 段长 CPU访存时： 根据段号与段表基地址拼接成对应的段表行 根据段表行的装入位判断是否调入内存 调入内存，则从段表读出该段在主存的起始地址，与段内地址相加，得到对应的主存实地址 段页式虚拟存储器根据程序的逻辑结构，将每段划分为固定的大小页，主存空间也划分为大小相等的页，程序对主存调入、调出仍以页为基本传送单位每个程序对应一个段表，每个段表对应一个页表，段的长度必须是页长的整数倍 段号 段内页号 页内地址 CPU访存： 根据段号得到段表地址 从段表中取出该段的页表起始地址，与虚地址段页号合成，得到页表地址 从页表中取出实页号，与页内地址拼接形成实地址 虚拟存储器与Cache的比较 相同 不同 目的均为提供系统性能 Cache主要解决系统速度，虚拟存储器解决主存容量 均把数据划分为小的信息块作为基本传递单位，虚存系统的信息块更大 Cache由硬件实现，对程序员透明，虚拟存储器由OS和硬件实现，是逻辑上的存储器，对系统程序员不透明，对应用程序员透明 都有地址的映射、替换算法、更新策略 均依据程序的局部性原理","categories":[],"tags":[]},{"title":"","slug":"YOLO_001_from-CNN-to-YOLOv1","date":"2023-08-09T03:00:41.226Z","updated":"2023-08-07T08:49:52.000Z","comments":true,"path":"2023/08/09/YOLO_001_from-CNN-to-YOLOv1/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/YOLO_001_from-CNN-to-YOLOv1/","excerpt":"","text":"CNN 分类猫和狗 使用一个还不错的相机采集图片(12M) RGB figure 36M 元素 使用100大小的单隐藏层MLP 模型有3.6B &#x3D; 14GB 元素 远多于世界上所有的猫狗总数(900M dog 600M cat) 两个原则 平移不变性 局部性 重新考察全连接层 将输入和输出变形为矩阵（宽度，高度） 将权重变形为4-D张量（h,w）到（h’,w’）$$h_{i,j}&#x3D;\\sum_{k,l}w_{i,j,k,l}x_{k,l}&#x3D;\\sum_{a,b}&#x3D;v_{i,j,a,b}x_{i+a,j+b}$$V是W的重新索引$$v_{i,j,a,b}&#x3D;w_{i,j,i+a,j+b}$$ 原则#1 - 平移不变性 x的平移导致h的平移$$h_{i,j}&#x3D;\\sum_{a,b}v_{i,j,a,b}x_{i+a,j+b}$$v不应依赖于（i, j） 解决方案：$$v_{i,j,a,b}&#x3D;v_{a, b},h_{i,j}&#x3D;\\sum_{a,b}v_{a,b}x_{i+a,j+b}$$这就是交叉相关 原则#2 - 局部性 局部性$$\\begin{aligned}&amp;为了收集用来训练参数[\\mathbf{H}]{i, j}的相关信息，\\&amp;我们不应偏离到距(i, j)很远的地方。\\&amp;这意味着在|a|&gt; \\Delta或|b| &gt; \\Delta的范围之外，\\&amp;我们可以设置[\\mathbf{V}]{a, b} &#x3D; 0。\\&amp;因此，我们可以将[\\mathbf{H}]{i, j}重写为:\\&amp;[\\mathbf{H}]*{i, j} &#x3D; u + \\sum_*{a &#x3D; -\\Delta}^{\\Delta} \\sum*_{b &#x3D; -\\Delta}^{\\Delta} [\\mathbf{V}]*{a, b} [\\mathbf{X}]{i+a, j+b}.\\end{aligned}$$当图像处理的局部区域很小时，卷积神经网络与多层感知机的训练差异可能是巨大的：以前，多层感知机可能需要数十亿个参数来表示网络中的一层，而现在卷积神经网络通常只需要几百个参数，而且不需要改变输入或隐藏表示的维数。 参数大幅减少的代价是，我们的特征现在是平移不变的，并且当确定每个隐藏活性值时，每一层只包含局部的信息。 以上所有的权重学习都将依赖于归纳偏置。当这种偏置与现实相符时，我们就能得到样本有效的模型，并且这些模型能很好地泛化到未知数据中。 但如果这偏置与现实不符时，比如当图像不满足平移不变时，我们的模型可能难以拟合我们的训练数据。 Sharing-Weight Pooling - Max Pooling Max-Pooling:选取最大的值 也可选取其他的采用 当然也可不做采用前提是性能足够 但CNN无法直接对一个放大的图像做识别，需要data augmentation(对数据集进行旋转，放大，缩小，等操作) YOLOv1Bounding-Box将一张图片分割为有限个单元格(Cell,图中红色网格)每一个输出和标签都是针对每一个单元格的物体中心(midpiont,图中蓝色圆点)每一个单元格会有[X1, Y1, X2, Y2]对应的物体中心会有一个[X, Y, W, H]X, Y 在[0, 1]内表示水平或垂直的距离W, H &gt; 1 表示物体水平或垂直方向上高于该单元格 数值表示水平或垂直方向的单位长度的倍数[0.95, 0.55, 0.5, 1.5]&#x3D;&gt;显然图像靠近右下角 单元格不能表示出完整的物体根据 [X, Y, W, H] &#x3D;&gt; [0.95, 0.55, 0.5, 1.5] 计算得到Bounding Box(图中蓝色网格) Image-Label$$\\begin{aligned}&amp;label_{cell}&#x3D;[C_1,C_2,\\cdots,C_{20},P_c,X,Y,W,H]\\&amp;[C_1,C_2,\\cdots,C_{20}]:20\\space different\\space classes\\&amp;[P_c]:Probability\\space for\\space there\\space is\\space an\\space object(0or1)\\&amp;[X,Y,W,H]:Bounding-Box\\&amp;pred_{cell}&#x3D;[C_1,C_2,\\cdots,C_{20},P_{c1},X_1,Y_1,W_1,H_1,P_{c2},X_2,Y_2,W_2,H_2]\\&amp;Taget\\space shape\\space for\\space one \\space images:(S, S, 25)\\&amp;Predication\\space shape \\space for\\space one\\space images:(S,S,30)\\\\end{aligned}$$ Model-Framework","categories":[],"tags":[]},{"title":"","slug":"PCC_007_第七章-I⁄O","date":"2023-08-09T03:00:41.195Z","updated":"2023-08-07T08:49:51.000Z","comments":true,"path":"2023/08/09/PCC_007_第七章-I⁄O/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_007_%E7%AC%AC%E4%B8%83%E7%AB%A0-I%E2%81%84O/","excerpt":"","text":"I&#x2F;O系统 CPU如何控制键盘I&#x2F;O的完成？$$\\begin{align}&amp;I&#x2F;O控制方式\\begin{cases}&amp;程序查询方式\\&amp;程序中断方式\\&amp;DMA控制方式\\\\end{cases}\\end{align}$$ I&#x2F;O硬件 程序查询方式 程序中断方式 DMA控制方式DMA控制器与主存每次传送1个字，当传送完一整块数据后才向CPU发送中断请求 通道控制方式 通道是具有特殊功能的处理器，能对I&#x2F;O设备进行统一的管理，通道可以识别并执行一系列通道指令，通过指令种类、功能通常比较单一 I&#x2F;O软件 总结 外部设备$$\\begin{align}&amp;外部设备&amp;\\begin{cases}&amp;输入设备\\&amp;输出设备\\&amp;外存设备\\\\end{cases}\\ \\end{align}$$ 输入设备 输出设备 总结 外存储器 一次只能读&#x2F;写1bit数据，且不能同时读写 磁盘存储器磁盘设备的组成存储区域 硬盘存储器 磁盘的性能指标磁盘的容量 记录密度 平均存取时间 数据传输率$$\\begin{align}假设磁盘转数为r(转&#x2F;秒)，每条磁道容量为N个字节，则数据传输率为D_r&#x3D;rN\\end{align}$$ 磁盘地址 磁盘的工作过程 磁盘阵列 RAID0&amp;RAID1 RAID2 RAID3&amp;RAID4&amp;RAID5 RAID通过同时使用多个磁盘，提高传输率；通过多个磁盘上并行存取来大幅度提高存储系统的数据吞吐量；通过镜像功能，可以提高安全可靠性；通过数据校验，可以提高容错能力 光盘存储器 固态硬盘 总结 I&#x2F;O接口 I&#x2F;O接口的作用 数据缓冲寄存器 状态&#x2F;控制寄存器 （1）发命令：发送命令字到I&#x2F;O控制寄存器，向设备发送命令（需要驱动程序的协助） （2）读状态：从状态寄存器读取状态字，获得设备或I&#x2F;O控制器的状态信息 （3）读&#x2F;写数据：从数据缓冲器发送或读取数据，完成主机与外设的数据交换 如何确定要操作的设备？ 每个设备对应一组寄存器，操作不同的寄存器就是在操作不同的设备 I&#x2F;O端口及其编址统一编址 独立编址 I&#x2F;O接口类型 总结 I&#x2F;O方式程序查询方式 例题 $$\\begin{align}&amp;时间角度：\\&amp;一个时钟周期为\\frac{1}{50MHZ}&#x3D;20ns\\&amp;一个查询操作时间10020ns&#x3D;2000ns\\&amp;(1)鼠标\\&amp;每次查询鼠标耗时302000ns&#x3D;60000ns\\&amp;查询鼠标所花费的时间比率&#x3D;\\frac{60000ns}{1s}&#x3D;0.006%\\&amp;(2)硬盘\\&amp;每32位需要查询一次，每秒传送22^{20}B\\&amp;每秒需要查询\\frac{22^{20}B}{4B}&#x3D;2^{19}次\\&amp;查询硬盘耗时2^{19}2000ns&#x3D;1.0510^9ns\\&amp;查询硬盘所花费的时间比率\\frac{1.05*10^9ns}{1s}&#x3D;105%\\&amp;结论：CPU将全部时间都用于硬盘查询也不能满足磁盘传输的要求\\end{align}$$ 程序中断方式 中断 中断方式 中断请求标记 中断判优硬件排队器&amp;查询程序 优先级设置 处理过程 中断隐指令（一系列的指令） 硬件向量法（二级指针【向量地址—–入口地址】） 中断服务程序 总结：中断处理过程单重中断 多重中断（中断嵌套）单重中断&amp;多重中断 中断屏蔽技术 例题 总结 程序中断方式 例题 $$\\begin{align}&amp;（1）数据位7位，1位校验位，1位起始位，1位终止位，共需传送10位\\&amp;每秒钟可送\\frac{1}{0.5ms}&#x3D;2000个字符\\&amp;（2）主频50MHZ,时钟周期为\\frac{1}{50MHZ}&#x3D;20ns\\&amp;0.5ms对应的周期数为\\frac{0.5ms}{20ns}&#x3D;25000\\&amp;传送1个字符需要的时钟周期数为25000+10+154&#x3D;25070\\&amp;传送1000个字符需要的时钟周期为250701000&#x3D;25070000\\end{align}$$$$\\begin{align}&amp;(3)CPU用于该任务的时间大约为1000*(10+204)&#x3D;910^4个时钟周期\\&amp;(4)中断隐指令：\\begin{cases}&amp;1.关中断\\&amp;2.保存断点(PC)\\&amp;3.引出中断服务程序\\\\end{cases}\\\\end{align}$$ 总结 DMA方式 DMA控制器 DMA传送过程 DMA方式的特点 DMA传送方式停止CPU访问主存 DMA与CPU交替访存 周期挪用（周期窃取） DMA方式&amp;中断方式 总结","categories":[],"tags":[]},{"title":"","slug":"PCC_006_第六章-总线","date":"2023-08-09T03:00:41.152Z","updated":"2023-08-07T08:49:51.000Z","comments":true,"path":"2023/08/09/PCC_006_第六章-总线/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_006_%E7%AC%AC%E5%85%AD%E7%AB%A0-%E6%80%BB%E7%BA%BF/","excerpt":"","text":"第六章 总线 总线概念总线是一组能为多个部件分时 共享的公共信息传送线路 共享 分时 总线的特性 总线的分类 串行总线&amp;并行总线 片内总线&amp;系统总线&amp;通信总线片内总线 系统总线 通信总线 系统总线结构单总线 双总线 三总线 【扩充】四总线 总结 总线的性能指标传输周期（总线周期） 时钟周期 工作频率 时钟频率 总线宽度 总线带宽 例题： 解答 总线复用 信号线数 总结 总线仲裁$$\\begin{align}总线仲裁分类\\begin{cases}&amp;集中仲裁方式\\begin{cases}&amp;链式查询方式\\&amp;计数器定时查询方式\\&amp;独立请求方式\\\\end{cases}\\&amp;分布仲裁方式\\\\end{cases}\\end{align}$$ 集中仲裁方式 链式查询方式 计数器查询方式 独立请求方式 小结 分布仲裁方式 总线的操作和定时 同步定时方式 异步定时方式 半同步通信 分离式通信 总结 总线标准并行传输 串行传输 支持热插拔 设备总线标准 为何串行总线取代并行总线 总结","categories":[],"tags":[]},{"title":"","slug":"PCC_005_第五章-中央处理器CPU","date":"2023-08-09T03:00:41.105Z","updated":"2023-08-07T08:49:51.000Z","comments":true,"path":"2023/08/09/PCC_005_第五章-中央处理器CPU/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_005_%E7%AC%AC%E4%BA%94%E7%AB%A0-%E4%B8%AD%E5%A4%AE%E5%A4%84%E7%90%86%E5%99%A8CPU/","excerpt":"","text":"第五章 中央处理器CPU CPU的功能和结构CPU的功能1.指令控制 2.操作控制 3.时间控制 4.数据加工 5.中断处理 运算器和控制器的功能 运算器的基本结构1.算数逻辑单元 进行算术&#x2F;逻辑运算 2.通用寄存器组 存放操作数（包括源操作数、目的操作数及中间结果和各地址信息等）如AX,BX,CX,DX,SP等 连线方式专用数据通路方式：根据指令执行过程中的数据和地址的流动方向安排连接线路如果直接用导线连接，相当于多个寄存器同时并且一直向ALU传输数据 方法1：使用多路选择器 方法2：使用三态门 可以控制每一路是否输出 $$\\begin{align}&amp;R0_{out}&#x3D;1时R_0中的数据输出到A端\\&amp;R0_{out}&#x3D;0时R_0中的数据无法输出到B端\\\\end{align}$$ 3.暂存寄存器 暂存从主存读来的数据，这个数据不能存放在通用寄存器中，否则会破坏其原有内容 如果两个操作数 CPU内部单总线方式：所有寄存器的输入端和输出端都连接在一条公共的通路上 $$\\begin{align}&amp;暂存寄存器解决需要多个数据时造成的总线冲突，如需要R_0+R_1时会造成总线冲突，先把R_0传输到数据总线，R_1放置于暂存寄存器，等待R_0传输结束,在将R_1从暂存寄存器中传至数据总线完成一次加法\\\\end{align}$$ 4.累加寄存器 5.程序状态字寄存器 6.移位器 对运算结果进行移位运算 7.计数器 控制乘除运算的操作步数 控制器的基本结构 1.程序计数器 2.指令寄存器 用于保存当前正在执行的那条指令 3.指令译码器 仅对操作码字段进行译码，向控制器提供特定的操作信号 4.存储器地址寄存器（MAR） 用于存放要访问的主存单元地址 5.存储器数据寄存器（MDR） 用于存放向主存写入的的信息或从主存读出的信息 6.时序系统 用于产生各种时序信号，它们都是由统一时钟（CLOCK）分频得到 7.微操作信号发生器 根据IR的内容（指令），PSW的内容（状态信息）及时序信号，产生控制整个计算机系统所需的各种控制信号，其结构有组合逻辑型和存储逻辑性 CPU的结构用户可见的寄存器（可编程，即程序员可以通过汇编语言操作的寄存器） 总结 指令执行过程指令周期：CPU从主存中每取出并执行一条指令所需的全部时间 指令周期常常用于若干机器周期来表示，机器周期又叫CPU周期 一个机器周期又包括若干个时钟周期（也称节拍，T周期或CPU时钟周期，是CPU操作的最基本单位） 指令周期流程 取指周期 间址周期 执行周期 中断周期 指令执行方案单指令周期 多指令周期 流水线周期 总结 数据通路的功能数据在功能部件之间传送的路径$$\\begin{align}&amp;数据通路基本结构\\begin{cases}1.CPU内部单总线方式\\2.CPU内部多总线方式\\3.专用数据通路方式\\\\end{cases}\\ \\end{align}$$ CPU内部总线方式 寄存器之间数据传输 主存与CPU之间的数据传送 执行算术或逻辑运算 例题 1.分析指令功能和指令周期 2.写出各阶段的指令流程 取指周期 时序 微操作 有效控制信号 1 (PC)-&gt;MAR PCout,MARin 2 M(MAR)-&gt;MDR,(PC)+1-&gt;PC MemR,MARout,MDRinE 3 (MDR)-&gt;IR MDRout,IRin 4 指令译码 - 间址周期 完成取数操作，被加数在主存中，加数已经放在寄存器R1中 时序 微操作 有效控制信号 1 (R0)-&gt;MAR R0out,MARin 2 M(MAR)-&gt;MDR MemR,MARout,MDRinE 3 (MDR)-&gt;Y MDRout,Yin 执行周期 完成取数操作，被加数在主存中，加数已经存放在寄存器R1中 时序 微操作 有效控制信号 1 (R1)+(Y)-&gt;Z R1out,ALUin,CU向ALU发ADD控制信号 2 (Z)-&gt;MDR Zout,MDRin 3 (MDR)-&gt;M(MAR) MenW,MDRoutE,MARout 总结 专用数据通路 取指周期$$\\begin{align}&amp;(PC)\\rightarrow MAR, C_0有效\\&amp;(MAR)\\rightarrow主存,C_1有效\\&amp;M(MAR)\\rightarrow MDR,C_2有效\\&amp;(MDR)\\rightarrow IR,C_3有效\\&amp;(PC)+1\\rightarrow PC\\&amp;Op(IR)\\rightarrow CU,C_4有效\\\\end{align}$$ 例题 $$\\begin{align}&amp;(1)a:MDR,b:IR,c:MAR,d:PC\\&amp;(2)(PC)\\rightarrow MAR\\&amp;M(MAR)\\rightarrow MDR\\&amp;(MDR)\\rightarrow IR\\&amp;OP(IR)\\rightarrow 微操作发生器\\&amp;(PC)+1\\rightarrow PC\\&amp;(3)取：\\&amp;M(MAR)\\rightarrow MDR\\&amp;(MDR)\\rightarrow ALU\\rightarrow ACC\\&amp;存：\\&amp;(ACC)\\rightarrow MDR\\&amp;(MDR)\\rightarrow M(MAR)\\&amp;(4)X\\rightarrow MAR\\&amp;M(MAR)\\rightarrow MDR\\&amp;(MDR)\\rightarrow ALU\\rightarrow ACC\\&amp;(5)Y\\rightarrow MAR\\&amp;(MAR)\\rightarrow ALU,(ACC)\\rightarrow ALU\\&amp;ALU\\rightarrow ACC\\&amp;(6)Z\\rightarrow MAR\\&amp;(ACC)\\rightarrow MDR\\&amp;(MDR)\\rightarrow M(MAR)\\\\end{align}$$ 总结 控制器设计 硬布线控制器 $$\\begin{align}&amp;CU发出一个微命令，可完成对应的微操作\\&amp;如：微命令1使得PC_{out},MAR_{in}有效,完成对应的微操作1(PC)\\rightarrow MAR\\\\end{align}$$ 根据指令操作码，目前的机器周期，节拍信号，机器状态条件，即可确定现在这个节拍下应该发出哪些“微命令” 硬布线控制器的设计 微操作序列取指周期微操作序列 间址周期微操作序列 执行周期微操作序列 安排微操作时序的原则 安排微操作时序 取指周期 安排微操作时序 间址周期 安排微操作时序 执行周期非访存指令 访存指令 转移指令 组合逻辑设计 列出操作时间表取值周期 间址周期 执行周期 微操作信号综合 画出逻辑图 硬布线控制器的设计 微程序控制器设计思路采用“存储程序”的思想，CPU出厂前将所有指令的“微程序”存入“控制器存储器”中 程序由机器指令序列组成 微程序由微指令序列组成，每一种指令对应应该微程序 机器指令是程序执行步骤的描述 微指令是对指令执行步骤的描述 微命令与微操作微命令与微操作一一对应 微指令中可能包含多个微命令 微程序与机器指令微程序与机器指令一一对应 一个微程序由多个微指令序列构成 机器指令是对微指令功能的“封装” 微程序控制器的基本结构 （1）由IR将机器指令操作码传入微地址形成部件 （2）微地址形成部件根据传入的操作码确定这条机器指令对应的微指令序列的起始地址 （3）根据顺序逻辑的标志等确定接下来执行微指令的存放地址 （4）将微指令的存放地址放入CMAR中 （5）由地址译码将地址信息转为控制信号传入控制存储器CM （6）控制存储器CM确定地址信息所指向的微指令（一条微指令包含两部分信息，【本条微指令的控制信号|接下来要执行的微指令的地址（下地址）】） （7）将选中的微指令放入CMDR （8）执行完本条微指令（硬件电路需要通过控制码向CPU的其他部件或向系统总线发送控制信号） （9）将下地址信息传递至顺序逻辑 重复（3）~（9） 所有机器指令的取指周期、间址周期、中断周期所对应的微指令序列都一样，是否可以共享使用？ 微程序控制器的工作原理 如果某指令系统中有n条机器指令，则CM中的微程序（段）的个数至少是n+1个 总结 微指令的设计微指令的格式相容性微命令：可以并行完成 互斥性微命令：不允许并行完成的微命令 水平型微指令 垂直型微指令 混合型微指令 微指令的编码方式 直接编码（直接控制）方式 字段直接编码方式 例题： 解答： 字段间接编码方式（隐式编码） 微指令的地址形成方式微指令的下地址字段指出 根据机器指令的操作码形成 增量计数器法 分支转移 通过网络测试网络由硬件产生微程序人口地址 例题-断定方式 解答 总结 微程序控制单元的设计设计步骤 取指周期的第一条微指令地址由硬件自动给出（指令a） 用微指令a的下地址表示b的地址 还需考虑如何读出这3条微指令，以及如何转入下一个机器周期$$\\begin{align}&amp;Ad(CMDR)\\rightarrow CMAR\\space\\space用当前微指令的下地址表示找到下一条微指令\\&amp;OP(IR)\\rightarrow 微地址形成部件\\rightarrow CMAR\\\\end{align}$$ 微程序设计的分类 硬布线控制器&amp;微程序控制器比较 总结 指令流水 指令流水的方式顺序执行方式 一次重叠执行方式 二次重叠执行方式 流水线的表示方式指令执行过程图 时空图 流水线的性能指标吞吐率 装入时间：第一条时间从开始到结束的时间 排空时间：最后一条指令从开始到结束的时间 加速比 效率 总结 指令流水线影响因素分类机器周期的设置Cache一般会被分为两部分，Instruction Cache和Data Cache 通常Cache中会保存主存的副本 因此取指令和访存阶段会先在Cache中寻找 没有找到再去访问主存 Imm为立即数寄存器 影响流水线的因素结构相关（资源冲突） 类比操作系统互斥 数据相关（数据冲突） 解决方法1：等待 （1）硬件阻塞 （2）软件插入 解决方案2：数据旁路技术（转发机制） 解决方案3：编译优化 控制相关（控制冲突） 小总结 流水线的分类 流水线的多发技术 总结 五段式指令流水线$$\\begin{align}&amp;五段式指令\\begin{cases}&amp;取指Fetch(IF)\\&amp;译码Decode(ID)\\&amp;执行Execute(EX)\\&amp;访存Memory(M)\\&amp;写回Writeback(WB)\\\\end{cases}\\\\end{align}$$ 运算类指令的执行过程 LOAD指令的执行过程 其他指令都是从寄存器中获得立即数进行运算 STORE指令的执行过程 条件转移指令的执行过程 $$\\begin{align}&amp;（PC）+指令字长+（偏移量指令字长）\\rightarrow PC\\&amp;指令字长&#x3D;4B\\&amp;(PC)+4+(9964)\\rightarrow PC\\\\end{align}$$ 无条件转移指令的执行过程 例题 解答 （1）I3与I1和I2存在数据相关 （2）I4的IF段必须在I3进入ID段后才能开始，否则会覆盖IF段锁存器的内容","categories":[],"tags":[]},{"title":"","slug":"PCC_004_第四章-指令系统","date":"2023-08-09T03:00:41.060Z","updated":"2023-08-07T08:49:50.000Z","comments":true,"path":"2023/08/09/PCC_004_第四章-指令系统/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_004_%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"第四章 指令系统操作码：指明了“做什么” 地址码：指明了“对谁动手” 有的指令不需要地址码（停机） 指令格式 零地址指令 一地址指令 二地址指令四次访存 三地址指令四次访存 四地址指令$$\\begin{align}&amp;n位地址码的直接寻址范围&#x3D;2^n\\&amp;若指令总长度固定不变,则地址码数量越多,寻址能力越弱\\end{align}$$ 指令按指令长度分类 定长操作码指令格式 可变长操作码指令格式 指令按操作类型分类 总结 扩展操作码指令格式在设计扩展操作码指令格式时需注意： （1）不允许短码是长码的前缀，即短操作码不能与长操作码的前面部分的代码相同 三地址指令前缀不能为1111 （2）各指令的操作码一定不能重复 扩展操作码示例$$\\begin{align}&amp;设地址长度为n，上一层留出m种状态，下一层可扩展出m*2^n种状态\\\\end{align}$$ 优缺点 指令寻址下一条欲执行指令的地址（始终由程序计数器PC给出） 顺序寻址定长指令寻址按字编址 按字节编址 变长指令寻址 跳跃寻址由转移指令指出（JMP） 总结 数据寻址 增加寻址特征 直接寻址指令字中的形式地址A就是操作数的真实地址EA,即EA&#x3D;A 一次指令的执行访存2次： （1）取指令访存1次 （2）执行指令访存1次 间接寻址指令的地址字段给出的形式地址不是操作数的真正地址，而是操作数有效地址所在的存储单元的地址，也就是操作数地址的地址，EA&#x3D;(A) 寄存器寻址$$\\begin{align}&amp;在指令字中直接给出操作数所在的寄存器编号，即EA&#x3D;R_i，其操作数在由R_i所指的寄存器内\\\\end{align}$$ 寄存器间接寻址$$\\begin{align}&amp;寄存器R_i中给出的不是一个操作数,而是操作数所在主存单元的地址，即EA&#x3D;(R_i)\\\\end{align}$$ 隐含地址不是显示的给出操作数的地址，而是在指令中隐含着操作数的地址 立即寻址形式地址A就是操作数本身，又称立即数，一般采用补码形式 **#**表示立即寻址的特征 总结 偏移寻址$$\\begin{align}&amp;偏移寻址&#x3D;\\begin{cases}&amp;基址寻址,EA&#x3D;(BR)+A\\&amp;变址寻址,EA&#x3D;(IX)+A\\&amp;相对寻址,EA&#x3D;(PC)+A\\\\end{cases}\\&amp;区别在于偏移的“起点”不一样\\&amp;BR-base\\space address\\space register\\&amp;EA-effective\\space address\\\\end{align}$$ 基址寻址将CPU中基址寄存器（BR）的内容加上指令格式中的形式地址A,而形成操作数的有效地址，即EA&#x3D;(BR)+A 根据通用寄存器总数判断要用几个bit指明寄存器 基址寻址的作用 当程序从0地址开始存储，可以使用直接寻址 但通常程序在内存非0地址起始，直接寻址就无法奏效 可以采用基址寻址 采用基址寻址无需修改指令中的地址 变址寻址 基址寻址中，BR保持不变作为基地址，A作为偏移量 变址寻址的作用 当实现循环时采用直接寻址需使用十条加法指令 使用变址寻址更为合适 基址寻址变址寻址复合寻址 IX-Index Register 先使用基址寻址将指令位置定位到100的基地址 再使用变址寄存器实现循环的偏移量增加 相对寻址 相对寻址的作用 总结 硬件如何实现数的“比较”（扩展） 无条件转移指令JMP2不会管PSW的各种标志位，直接跳转到2 PSW也被称为标志寄存器 堆栈寻址 硬堆栈与软堆栈 总结 CISC&amp;RISCCISC RISC 对比","categories":[],"tags":[]},{"title":"","slug":"PCC_003_第三章-存储系统","date":"2023-08-09T03:00:41.016Z","updated":"2023-08-07T08:49:50.000Z","comments":true,"path":"2023/08/09/PCC_003_第三章-存储系统/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_003_%E7%AC%AC%E4%B8%89%E7%AB%A0-%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"第三章 存储系统存储器层次化结构多级存储系统 主存——辅存：实现虚拟存储系统，解决了贮存容量不够的问题 Cache——主存：解决了主存与CPU速度不匹配问题 存储器的分类存储介质分类半导体存储器 磁表面存储器 光存储器 存取方式相联存储器(Associative Memory) 如：”快表”，按照内容检索到存储位置进行读写 CAM(Content Addressed Memory)存储器 RAM(Random Access Memory)随机存取存储器 SAM(Sequential Access Memory)顺序存取存储器 DAM(Direct Access Memory)直接存取存储器 信息的可改性读写存储器(Read&#x2F;Write Memory)可读可写（磁盘、内存、Cache） 只写存储器(Read Only Memory)可读不可写（CD-ROM，BIOS通常写在ROM） 信息的可保存性 存储器的性能指标$$\\begin{align}&amp;1.存储容量&#x3D;存储字数字长（1M8位）\\&amp;2.单位成本：每位价格&#x3D;\\frac{总成本}{总容量}\\&amp;3.存储速度：数据传输率&#x3D;\\frac{数据的宽度}{存储周期}&#x3D;\\frac{存储字长}{存储周期}\\&amp;主存带宽:又称数据传输率，表示每秒从主存进出信息的最大数量，单位为字&#x2F;秒，字节&#x2F;秒(B&#x2F;s)，位&#x2F;秒(b&#x2F;s)\\\\end{align}$$ 主存储体的基本组成 一行存储元就是一个存储字，存储字长：8bit 计算引脚数量（数据线引脚+地址线引脚+读&#x2F;写控制线引脚+片选线引脚+等等） 可以通过描述如$$\\begin{align}&amp;64K*16位\\&amp;数据线:16根\\&amp;地址线:16根\\\\end{align}$$ 将按字寻址转换为对应的按字节寻址：将字地址算术左移两位后转为相应的十进制 字地址：1 算数左移2位后：100 转为十进制：100 ——&gt; 4 SRAM和DRAM存储器特性对比 3.2 DRAM的刷新（由存储器独立完成，不需要CPU控制） 左侧为单个的行地址译码 直接将地址送到译码器$$\\begin{align}&amp;2^n根选通线\\\\end{align}$$ 0000 0000 选择0号存储单元 右侧为行列译码 高位地址送到行地址译码器 低位地址送到列地址译码器$$\\begin{align}&amp;存储单元排成n*n的矩阵,2^{\\frac{n}{2}}+2^{\\frac{n}{2}}根选通线\\\\end{align}$$ 0000 0000 选中0,0号存储单元 地址线复用 ROMROM的类型 计算机内的重要ROM内存&#x3D;内存条+BIOS的芯片 RAM和ROM统一编制 主存储器与CPU的连接 数据总线（宽度&#x3D;存储字长） 位扩展（解决：数据总线宽度&gt;存储芯片字长） 带横线表示低电平有效，不带横线表示高电平有效 CPU连接单个主存 CPU连接两块主存 1号主存和2号主存连接相同的地址线，接收相同的地址，CPU可以同时对两块主存的同一地址进行读写 CPU连接多块主存 字扩展（增加主存的存储字数）线选法 高地址11和00时产生总线冲突 高地址为11和00时不能使用 译码器片选法 字位同时扩展法 译码器 双端口RAM和多模块存储器（优化多核CPU访问一根内存条的速度）双端口RAM 问题： 优化多喝CPU访问一根内存条的速度 对比操作系统“读者”“写者”问题 可以同时读不可同时写 多体并行存储器 低位交叉编制可以降低存取时间 $$\\begin{align}&amp;m&#x3D;\\frac{T}{r}满足\\\\end{align}$$例题： （1）计算连续取n个存储字的时间？ （2）给定一个地址x，如何确定它属于第几个存储体？ 例如：01101在哪个存储体？ 法1：直接通过低位体号求得$$\\begin{align}&amp;01\\Rightarrow M_1 \\\\end{align}$$ 法2：对地址的十进制数求余$$\\begin{align}&amp;(01101)2&#x3D;(13){10},13\\pmod{4}&#x3D;1\\Rightarrow M_1 \\\\end{align}$$ 多模块存储器 一次读一行 适用于连续读写 高速缓冲存储器CacheCache工作原理程序的局部性原理 性能分析 例题 $$\\begin{align}&amp;设Cache的访存周期为t,主存的存取周期为5t\\&amp;(1)Cache和主存同时访问:\\&amp;命中时访问时间为t,未命中访问时间为5t\\&amp;\\bar t&#x3D;0.95t+5t(1-0.95)&#x3D;1.2t\\&amp;\\eta&#x3D;\\frac{5t}{\\bar t}\\sim4.17\\&amp;(2)先访问Cache再访问主存:\\&amp;命中时访问时间为t,未命中访问时间为t+5t\\&amp;\\bar t&#x3D;0.95t+6t(1-0.95)&#x3D;1.25t\\&amp;\\eta&#x3D;\\frac{5t}{1.25t}&#x3D;4\\\\end{align}$$ 剩余问题 Cache-主存映射方式三种映射添加标记为和有效位 全相联映射 直接映射 当两块冲突时无法解决 对标记位进行优化：$$\\begin{align}&amp;主存块号%2^3\\Leftrightarrow 留下最后三位二进制数\\&amp;即：Cache总块数&#x3D;2^n,则主存块号末尾n位反映它在Cache中的位置\\&amp;将主存块号的其余位作为标记\\\\end{align}$$ CPU访存： 组相联映射 CPU访存： 总结 Cache替换算法（解决Cache被装满的问题）Cache完全装满了才需要替换，需要在全局选择替换哪一块 如果对应位置非空，直接替换 分组内满了才需要替换需要在分组内选择替换哪一块 随机算法(RAND)随机选择一块替换 算法性能：实现简单，但完全没有考虑局部性原理，命中率低，实际效果很不稳定 先进先出算法(FIFO)替换最先调入Cache的块 实现简单，最开始#0#1#2#3放入Cache，之后轮流替换#0#1#2#3 算法性能：没有考虑局部性原理，最先调入Cache的块也可能是被频繁访问的 抖动现象：频繁的换入换出现象（刚被替换掉块很快又被调入） 近期最少使用算法(LRU)为每一个Cache块设置一个计数器，用于记录每个Cache块已经有多久没有被访问了，当Cache满后替换计数器最大的$$\\begin{align}&amp;Cache块的总数&#x3D;2^n,则计数器只需要n位,且Cache装满后所有计数器的值一定不重复\\\\end{align}$$（1）命中时，所命中的行的计数器清零，比其低的计数器加1，其余不变 （2）未命中且还有空闲行时，新装入的行的计数器置0，其余非空闲行全加1 （3）未命中且无空闲行时，计数值最大的行的信息块被淘汰，新装行的块的计数器置0，其余全加1 算法性能：基于局部性原理，近期被访问过的主存块，在不久的将来也很可能再次被访问，因此淘汰最久没访问过的块是合理的，实际运行效果优秀，Cache命中率高 最不经常使用算法(LFU)为每一个Cache块设置一个计数器，用于记录每个Cache块被访问过几次，当Cache满后替换计数器最小的 算法性能：曾经被经常访问的主存块在未来不一定会用到，算法实际运行效果不让LRU 总结 Cache的写策略（如何保持Cache里的数据和内存里的数据保持一致） 写命中写回法 全写法 写不命中写分配法 非写分配法 多级Cache 总结 虚拟存储器页式虚拟存储器原理 虚地址与实地址Cache-主存分块是面向物理地址 程序的分页是面向逻辑地址 页表（记录逻辑页号与主存块号的关系） 查询过程: 快表(TLB)快表是一种相联存储器，可以按内容访存 将近期访问的页表项放入更高速的存储器中可加快地址转换的速度 快表和Cache的区别快表中储存的是页表项的副本 Cache中储存的是主存块的副本 访问策略访问快表根据页号查找 命中根据快表页表项得到主存块号 去Cache中寻找相应主存的副本 命中访存","categories":[],"tags":[]},{"title":"","slug":"PCC_002_data_expression_and_calculate","date":"2023-08-09T03:00:40.944Z","updated":"2023-08-07T08:49:49.000Z","comments":true,"path":"2023/08/09/PCC_002_data_expression_and_calculate/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_002_data_expression_and_calculate/","excerpt":"","text":"数据表示和运算进制 r进制 $K_{n} K_{n-1} K_{n-2} \\dots K_{0} K_{-1} \\dots K_{-m}$数值表示 $K_{n} r^{n} + K_{n-1} r^{n-1} + \\dots + K_{0} r^{0} + K_{-1} r^{-1} + \\dots + K_{-m} r^{-m} &#x3D; \\sum_{i&#x3D;n}^{-m} K_{i} r^{i}$ 二进制$01 \\space r&#x3D;2$ 八进制$01234567 \\space r&#x3D;8&#x3D;2^3$ 十六进制$0123456789ABCDEF \\space r&#x3D;16&#x3D;2^4$ 进制转换4位二进制数码与1位十六进制数码相对应3位二进制数码与1位八进制数码相对应 原反补移 原码 反码 补码 移码","categories":[],"tags":[]},{"title":"","slug":"PCC_001_第一章-计算机系统概述","date":"2023-08-09T03:00:40.915Z","updated":"2023-08-07T08:49:49.000Z","comments":true,"path":"2023/08/09/PCC_001_第一章-计算机系统概述/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/PCC_001_%E7%AC%AC%E4%B8%80%E7%AB%A0-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0/","excerpt":"","text":"第一章 计算机系统概述发展历程 计算机 时代 第一代计算机 电子管时代 第二代计算机 晶体管时代 第三代计算机 中小规模集成电路时代 第四代计算机 超大规模集成电路时代 产生了微处理器 计算机硬件的基本组成早期冯诺依曼机（1）采用存储程序的概念 （2）由运算器、存储器、控制器、输入设备、输出设备构成 （3）指令和数据以同等地位存储在存储器中，并可按地址寻访 （4）指令和数据均用二进制代码表示 （5）指令由操作码和地址码组成，操作码指出操作的类型，地址码指出操作数的地址 （6）指令在存储器内按顺序存放 （7）早期冯诺依曼机以运算器为中心，输入输出设备通过运算器与存储器传送数据 存储程序：将指令以代码的形式事先输入到计算机的主存储器中，然后按其在存储器中的首地址执行程序的第一条指令，按该程序规定顺序执行其他指令，直至结束 在计算机系统中，软件和硬件在逻辑上是等效的 计算机功能部件输入设备输出设备存储器工作方式：按存储单元的地址进行存取（按地址存取方式，相联存储器是按内容访问） 主存储器由很多存储单元组成，每个存储单元包含若干个存储元件，每个存储单元存储一位二进制代码“0”或“1”；存储单元可存储一串二进制代码，这串代码位存储字，这串代码的位数为存储字长 主存储器由存储体、地址寄存器（MAR）、数据寄存器（MDR）、时序控制逻辑组成$$\\begin{align}&amp;MAR&#x3D;4位\\rightarrow 具有2^4个存储单元\\&amp;MDR&#x3D;16位\\rightarrow 每个存储单元可存放16bit\\&amp;1个字（Word）&#x3D;16bit[根据计算机硬件64、32、16、8bit]\\&amp;1个字节（Byte）&#x3D;8bit\\&amp;1B&#x3D;1个字节&#x3D;8bit\\&amp;1b&#x3D;1bit\\\\end{align}$$存储元：存储二进制的电子元件，每个存储元可存储1 bit（电容） 运算器运算器核心为算数逻辑单元(ALU)，运算器包含若干通用寄存器，用于暂存操作数和中间结果，如累加器（ACC）、乘商寄存器（MQ）、操作数寄存器（X）、变址寄存器（IX）、基址寄存器（BR）、程序状态寄存器（PSW）等 控制器控制器由程序计数器（PC）、指令寄存器（IR）、控制单元（CU）组成 一般将运算器和控制器集成在同一个芯片上成为CPU 计算机系统的多级层次结构 计算机性能指标机器字长计算机进行一次整数运算（定点整数运算）所能处理的二进制数据的位数 存储字长一个存储单元中的二进制代码的位数，等于MDR的位数 数据字长数据总线一次可以传输的位数 数据通路带宽数据总线一次所能并行传送的位数 主存容量主存储器所能存储信息的最大容量$$\\begin{align}&amp;MAR为16位，表示2^{16}&#x3D;65536，即此存储体内有65536个存储单元(64K)\\&amp;MDR为32位，表示存储容量为64K*32位\\end{align}$$ 运算速度吞吐量系统在单位时间内处理的请求数量 响应时间从用户向计算机发生一个请求，到系统对该请求做出响应并获得所需结果的等待时间 主频机内主时钟的频率 CPU的时钟周期主频的倒数 CPI(Clock cycle Per Instruction)执行一条指令所需的时钟周期数，CPI指该程序或该机器指令集中的所有指令执行所需的平均时钟周期数 CPU执行周期运行一个程序所花费的时间$$\\begin{align}&amp;CPU执行时间&#x3D;\\frac{CPU时钟周期数}{主频}&#x3D;\\frac{指令条数*CPI}{主频}\\\\end{align}$$ MIPS(Million Instruction Per Second)每秒执行多少百万条指令$$\\begin{align}&amp;MIPS&#x3D;\\frac{指令条数}{执行时间10^6}&#x3D;\\frac{主频}{CPI10^6}\\\\end{align}$$ FLOPS(Floating-point Operations Per Second)每秒执行多少次浮点运算$$\\begin{align}&amp;FLOPS&#x3D;\\frac{浮点操作数次数}{执行时间}\\\\end{align}$$ *单位计量$$\\begin{align}&amp;M&#x3D;10^6\\&amp;G&#x3D;10^9\\&amp;T&#x3D;10^{12}\\&amp;P&#x3D;10^{15}\\&amp;E&#x3D;10^{18}\\&amp;Z&#x3D;10^{21}\\\\end{align}$$ 基准程序专门用于进行性能评价的一组程序","categories":[],"tags":[]},{"title":"","slug":"OS_002_Sync-problem","date":"2023-08-09T03:00:40.850Z","updated":"2023-08-07T08:49:48.000Z","comments":true,"path":"2023/08/09/OS_002_Sync-problem/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/OS_002_Sync-problem/","excerpt":"","text":"经典同步问题 1.生产者消费者问题 一组生产者进程(Producer) 一组消费者进程(Consumer) 共享初始为空 大小为n的缓冲区(Buffer) 1234567891011121314151617181920212223semaphore mutex = 1; //mutexsemaphore empty = n; //buffersemaphore full = 0; //fullProducer()&#123; while(1)&#123; Produce(); P(mutex); add2Buffer(); V(mutex); V(full); &#125;&#125;Consumer()&#123; while(1)&#123; P(full); P(mutex); getFromBuffer(); V(mutex); Consume(); &#125;&#125; 2.读者写者问题 读者进程(Reader) 写者进程(Writer) 共享一个文档(Document) 多进程读，不可多进程写 写进程写，不可读 写进程检查是否有读进程读 读进程优先 12345678910111213141516171819202122232425int count = 0;semaphore mutex = 1;semaphore rw = 1;Reader()&#123; while(1)&#123; P(mutex); if(count == 0) P(rw); count++; V(mutex); Read(); P(mutex); count--; if(count == 0) V(rw); V(mutex); &#125;&#125;Writer()&#123; while(1)&#123; P(rw); write(); v(rw); &#125;&#125; 写进程优先 123456789101112131415161718192021222324252627282930int count = 0;semaphore mutex = 1;semaphore rw = 1;semaphore w = 1;Writer()&#123; while(1)&#123; P(w); P(rw); Write(); V(rw); V(w); &#125;&#125;Reader()&#123; while(1)&#123; P(w); P(mutex); if(count == 0) P(rw); count++; V(mutex); V(w); Read(); P(mutex); count--; if(count == 0) V(rw); V(mutex); &#125;&#125; 3.哲学家进餐问题 5名哲学家(Philosopher) 每两名之间有一根筷子(Chopstick) 每名有一碗饭 吃完饭思考 1234567891011121314semaphore Chopsticks[5] = &#123;1, 1, 1, 1, 1&#125;;semaphore mutex = 1;Philosopher()&#123; do&#123; P(mutex); P(Chopsticks[i]); P(Chopsticks[(i+1)%5]); V(mutex); eat(); V(Chopsticks[i]); V(Chopsticks[(i+1)%5]); think(); &#125;&#125; 4.吸烟者问题 3个吸烟者进程(Smoker) 1个提供者进程(Offer) Smoker1(paper, glue) Smoker2(tobacco, glue) Smoker3(paper, tobacco) Offer(offer1) return paper, glue Offer(offer2) return tobacco, glue Offer(offer3) return paper, tobacco 123456789101112131415161718192021222324252627282930313233343536373839int num = 0; //store random numsemaphore offer1 = 0;semaphore offer2 = 0;semaphore offer3 = 0;semaphore end = 0;Offer()&#123; whlie(1)&#123; num++; num = num % 3; if(num == 0) V(offer1); else if(num == 1) V(offer2); else V(offer3); P(end); &#125;&#125;Smoker1()&#123; while(1)&#123; P(offer3); smoke(); V(end); &#125;&#125;Smoker2()&#123; while(1)&#123; P(offer2); smoke(); V(end); &#125;&#125;Smoker3()&#123; while(1)&#123; P(offer1); smoke(); V(end); &#125;&#125; eg1 3个进程P1 P2 P3 互斥使用N个单元的缓冲区(Buffer) P1 produce() return (int num) put() @Buffer P2","categories":[],"tags":[]},{"title":"","slug":"OS_001_introduction-Operator-System","date":"2023-08-09T03:00:40.815Z","updated":"2023-08-07T08:49:48.000Z","comments":true,"path":"2023/08/09/OS_001_introduction-Operator-System/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/OS_001_introduction-Operator-System/","excerpt":"","text":"第一章 计算机系统概述并发、共享、虚拟、异步没有并发和共享，就没有虚拟和异步 并发和共享互为存在条件 只有系统有并发性，才能导致异步性 命令接口联机：交互式 脱机：批处理 程序接口系统调用（广义指令） 操作系统用作扩充机器没有任何软件支持的计算机称为裸机 覆盖了软件的机器称为扩充机器或虚拟机 操作系统发展手工操作系统：独占全机、CPU等待手工操作 单道批处理操作系统：解决人机矛盾、CPU和I&#x2F;O设备速率不匹配、每次主机内存仅存放一道作业 多道批处理操作系统：资源利用率高、用户响应时间长、不提供人机交互 分时操作系统：同时、交互、独立、及时 实时操作系统：及时、可靠 网络操作系统 分布式计算机系统 个人操作系统 操作系统运行机制CPU的状态划分为用户态、核心态 中断和异常中断（外中断）：CPU执行指令意外的事件【设备发出的I&#x2F;O结束中断】 异常（内中断、陷入）：CPU执行指令内部的事件【非法操作码、地址越界、算术溢出、虚拟存储系统的缺页、陷入指令】 中断处理过程12345678910graph LR关中断1--&gt;保存断点保存断点--&gt;中断服务程序寻址中断服务程序寻址--&gt;保存现场和屏蔽字保存现场和屏蔽字--&gt;开中断1开中断1--&gt;执行中断服务程序执行中断服务程序--&gt;关中断2关中断2--&gt;恢复现场和屏蔽字恢复现场和屏蔽字--&gt;开中断2开中断2--&gt;中断返回 系统调用用户程序中调用操作系统提供的子功能 程序员可以使用高级语言，估计又要可以调用库函数，有的库函数封装了系统调用 用户进程执行 调用系统调用 进入核心态 执行系统调用 退出核心态 从系统调用返回 大内核 微内核大内核：内核代码庞大、结构混乱、难以维护、提供高性能系统服务 微内核：分离内核与服务、频繁切换用户核心态、操作系统执行开销大、内核内容少、方便维护 第二章 进程管理进程:程序段、数据段、PCB进程控制块 进程映像是静态的，进程是动态的 动态、并发、独立、异步、结构 进程状态运行、就绪、阻塞、创建、结束 进程通信共享存储（需要同步互斥工具PV操作、低级：基于共享数据结构、高级：基于存储区） 消息传递（进程间数据交换以格式化消息为单位、直接&#x2F;间接） 管道通信（pipe文件用于连接一个读进程和写进程通信、半双工、全双工需要两条管道） 线程 多线程引入进程目的：更好的使多道程序并发执行 引入线程目的：减小程序在并发执行付出的时空开销，提高并发性能 线程：线程ID、程序计数器、寄存器结合、堆栈组合、三态【就绪、阻塞、运行】 轻量实体，无系统资源，唯一ID和线程控制块 用户级线程ULT：操作系统意识不到ULT的存在，有关线程管理由应用程序完成 内核级线程KLT：有关线程管理由内核完成 多线程模型：多对一、一对一、多对多（m&lt;&#x3D;n） 处理机调度调度层次：作业调度（高）、内存调度（中）、进程调度（低） 不能进行进程调度与切换：处理中断、进程处于内核态、其他需要完全屏蔽中断的原子操作 调度方式：非剥夺调度（非抢占方式、适用于大多批处理系统、不能用于分时系统和大多数的实时系统）、剥夺调度（抢占方式、有优先权、短进程优先、时间片原则） 调度基本原则：CPU利用率、系统吞吐量、周转时间、等待时间、响应时间 系统吞吐量：单位时间内CPU完成作业的数量 周转时间：从作业提交到作业完成所用时间 $$T&#x3D;t_{等待}+t_{就绪队列排队}+t_{上处理机运行及输入输出}\\t_{周转时间}&#x3D;t_{作业完成时间}-t_{作业提交时间}\\t_{平均周转时间}&#x3D;\\frac{\\sum_i^n{t_i}}{n}(作业i的周转时间：t_i)\\t_{带权周转时间}&#x3D;\\frac{t_{作业周转时间}}{t_{作业实际运转}}\\geq{1}\\t_{带权平均周转时间}&#x3D;\\frac{\\sum_i^n{t_i}}{n}(作业i的带权周转时间：t_i)\\$$ 等待时间：进程处于等处理机状态的时间和 响应时间：从用户提交请求到系统首次产生响应所用时间 调度算法：先来先服务（FCFS、非抢占）、短作业优先（SJF、SPF）、抢占式短作业优先（SRTN）、优先级调度、高响应比优先（HRRN）、时间片轮转调度（RR）、多级反馈队列调度 1.先来先服务（FCFS）：简单，效率低；对长作业有利，对短作业不利；有利于CPU繁忙型作业，不利于I&#x2F;O繁忙型作业；不会导致饥饿；多用于早期批处理系统 2.短作业优先（SJF、SPF）：当前已到达的最短作业先上处理机；有优先权、短进程优先、时间片原则；适用于实时&#x2F;分时操作系统；调度机制导致长作业长时间不被调度（饥饿）；多用于早期批处理系统 3.抢占式短作业优先（SRTN）：最短剩余时间算法 4.优先级调度：适用于实时操作系统；剥夺、非剥夺、静态优先级、动态优先级；系统进程&gt;用户进程 交互型进程&gt;非交互型进程 I&#x2F;O型进程&gt;计算型进程;不导致饥饿 5.高响应比优先（HRRN）：多用于早期批处理系统 $$响应比R_p&#x3D;\\frac{t_{等待时间}+t_{要求服务时间}}{t_{要求服务时间}}\\geq{1}\\$$ 6.时间片轮转调度（RR）：时间片大小设置对系统性能影响很大，时间片足够大，以至于所有进程都能在一个时间片完成，退化为先来先服务算法；时间片太小，处理机频繁在进程间切换，增加处理机开销；时间片长短由系统的响应时间、就绪队列的进程数目、系统处理能力决定；不会导致饥饿 7.多级反馈队列调度：设置多个就绪队列，各个队列赋予不同优先级，赋予各个队列中进程执行时间片的大小各个不同，一个进程进入内存后挂在一级队列队尾，时间片内未完成进入第二级队列队尾，第一级队列为空下一级运行；课本认为是抢占式算法； 进程同步临界资源：进入区、临界区、退出区、剩余区 同步：完成某种任务而建立的两个或多个进程，需要协调制约关系 互斥：一个进程使用临界资源另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程允许访问临界资源 为禁止两个进程进入同一个临界区，指定准则： （1）空闲让进 （2）忙则等待 （3）有限等待 （4）让权等待 实现互斥基本方法： （1）软件：单标志法（违背空闲让进）、双标志法先检查（违法忙则等待）、双标志法后检查（导致饥饿、违背空闲让进、有限等待）、皮特森算法Peterson’s Algorithm（违法让权等待） （2）硬件：中断屏蔽方法（禁止一切中断发生、优：简单高效 &#x2F; 缺：不适用多处理机、用户进程，只适用于系统内核进程）、硬件指令方法TS&#x2F;TSL（优：适用于任意数目的进程，无论是单处理机还是多处理机，简单容易验证其正确性 &#x2F; 缺：不能实现让权等待，从等待中随机选择一个进临界区，可能导致饥饿） 信号量 整型信号量：用于表示资源数目的整型量S，只要信号量S&lt;&#x3D;0，就会不断测试，违背让权等待 1234567wait(S)&#123; while(S &lt;= 0) S = S - 1;&#125;signal(S)&#123; S = S + 1;&#125; 记录型信号量：一个用于记录资源数目的整型量value，一个进程链表L，链接等待资源的进程,S.L解决了让权等待的问题 1234567891011121314151617181920typedef struct&#123; int value; struct process *L;&#125;semaphore;void wait(semaphore S)&#123; S.value --; if(S.value &lt; 0)&#123; add this process P to S.L; &#125; block(S.L);&#125;void signal(semaphore S)&#123; S.value ++; if(S.value &lt;= 0)&#123; remove a process P from S.L; wakeup(P); &#125;&#125; 利用信号量实现同步：前V后P，必须保证一前一后，实现同步关系 1234567891011semaphore S = 0;P1()&#123; x; V(S); ...&#125;P2()&#123; P(S); y; ...&#125; 利用信号量实现互斥： 123456789101112131415seamphore S = 1;P1()&#123; ... P(S); 进程P1的临界区; V(S); ...&#125;P2()&#123; ... P(S); 进程P2的临界区 V(S); ...&#125; 利用信号量实现前驱关系： 对不同的临界资源设置不同的互斥信号量，PV必须成对出现 123456789101112131415161718192021222324252627282930313233343536373839semaphore a1 = a2 = b1 = b2 = c = d = e = 0;S1()&#123; ... V(a1); V(a2);&#125;S2()&#123; P(a1); ... V(b1); V(b2);&#125;S3()&#123; P(a2); ... V(c);&#125;S4()&#123; P(b1); ... V(d);&#125;S5()&#123; P(b2); ... V(e);&#125;S6()&#123; P(c); P(d); P(e); ...&#125; 另一角度：图论出度（P）入度（V） 管程 名称、共享结构数据、一组过程（函数）、设置共享结构数据初值 互斥特性由编译器负责实现 各外部进程&#x2F;线程，只能从管程提供的特定入口才能访问共享数据 每次只允许一个进程在管程内执行某个内部过程 经典同步问题 1.生产者消费者问题 一组生产者进程(Producer) 一组消费者进程(Consumer) 共享初始为空 大小为n的缓冲区(Buffer) 1234567891011121314151617181920212223semaphore mutex = 1; //mutexsemaphore empty = n; //buffersemaphore full = 0; //fullProducer()&#123; while(1)&#123; Produce(); P(mutex); add2Buffer(); V(mutex); V(full); &#125;&#125;Consumer()&#123; while(1)&#123; P(full); P(mutex); getFromBuffer(); V(mutex); Consume(); &#125;&#125; 2.读者写者问题 读者进程(Reader) 写者进程(Writer) 共享一个文档(Document) 多进程读，不可多进程写 写进程写，不可读 写进程检查是否有读进程读 读进程优先 12345678910111213141516171819202122232425int count = 0;semaphore mutex = 1;semaphore rw = 1;Reader()&#123; while(1)&#123; P(mutex); if(count == 0) P(rw); count++; V(mutex); Read(); P(mutex); count--; if(count == 0) V(rw); V(mutex); &#125;&#125;Writer()&#123; while(1)&#123; P(rw); write(); v(rw); &#125;&#125; 写进程优先 123456789101112131415161718192021222324252627282930int count = 0;semaphore mutex = 1;semaphore rw = 1;semaphore w = 1;Writer()&#123; while(1)&#123; P(w); P(rw); Write(); V(rw); V(w); &#125;&#125;Reader()&#123; while(1)&#123; P(w); P(mutex); if(count == 0) P(rw); count++; V(mutex); V(w); Read(); P(mutex); count--; if(count == 0) V(rw); V(mutex); &#125;&#125; 3.哲学家进餐问题 5名哲学家(Philosopher) 每两名之间有一根筷子(Chopstick) 每名有一碗饭 吃完饭思考 1234567891011121314semaphore Chopsticks[5] = &#123;1, 1, 1, 1, 1&#125;;semaphore mutex = 1;Philosopher()&#123; do&#123; P(mutex); P(Chopsticks[i]); P(Chopsticks[(i+1)%5]); V(mutex); eat(); V(Chopsticks[i]); V(Chopsticks[(i+1)%5]); think(); &#125;&#125; 4.吸烟者问题 3个吸烟者进程(Smoker) 1个提供者进程(Offer) Smoker1(paper, glue) Smoker2(tobacco, glue) Smoker3(paper, tobacco) Offer(offer1) return paper, glue Offer(offer2) return tobacco, glue Offer(offer3) return paper, tobacco 123456789101112131415161718192021222324252627282930313233343536373839int num = 0; //store random numsemaphore offer1 = 0;semaphore offer2 = 0;semaphore offer3 = 0;semaphore end = 0;Offer()&#123; whlie(1)&#123; num++; num = num % 3; if(num == 0) V(offer1); else if(num == 1) V(offer2); else V(offer3); P(end); &#125;&#125;Smoker1()&#123; while(1)&#123; P(offer3); smoke(); V(end); &#125;&#125;Smoker2()&#123; while(1)&#123; P(offer2); smoke(); V(end); &#125;&#125;Smoker3()&#123; while(1)&#123; P(offer1); smoke(); V(end); &#125;&#125; eg1 3个进程P1 P2 P3 互斥使用N个单元的缓冲区(Buffer) P1 produce() return (int num) put() @Buffer P2 死锁 多个进程因竞争资源造成的一种互相等待，若无外力作用，这些进程都将无法向前推进 死锁产生的原因：（1）系统资源的竞争（2）进程推进顺序非法（3）死锁产生的必要条件：互斥条件、不剥夺条件（已经至少保持了一个资源）、请求并保持条件、循环等待条件 发生死锁一定有循环等待，但发生循环等待未必死锁 死锁的处理策略 死锁预防：破坏四个必要条件之一即可 破坏互斥条件 破坏不剥夺条件 破坏请求并保持条件 破坏循环等待条件 死锁避免：在资源分配过程，防止进入不安全状态 银行家算法 $$Need &#x3D; Max -Allocation$$ 死锁检测和解除：系统分配进程时不做措施，应该提供死锁检测和解除的手段 资源分配图 请求边：从进程到资源分配边：从资源到进程 死锁定理：依次消除与不阻塞进程相连接的边，直到无边可消除 死锁解除：资源剥夺法、撤销进程法、进程回退法 第三章 内存管理程序装入和链接 编译：编译程序将源代码编译成若干目标模块 链接：链接程序将编译后形成一组目标模块及所需的库函数链接在一起，形成一个完整的装入模块 装入：装入程序装入模块装入内存运行 链接方式 静态：程序运行前，链接为一个完整可执行的程序 装入时动态：装入过程中，边装入边链接 运行时动态：程序执行过程需要该模块才进行 装入方式： 绝对装入：仅适用于单道程序，装入程序按照装入模块中的地址，将程序数据装入内存，逻辑地址与物理地址完全相同 可重定位装入（静态重定位）：一个作业装入内存，必须给它分配要求的全部内存空间，若没有足够的内存，则无法装入。一旦运行，作业进入内存，整个运行期间不能在内存中移动，也不能再申请内存空间 动态运行时装入（动态重定位）：程序在内存中发生移动，则需要采用动态的装入方式。装入程序把装入内存后，并不立即将装入模块中的相对地址转换为绝对地址，而是将地址转换推迟到程序真正要执行时才进行，需要重定位寄存器的支持 逻辑地址和物理地址 地址重定位：当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成为物理地址 内存保护 在CPU中设置一堆上、下限寄存器，CPU要访问一个地址，分别和两个寄存器值相比，判断有无越界 采用重定位寄存器（基址寄存器）和界地址寄存器（限长寄存器）来实现这种保护 连续分配管理方式 单一连续分配 内存分为系统区、用户区，无需进行内存保护，内存中只允许有有一道程序 优 简单无外部碎片 缺 只能用于单用户、单任务的操作系统中，有内部碎片，存储器利用率极低 固定分区分配 将用户内存空间划分为若干固定大小的区域，每个分区只装入一道作业。当有空闲分区时，可从外存的后备作业队列中选择适当大小的作业装入 划分分区方式：分区大小相等、分区大小不等 问题 程序可能太大放不进任何一块分区，用户不得不使用覆盖技术使用内存空间 主存利用率低，程序小也要占用一个分区，现象称为内部碎片 动态分区分配 又称可变分区分配，不预先划分内存，在进程装入内存时，根据进程大小动态地建立分区，并使分区的大小正好适合进程的需要 问题：所有分区外的存储空间会产生越来越多的碎片，克服外部碎片可以通过紧凑技术来解决 动态分区策略 首次适应Fiist Fit：空闲分区以地址递增的次序链接，找到大小能满足要求的第一个空间。会使得内存的低地址部分出现很多小的空闲分区，每次分配查找时，都要经过这些分区，增加了查找的开销 最佳适应Best Fit：空闲分区按容量递增的方式形成分区链，找到第一个能满足要求的空间。性能通常很差，因为每次最佳的分配会留下很小难以利用的内存块，会产生最多的外部碎片。 最坏适应Worst Fit（最大适应Lasgest Fit）:以容量递减的次序链接，找到第一个能满足要求的空闲分区。选择最大的块，但却把最大的连续内存的划分开，会很快导致没有可用的大内存块，导致性能很差 邻近适用Next Fit（循环首次适应算法）：分配内存时从上次查询结束的位置开始继续查找。在一次扫描中，内存前面部分使用后再释放时，不会参与分配，导致在内存的末尾分配空间分裂为小碎片。 非连续分配 基本分页存储管理方式：主存空间划分为大小相等且固定的块，块相对较小，作为主存的基本单位。每个进程以块为单位进行划分，进程在执行时，以块为单位逐个申请贮存中的空间 分页管理不会产生外部碎片，块的大小相对分区要小，进程按块划分，进程运行时按块申请主存可用空间并执行，进程只会在最后一个不完整的块，不产生主存碎片，，每个进程平均只产生半个块大小的内部碎片 页面和页面大小：进程中的块称为页（page），内存中的块称为页框（page frame、或页帧）。外存也以同样的单位进行划分，称为块（block）。进程在执行时需要申请主存空间，要为每个页面分配主存中的可用页框，页和页框一一对应 地址结构 页表：通过查找页表即可找到相应的物理块 基本地址变换： 将逻辑地址变换为内存中的物理地址，在系统中设置页表寄存器（PTR）存放内存起始地址F和页表长度M。 $$\\begin{aligned}&amp;（1）页号P&#x3D;\\frac{A}{L}\\&amp;（2）页内偏移量W&#x3D;A%L\\&amp;（3）比较页号P和页表长度M，P\\geq M产生越界中断\\&amp;（4）页表中页号P对应的页表项地址A&#x3D;F+PM\\&amp;（5）物理地址E&#x3D;bL+W\\\\end{aligned}$$页式管理中空间地址是一维的 问题 每次访存均需地址变换，地址变换必须足够快，否则访存速度会降低 每个进程引入页表，用于存储映射机制，页表不能过大 引入快表机制：在地址变换过程中加入具有并行查找能力的高速缓冲存储器——快表（相联存储器TLB，主存中的页表为慢表） 一次地址变换流程 （1）CPU给出逻辑地址，将页号送入高速缓冲寄存器，查询此页号是否存在于快表内 （2）若匹配到，直接取出对应页的页框号，与页内偏移拼接为物理地址，访存 （3）若未匹配到，访问慢表查询，读取页表项后复制到快表中，进行地址变换后访存 值得注意的是，题中是否说明快表初始为空以及快表慢表的查询机制，是否同时查询！ 两级页表 引入页表，执行时不需要调入所有内存页框，为了压缩页表，采取多级映射 多级页表大小不能超过一个页面 $$\\begin{aligned}&amp;逻辑地址：32bit\\&amp;以字节编址\\&amp;页表项：4B\\&amp;页面大小：4KB&#x3D;2^{12}B （页内偏移量）\\&amp;页号：32-12&#x3D;20\\&amp;全映射需要2^{20}个页表项\\&amp;共需4B2^{20}&#x3D;4MB大小空间存储页表\\&amp;\\&amp;以40MB进程为例\\&amp;页表项：\\frac{40MB}{4B4KB}&#x3D;40KB\\&amp;需要\\frac{40KB}{4KB}&#x3D;10页面\\&amp;整个进程需要\\frac{40MB}{4KB}&#x3D;10*2^{10}个页面\\&amp;\\&amp;为了压缩页表，采取二级页表机制\\&amp;页表10页进行映射只需要10个页表项\\&amp;上一级页表只需要1页就已经足够2^{10}&#x3D;1024个页表项\\&amp;进程执行时，仅需将这一页的上级页表调入即可\\&amp;页面大小：4KB&#x3D;2^{12}B （页内偏移量）\\&amp;页号：32-12&#x3D;20\\&amp;顶级（一级）页表为1个页面\\&amp;一级页表项\\frac{4KB}{4B}&#x3D;1K\\&amp;一级页表占用log_2{1K}&#x3D;10位\\&amp;二级页表占用20-10&#x3D;10位\\&amp;二级页表大小2^{10}*4B&#x3D;4KB\\leq页面大小4KB\\\\end{aligned}$$ 基本分段存储管理方式 分页管理是从计算机角度考虑设计，提高内存利用率，分页通过硬件机制实现 分段管理是从用户和程序员出发，方便编程、信息保护和共享、动态增长以及动态链接等方面的需要 段号决定每个进程最多可以分几个段，段内地址决定每个段内的最大长度 段表：每个进程都有一张逻辑空间与内存空间映射的段表 地址变换机构 $$从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W\\比较段号S和段表长度M，S\\geq M,产生越界中断，否则继续执行\\段表中段号S对应段表项地址&#x3D;段表始址F+段号S*段表项长度\\段内偏移量\\geq C，产生越界中断，否则继续执行\\取出段表项中该段的起始地址b，E&#x3D;b+W，得到物理地址E去访问内存\\$$ 段的共享与保护：分段系统，通过两个作业的段表中相应表项指向被共享的段同一个物理副本，当地一个作业从共享段读取数据，必须防止另一个修改数据，不能修改的的代码称为纯代码、可重入代码 段号和段内偏移需要显示给出，地址空间为二维 段页式管理方式 作业的地址空间被分为若干逻辑段，每段都有自己的段号，将每段分为若干大小的固定的页，内存空间分为若干和页面大小相同的存储块，对内存的分配以块为单位。 段号位数决定每个进程最多可以分几个段 页号位数决定最多有几个页 页内偏移量决定页面大小、内存块的大小 段页式系统的逻辑地址结构： 地址变换 需要三次访存，可以使用快表机制 虚拟内存管理 传统存储管理方式 一次性：一次全部装入，才能开始 驻留性：装入后，常驻内存，任何部分都不会被换出，直至结束 局部性原理 一个程序，一段时间内，只有一部分会被访问 空间 时间 虚拟存储器 将程序的一部分装入内存，其余部分留在外存，当所访问的部分不在内存，操作系统将需要的部分调入内存，将暂时不需要的内容换到外存 多次性：无需一次全部装入，允许分为多次调入 对换性：无需常驻内存 虚拟性：从逻辑上扩充内存容量，使用户看到的内存容量远大于内存容量 本质：用时间换空间 实现：请求分页、请求分段、请求段页式 支持：内存、外存、页表机制、段表机制、中断机构、地址变换机构 请求分页管理方式 访问不存在内存中的页面，通过调页将其调入，通过置换算法将暂时不需要的页面调到外存上 页表机制 $$状态位P：指示是否调入内存\\访问字段A：记录一段时间内被访问的次数\\修改位M：标识页面调入内存后是否被修改过\\外存地址：指出该页在外存上的地址，通常是物理块号\\$$ 缺页中断机制 访问页面不在内存中时，产生一个缺页中断，请求操作系统将缺页调入内存，将缺页的进程阻塞，若内存有闲置的空闲块，则分配一个块，将页面装入，并修改页表相应的页表项，若内存中无空闲块，则淘汰某页，淘汰页若在内存中修改过需要同步，写回外存 地址变换机构 页面置换算法 最佳置换算法OPT 选择的被淘汰页是以后永不使用的页面，或是最长时间内不再被访问的页面 先进先出算法FIFO 优先淘汰最早进入内存的页面，即内存中驻留时间最久的页面 会产生所分配的物理块增大页故障数不减反增的异常现象，Belady异常 最近最久未使用算法LRU 选择最近最长时间未访问过的页面予以淘汰，为每个页面设置一个访问字段记录上次被访问所经历的时间 性能较好，需要寄存器和栈的硬件支持 时钟置换算法CLOCK 简单的CLOCK算法：每帧关联一个附加位，使用位u，连成一个循环队列。某页装入时，使用位置为1；被访问时，使用位置为1；置换时，操作系统扫描缓冲区，每当遇到一个使用位为1的帧，置为0；最后停留在第一个使用位为0的帧 CLOCK算法性能比较接近LRU算法 改进的CLOCK算法：再增加一个修改位m，P(u,m) 第一轮扫描，指针扫描过的页面使用位u置为0 第一轮扫描中，未找到使用位u为0的页面进行第二轮扫描 第二轮扫描，第一个页面置换出，换入页面m修改位置为1，并将指针后移 页面分配策略 一个进程分配的物理页框的集合，分配给一个进程的存储量越小，任何时候驻留在主存中的进程数就越多，从而提高处理机的效率；一个进程页数过少，基于局部性原理，页错误率会相对较高；页数过多，基于局部性原理，给特定的进程分配更多主存空间对该进程的错误率没有明显改善。 策略：固定分配局部置换、可变分配全局置换、可变分配局部置换（没有固定分配全局分配） 调入时机：预调页策略、请求调页策略 从何处调页：系统拥有足够的对换区间、系统缺少足够的对换区间、UNIX方式 抖动 某进程频繁访问的页面数目高于可用的物理页帧数目 工作集 在某段时间间隔内，进程要访问的页面集合 一般，分配给进程的物理块数（驻留集大小）要大于工作集大小 第四章 文件系统文件 文件结构：数据项、记录、文件 属性：名称、标识符、类型、位置、大小、保护、时间 所有文件的信息都保存在目录结构中，而目录结构保存在外存上，文件信息在需要时调入内存。 基本操作：创建、写、读、重定位、删除、截断 文件逻辑结构 无结构文件（流式文件） 二进制式字符流组成 有结构文件（记录式文件） 顺序文件 索引文件 索引顺序文件 直接文件或散列文件 目录结构","categories":[],"tags":[]},{"title":"","slug":"ML_002_二分类问题使用SVM和BP神经网络的样例[Speech-and-not-speech-detection]","date":"2023-08-09T03:00:40.779Z","updated":"2023-08-07T08:49:48.000Z","comments":true,"path":"2023/08/09/ML_002_二分类问题使用SVM和BP神经网络的样例[Speech-and-not-speech-detection]/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/ML_002_%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E4%BD%BF%E7%94%A8SVM%E5%92%8CBP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A0%B7%E4%BE%8B[Speech-and-not-speech-detection]/","excerpt":"","text":"二分类问题使用SVM和BP神经网络的样例【Speech and not-speech detection】 - ebxeax - 博客园 (cnblogs.com) 数据集采用浙大胡老师的编程作业为例 Assignment 1: Speech and not-speech detectionDDL：2017-10-17 Tue.（1）This assignment is carried out by group. You could choose your teammate freely. Each group consists of at most 3 students. （2）The ‘training.data’ contains the training data. It is from our project to detect whether a person in a video speaks or not. The features are generated in the following way, which may help you making the most of these features.1、Get the mouth region M from the origin image based on facial landmark detection.2、Calculate dense optic flow between mouth region of last frame and the current frame and generate a score S that depicts the motion of mouth.3、Calculate the parameter V which depicts the degree of mouth opening.4、For frame i, we also calculate the S and V for its previous and next frames.5、Hence, we generate a 6 dimensional feature vector is X&#x3D;[Si-1 Si Si+1 Vi-1 Vi Vi+1].6、The label is at the end of each line, where +1 represents speaking, and -1 represents not-speaking. In the training.data, the ratio of positive examples over negative examples is 1:1. Keep this in mind, for if you find your training error or validation error is larger than 50%, that means your solution learns nothing and performs worse than guessing. （4）You need to write a program to predict speaking or not speaking.For convenience to evaluate your grogram, please use this name for your matlab main function:speakingDetection.mNote about the interface in your function ‘speakingDetection.m’, it should be:function predY&#x3D; speakingDetection (X)X: The input feature vectors, which is an N6 matrix, where N is the number of feature vectors.predY: The output vector to predict labels of X, which is a N1 vector, and predY(i) &#x3D; 1 or -1. Besides MATLAB, you also use Python, as long as you hold the interface protocol above. Note we don’t recommend C&#x2F;C++. （5）You can use ANY method to solve this problem. 问题分析 数据解读：training.data数据为N*7的matrix矩阵，其中6维vector向量为输入特征 input feature 数据预处理：将training.data读入，进行dataset的分割，分为6维向量input feature和1维向量label,分割前对数据集进行shuffle，分出测试集以及训练集 模型选择：该问题为数据的分类，采用分类算法可以解决，本文以SVM和BP神经网络为样例 建立相应模型求解问题 调节参数，达到最优解 BP神经网络代码 123456789101112131415161718192021222324252627282930313233343536373839clc;clear;data=importdata(&#x27;training.data&#x27;);P=data(:,1:6);T=data(:,7);temp = randperm(size(data,1));% 训练集——5000个样本P_train = P(temp(1:5000),:)&#x27;;T_train = T(temp(1:5000),:)&#x27;;P_test = P(temp(end-50:end),:)&#x27;;T_test = T(temp(end-50:end),:)&#x27;;N=size(T_test,2);[pn,minp,maxp,tn,mint,maxt]=premnmx(P_train,T_train);[pn_,minp_,maxp_,tn_,mint_,maxt_]=premnmx(P_test,T_test);dx=[-1,1;-1,1;-1,1;-1,1;-1,1;-1,1];net=newff(dx,[6,10,1]);net.trainParam.goal = 0;net.trainParam.epochs = 30000;net.trainParam.lr = 0.03;net.trainParam.showWindow = 1;net = train(net,pn,tn);an = sim(net,pn_);a=postmnmx(an,mint_,maxt_);disp([&#x27;mse: &#x27; num2str(mse(T_test-an))]);count=0;error=0;for i=1:N if abs(a(i)-T_test(i))&lt;0.2 count=count+1; else error=error+1; endendaccuracy=count/(count+error)figureplot(1:N,T_test,&#x27;b*&#x27;,1:N,a,&#x27;ro&#x27;)legend(&#x27;真实值&#x27;,&#x27;预测值&#x27;)xlabel(&#x27;预测样本&#x27;)ylabel(&#x27;实值&#x27;) 运行结果 accuracy：0.7059 mse: 1.0783 SVM代码[采用LIBSVM] 123456789101112131415161718192021222324252627data=importdata(&#x27;training.data&#x27;);features=data(:,1:6);%特征列表classlabel=data(:,7);%对应类别n = randperm(size(features,1));%随机产生训练集和测试集%% 训练集--70个样本train_features=features(n(1:44000),:);train_label=classlabel(n(1:44000),:);%% 测试集--30个样本test_features=features(n(44000:end),:);test_label=classlabel(n(44000:end),:);%% 数据归一化 [Train_features,PS] = mapminmax(train_features&#x27;); Train_features = Train_features&#x27;; Test_features = mapminmax(&#x27;apply&#x27;,test_features&#x27;,PS); Test_features = Test_features&#x27;; %% 创建/训练SVM模型model = svmtrain(train_label,Train_features,&#x27;-h 0&#x27;);%% SVM仿真测试[predict_train_label] = svmpredict(train_label,Train_features,model);[predict_test_label] = svmpredict(test_label,Test_features,model);%% 打印准确率compare_train = (train_label == predict_train_label);accuracy_train = sum(compare_train)/size(train_label,1)*100; fprintf(&#x27;训练集准确率：%f\\n&#x27;,accuracy_train)compare_test = (test_label == predict_test_label);accuracy_test = sum(compare_test)/size(test_label,1)*100;fprintf(&#x27;测试集准确率：%f\\n&#x27;,accuracy_test) 运行结果 12345678910.................*optimization finished, #iter = 17228nu = 0.658959obj = -28684.553581, rho = 4.599546nSV = 29001, nBSV = 28987Total nSV = 29001Accuracy = 71.7273% (31560/44000) (classification)Accuracy = 71.1948% (435/611) (classification)训练集准确率：71.727273测试集准确率：71.194763 结果分析 两种模型按照题目要求可以达到错误率低于50%的要求，相对而言，SVM在该问题上无论是性能还是效果都略高于BP神经网络算法，SVM更适用于小样本的分类问题 文件下载：training.data https://files.cnblogs.com/files/Carraway-Space/training.zip","categories":[],"tags":[]},{"title":"","slug":"ML_001_introduce-machine-learning","date":"2023-08-09T03:00:40.726Z","updated":"2023-08-07T08:49:48.000Z","comments":true,"path":"2023/08/09/ML_001_introduce-machine-learning/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/ML_001_introduce-machine-learning/","excerpt":"","text":"*《机器学习》 学习笔记（**1**）–**绪论* 机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材前两个章节的知识点以及自己一点浅陋的理解。 *1 绪论* 傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！哈哈，也希望自己这学期的machine learning课程取得一个好成绩！ *1.1 机器学习的定义* 正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。 另一本经典教材的作者Mitchell给出了一个形式化的定义，假设： · P：计算机程序在某任务类T上的性能。 · T：计算机程序希望实现的任务类。 · E：表示经验，即历史的数据集。 若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。 *1.2 机器学习的一些基本术语* 假设我们收集了一批西瓜的数据，例如：（色泽&#x3D;青绿;根蒂&#x3D;蜷缩;敲声&#x3D;浊响)， (色泽&#x3D;乌黑;根蒂&#x3D;稍蜷;敲声&#x3D;沉闷)， (色泽&#x3D;浅自;根蒂&#x3D;硬挺;敲声&#x3D;清脆)……每对括号内是一个西瓜的记录，定义： · 所有记录的集合为：数据集。 · 每一条记录为：一个实例（instance）或样本（sample）。 · 例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。 · 对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。 · 一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。 在计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义： · 所有训练样本的集合为：训练集（trainning set），[特殊]。 · 所有测试样本的集合为：测试集（test set），[一般]。 · 机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。 在西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义： · 预测值为离散值的问题为：分类（classification）。 · 预测值为连续值的问题为：回归（regression）。 在我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义： · 训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。 · 训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。 *2 模型的评估与选择* *2.1 误差与过拟合* 我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义： · 在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。 · 在测试集上的误差称为测试误差（test error）。 · 学习器在所有新样本上的误差称为泛化误差（generalization error）。 显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义： · 学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。 · 学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。 可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。 *2.2 评估方法* 在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。 因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why： 假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。 *2.3 训练集与测试集的划分方法* 如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法： *2.3.1 留出法* 将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D&#x3D;S∪T且S∩T&#x3D;∅，常见的划分为：大约2&#x2F;3-4&#x2F;5的样本用作训练，剩下的用作测试。需要注意的是：训练&#x2F;测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。 *2.3.2 交叉验证法* 将数据集D划分为k个大小相同的互斥子集，满足D&#x3D;D1∪D2∪…∪Dk，Di∩Dj&#x3D;∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集&#x2F;测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。 与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练&#x2F;测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。 *2.3.3 自助法* 我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。 自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为： 这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集&#x2F;测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。 *2.4 调参* 大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。 学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练&#x2F;测试集就有555&#x3D; 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。 最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。 *《机器学习》 学习笔记（2）–性能度量* 本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练&#x2F;测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。 *2.5 性能度量* 性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。 *2.5.1 最常见的性能度量* 在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。 在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度&#x3D;1。 *2.5.2 查准率&#x2F;查全率&#x2F;F1* 错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率: precision），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率:recall）。因此，使用查准&#x2F;查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准&#x2F;查全率定义如下： 初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negative，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边： 正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。 “P-R曲线”正是描述查准&#x2F;查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示： P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P&#x3D;R时的取值，平衡点的取值越高，性能更优。 P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即： 特别地，当β&#x3D;1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。 有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。 *2.5.3 ROC与AUC* 如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC（Receiver Operating Characteristic）曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。 简单分析图像，可以得知：当FN&#x3D;0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。 现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。 同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Under ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。 *2.5.4 代价敏感错误率与代价曲线* 上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt;有疾病只是增多了检查，但有疾病–&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。 在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为： 同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。 代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示： *《机器学习》 学习笔记（3）–假设检验、方差与偏差* 在上两篇中，我们介绍了多种常见的评估方法和性能度量标准，这样我们就可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差“。但由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。本篇延续上一篇的内容，主要讨论了比较检验、方差与偏差。 *2.6 比较检验* 在比较学习器泛化性能的过程中，统计假设检验（hypothesis test）为学习器性能比较提供了重要依据，即若A在某测试集上的性能优于B，那A学习器比B好的把握有多大。 为方便论述，本篇中都是以“错误率”作为性能度量的标准。 *2.6.1 假设检验* “假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望u&#x3D;u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。 *2.6.2 交叉验证t检验* *2.6.3 McNemar检验* MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01&#x3D;e10，且|e01-e10|服从N（1，e01+e10）分布。 因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布N（0,1）的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设–&gt;求出满足显著度的临界点–&gt;给出拒绝域–&gt;验证假设。 *2.6.4 Friedman检验与Nemenyi后续检验* 上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3…，相同则平分序值，如下图所示： 若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）&#x2F;2，（k+1）(k-1)&#x2F;12），则有： 服从自由度为k-1和(k-1)(N-1)的F分布。下面是F检验常用的临界值： 若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。 *2.7 偏差与方差* 偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。通过对泛化误差的进行分解，可以得到： · *期望泛化误差&#x3D;方差+偏差* · *偏差刻画学习器的拟合能力* · *方差体现学习器的稳定性* 易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。 *《机器学习》 学习笔记（4）–线性模型* 前一部分主要是对机器学习预备知识的概括，包括机器学习的定义&#x2F;术语、学习器性能的评估&#x2F;度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容–线性模型。 *3、线性模型*谈及线性模型，其实我们很早就已经与它打过交道：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y&#x3D;ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。 *3.1 线性回归*线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。 有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理： · 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。 · 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。 （1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y&#x3D;wx+b的两个参数w和b，计算过程如下图所示： （2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y&#x3D;wx+b需要写成： 通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式： 同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。 另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示： 更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。 *3.2 线性几率回归*回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。 若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。 *3.3 线性判别分析*线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示： 想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。 · 类内散度矩阵（within-class scatter matrix） · 类间散度矩阵(between-class scaltter matrix) 因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。 从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。 若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。 *3.4 多分类学习*现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。 · OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类&#x2F;一个反类），从而产生N（N-1）&#x2F;2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。 · OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。 · MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明&#x2F;欧式距离选择距离最小的类别作为最终分类结果。 *3.5 类别不平衡问题*类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种： \\1. 在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。 \\2. 在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。 \\3. 直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。 《机器学习》 学习笔记（5）–决策树上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法–决策树（Decision Tree）。 *4、决策树**4.1 决策树基本概念*顾名思义，决策树是基于树结构来进行决策的，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话： 女儿：多大年纪了？ 母亲：26。 女儿：长的帅不帅？ 母亲：挺帅的。 女儿：收入高不？ 母亲：不算很高，中等情况。 女儿：是公务员不？ 母亲：是，在税务局上班呢。 女儿：那好，我去见见。 这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。 在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知： * 每个非叶节点表示一个特征属性测试。 * 每个分支代表这个特征属性在某个值域上的输出。 * 每个叶子节点存放一个类别。 * 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。 *4.2 决策树的构造*决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示： 可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。 *4.2.1 ID3算法*ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为： 假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。 信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。 *4.2.2 C4.5算法*ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为： *4.2.3 CART算法*CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下： 进而，使用属性α划分后的基尼指数为： *4.3 剪枝处理*从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下： * 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。 * 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。 评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。 上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。 *4.4 连续值与缺失值处理*对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。 * 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。 * 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。 * 选择最大信息增益的划分点作为最优划分点。 现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义： 对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即： 对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为： *《机器学习》 学习笔记（6）–神经网络* 上篇主要讨论了决策树算法。首先从决策树的基本概念出发，引出决策树基于树形结构进行决策，进一步介绍了构造决策树的递归流程以及其递归终止条件，在递归的过程中，划分属性的选择起到了关键作用，因此紧接着讨论了三种评估属性划分效果的经典算法，介绍了剪枝策略来解决原生决策树容易产生的过拟合问题，最后简述了属性连续值&#x2F;缺失值的处理方法。本篇将讨论现阶段十分热门的另一个经典监督学习算法–神经网络（neural network）。 *5、神经网络*在机器学习中，神经网络一般指的是“神经网络学习”，是机器学习与神经网络两个学科的交叉部分。所谓神经网络，目前用得最广泛的一个定义是“神经网络是由具有适应性的简单单元组成的广泛并行互连的网络，它的组织能够模拟生物神经系统对真实世界物体所做出的交互反应”。 *5.1 神经元模型*神经网络中最基本的单元是神经元模型（neuron）。在生物神经网络的原始机制中，每个神经元通常都有多个树突（dendrite），一个轴突（axon）和一个细胞体（cell body），树突短而多分支，轴突长而只有一个；在功能上，树突用于传入其它神经元传递的神经冲动，而轴突用于将神经冲动传出到其它神经元，当树突或细胞体传入的神经冲动使得神经元兴奋时，该神经元就会通过轴突向其它神经元传递兴奋。神经元的生物学结构如下图所示： 一直沿用至今的“M-P神经元模型”正是对这一结构进行了抽象，也称“阈值逻辑单元”，其中树突对应于输入部分，每个神经元收到n个其他神经元传递过来的输入信号，这些信号通过带权重的连接传递给细胞体，这些权重又称为连接权（connection weight）。细胞体分为两部分，前一部分计算总输入值（即输入信号的加权和，或者说累积电平），后一部分先计算总输入值与该神经元阈值的差值，然后通过激活函数（activation function）的处理，产生输出从轴突传送给其它神经元。M-P神经元模型如下图所示： 与线性分类十分相似，神经元模型最理想的激活函数也是阶跃函数，即将神经元输入值与阈值的差值映射为输出值1或0，若差值大于零输出1，对应兴奋；若差值小于零则输出0，对应抑制。但阶跃函数不连续，不光滑，故在M-P神经元模型中，也采用Sigmoid函数来近似， Sigmoid函数将较大范围内变化的输入值挤压到 (0,1) 输出值范围内，所以也称为挤压函数（squashing function）。 将多个神经元按一定的层次结构连接起来，就得到了神经网络。它是一种包含多个参数的模型，比方说10个神经元两两连接，则有100个参数需要学习（每个神经元有9个连接权以及1个阈值），若将每个神经元都看作一个函数，则整个神经网络就是由这些函数相互嵌套而成。 *5.2 感知机与多层网络*感知机（Perceptron）是由两层神经元组成的一个简单模型，但只有输出层是M-P神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。这样一来，感知机与之前线性模型中的对数几率回归的思想基本是一样的，都是通过对属性加权与另一个常数求和，再使用sigmoid函数将这个输出值压缩到0-1之间，从而解决分类问题。不同的是感知机的输出层应该可以有多个神经元，从而可以实现多分类问题，同时两个模型所用的参数估计方法十分不同。 给定训练集，则感知机的n+1个参数（n个权重+1个阈值）都可以通过学习得到。阈值Θ可以看作一个输入值固定为-1的哑结点的权重ωn+1，即假设有一个固定输入xn+1&#x3D;-1的输入层神经元，其对应的权重为ωn+1，这样就把权重和阈值统一为权重的学习了。简单感知机的结构如下图所示： 感知机权重的学习规则如下：对于训练样本（x，y），当该样本进入感知机学习后，会产生一个输出值，若该输出值与样本的真实标记不一致，则感知机会对权重进行调整，若激活函数为阶跃函数，则调整的方法为（基于梯度下降法）： 其中 η∈（0，1）称为学习率，可以看出感知机是通过逐个样本输入来更新权重，首先设定好初始权重（一般为随机），逐个地输入样本数据，若输出值与真实标记相同则继续输入下一个样本，若不一致则更新权重，然后再重新逐个检验，直到每个样本数据的输出值都与真实标记相同。容易看出：感知机模型总是能将训练数据的每一个样本都预测正确，和决策树模型总是能将所有训练数据都分开一样，感知机模型很容易产生过拟合问题。 由于感知机模型只有一层功能神经元，因此其功能十分有限，只能处理线性可分的问题，对于这类问题，感知机的学习过程一定会收敛（converge），因此总是可以求出适当的权值。但是对于像书上提到的异或问题，只通过一层功能神经元往往不能解决，因此要解决非线性可分问题，需要考虑使用多层功能神经元，即神经网络。多层神经网络的拓扑结构如下图所示： 在神经网络中，输入层与输出层之间的层称为隐含层或隐层（hidden layer），隐层和输出层的神经元都是具有激活函数的功能神经元。只需包含一个隐层便可以称为多层神经网络，常用的神经网络称为“多层前馈神经网络”（multi-layer feedforward neural network），该结构满足以下几个特点： * 每层神经元与下一层神经元之间完全互连 * 神经元之间不存在同层连接 * 神经元之间不存在跨层连接 根据上面的特点可以得知：这里的“前馈”指的是网络拓扑结构中不存在环或回路，而不是指该网络只能向前传播而不能向后传播（下节中的BP神经网络正是基于前馈神经网络而增加了反馈调节机制）。神经网络的学习过程就是根据训练数据来调整神经元之间的“连接权”以及每个神经元的阈值，换句话说：神经网络所学习到的东西都蕴含在网络的连接权与阈值中。 *5.3 BP神经网络算法*由上面可以得知：神经网络的学习主要蕴含在权重和阈值中，多层网络使用上面简单感知机的权重调整规则显然不够用了，BP神经网络算法即误差逆传播算法（error BackPropagation）正是为学习多层前馈神经网络而设计，BP神经网络算法是迄今为止最成功的的神经网络学习算法。 一般而言，只需包含一个足够多神经元的隐层，就能以任意精度逼近任意复杂度的连续函数[Hornik et al.,1989]，故下面以训练单隐层的前馈神经网络为例，介绍BP神经网络的算法思想。 上图为一个单隐层前馈神经网络的拓扑结构，BP神经网络算法也使用梯度下降法（gradient descent），以单个样本的均方误差的负梯度方向对权重进行调节。可以看出：BP算法首先将误差反向传播给隐层神经元，调节隐层到输出层的连接权重与输出层神经元的阈值；接着根据隐含层神经元的均方误差，来调节输入层到隐含层的连接权值与隐含层神经元的阈值。BP算法基本的推导过程与感知机的推导过程原理是相同的，下面给出调整隐含层到输出层的权重调整规则的推导过程： 学习率η∈（0，1）控制着沿反梯度方向下降的步长，若步长太大则下降太快容易产生震荡，若步长太小则收敛速度太慢，一般地常把η设置为0.1，有时更新权重时会将输出层与隐含层设置为不同的学习率。BP算法的基本流程如下所示： BP算法的更新规则是基于每个样本的预测值与真实类标的均方误差来进行权值调节，即BP算法每次更新只针对于单个样例。需要注意的是：BP算法的最终目标是要最小化整个训练集D上的累积误差，即： 如果基于累积误差最小化的更新规则，则得到了累积误差逆传播算法（accumulated error backpropagation），即每次读取全部的数据集一遍，进行一轮学习，从而基于当前的累积误差进行权值调整，因此参数更新的频率相比标准BP算法低了很多，但在很多任务中，尤其是在数据量很大的时候，往往标准BP算法会获得较好的结果。另外对于如何设置隐层神经元个数的问题，至今仍然没有好的解决方案，常使用“试错法”进行调整。 前面提到，BP神经网络强大的学习能力常常容易造成过拟合问题，有以下两种策略来缓解BP网络的过拟合问题： · 早停：将数据分为训练集与测试集，训练集用于学习，测试集用于评估性能，若在训练过程中，训练集的累积误差降低，而测试集的累积误差升高，则停止训练。 · 引入正则化（regularization）：基本思想是在累积误差函数中增加一个用于描述网络复杂度的部分，例如所有权值与阈值的平方和，其中λ∈（0,1）用于对累积经验误差与网络复杂度这两项进行折中，常通过交叉验证法来估计。 *5.4 全局最小与局部最小*模型学习的过程实质上就是一个寻找最优参数的过程，例如BP算法试图通过最速下降来寻找使得累积经验误差最小的权值与阈值，在谈到最优时，一般会提到局部极小（local minimum）和全局最小（global minimum）。 * 局部极小解：参数空间中的某个点，其邻域点的误差函数值均不小于该点的误差函数值。 * 全局最小解：参数空间中的某个点，所有其他点的误差函数值均不小于该点的误差函数值。 要成为局部极小点，只要满足该点在参数空间中的梯度为零。局部极小可以有多个，而全局最小只有一个。全局最小一定是局部极小，但局部最小却不一定是全局最小。显然在很多机器学习算法中，都试图找到目标函数的全局最小。梯度下降法的主要思想就是沿着负梯度方向去搜索最优解，负梯度方向是函数值下降最快的方向，若迭代到某处的梯度为0，则表示达到一个局部最小，参数更新停止。因此在现实任务中，通常使用以下策略尽可能地去接近全局最小。 * 以多组不同参数值初始化多个神经网络，按标准方法训练，迭代停止后，取其中误差最小的解作为最终参数。 * 使用“模拟退火”技术，这里不做具体介绍。 * 使用随机梯度下降，即在计算梯度时加入了随机因素，使得在局部最小时，计算的梯度仍可能不为0，从而迭代可以继续进行。 *5.5 深度学习*理论上，参数越多，模型复杂度就越高，容量（capability）就越大，从而能完成更复杂的学习任务。深度学习（deep learning）正是一种极其复杂而强大的模型。 怎么增大模型复杂度呢？两个办法，一是增加隐层的数目，二是增加隐层神经元的数目。前者更有效一些，因为它不仅增加了功能神经元的数量，还增加了激活函数嵌套的层数。但是对于多隐层神经网络，经典算法如标准BP算法往往会在误差逆传播时发散（diverge），无法收敛达到稳定状态。 那要怎么有效地训练多隐层神经网络呢？一般来说有以下两种方法： · 无监督逐层训练（unsupervised layer-wise training）：每次训练一层隐节点，把上一层隐节点的输出当作输入来训练，本层隐结点训练好后，输出再作为下一层的输入来训练，这称为预训练（pre-training）。全部预训练完成后，再对整个网络进行微调（fine-tuning）训练。一个典型例子就是深度信念网络（deep belief network，简称DBN）。这种做法其实可以视为把大量的参数进行分组，先找出每组较好的设置，再基于这些局部最优的结果来训练全局最优。 · 权共享（weight sharing）：令同一层神经元使用完全相同的连接权，典型的例子是卷积神经网络（Convolutional Neural Network，简称CNN）。这样做可以大大减少需要训练的参数数目。 深度学习可以理解为一种特征学习（feature learning）或者表示学习（representation learning），无论是DBN还是CNN，都是通过多个隐层来把与输出目标联系不大的初始输入转化为与输出目标更加密切的表示，使原来只通过单层映射难以完成的任务变为可能。即通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示，从而使得最后可以用简单的模型来完成复杂的学习任务。 传统任务中，样本的特征需要人类专家来设计，这称为特征工程（feature engineering）。特征好坏对泛化性能有至关重要的影响。而深度学习为全自动数据分析带来了可能，可以自动产生更好的特征。 *《机器学习》 学习笔记（7）–支持向量机* 上篇主要介绍了神经网络。首先从生物学神经元出发，引出了它的数学抽象模型–MP神经元以及由两层神经元组成的感知机模型，并基于梯度下降的方法描述了感知机模型的权值调整规则。由于简单的感知机不能处理线性不可分的情形，因此接着引入了含隐层的前馈型神经网络，BP神经网络则是其中最为成功的一种学习方法，它使用误差逆传播的方法来逐层调节连接权。最后简单介绍了局部&#x2F;全局最小以及目前十分火热的深度学习的概念。本篇围绕的核心则是曾经一度取代过神经网络的另一种监督学习算法–****支持向量机*（Support Vector Machine），简称*SVM****。 *6、支持向量机*支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。 *6.1 函数间隔与几何间隔*对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c&#x3D;0来表示，超平面实际上表示的就是高维的平面，如下图所示： 对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。 *6.1.1 函数间隔*在超平面w’x+b&#x3D;0确定的情况下，|w’x*+b|能够代表点x距离超平面的远近，易知：当w’x+b&gt;0时，表示x在超平面的一侧（正类，类标为1），而当w’x+b&lt;0时，则表示x在超平面的另外一侧（负类，类别为-1），因此（w’x+b）y* 的正负性恰能表示数据点x是否被分类正确。于是便引出了***函数间隔****的定义（functional margin）: 而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔： 可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b&#x3D;0其实等价于2w1x1+2w2x2+2w3x3+2b&#x3D;0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念–几何间隔（geometrical margin）。 *6.1.2 几何间隔****几何间隔**代表的则是数据点到超平面的真实距离，对于超平面w’x+b&#x3D;0，w代表的是该超平面的法向量，设x为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x*&#x3D;x-r(w&#x2F;||w||)，又x在超平面上，即w’x+b&#x3D;0，代入即可得： 为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义： 从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w’x+b|，而几何间隔就是点到超平面的距离。 *6.2 最大间隔与支持向量*通过前面的分析可知：函数间隔不适合用来最大化间隔，因此这里我们要找的最大间隔指的是几何间隔，于是最大间隔分类器的目标函数定义为： 一般地，我们令r^为1（这样做的目的是为了方便推导和目标函数的优化），从而上述目标函数转化为： 对于y(w’x+b)&#x3D;1的数据点，即右图中位于w’x+b&#x3D;1或w’x+b&#x3D;-1上的数据点，我们称之为***支持向量*（support vector），易知：对于所有的支持向量，它们恰好满足y(w’x+b)&#x3D;1，而所有不是支持向量的点，有y(w’x+b)&gt;1。 *6.3 从原始优化问题到对偶问题*对于上述得到的目标函数，求1&#x2F;||w||的最大值相当于求||w||^2的最小值，因此很容易将原来的目标函数转化为： 即变为了一个带约束的凸二次规划问题，按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的****对偶问题****，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因： * 一是因为使用对偶问题更容易求解； * 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。 对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数： 上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1&#x2F;2||w||^2（此时令所有的α为0），因此实际上原问题等价于： 由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题： 这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。 （1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出： 将上述结果代入L得到： （2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。 （3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。 在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)&#x3D;w’x+b中，若f(x)&gt;0，则为正类，f(x)&lt;0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。 这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w’x+b)-1≥0，满足： *6.4 核函数*由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用****映射****的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分。若∅代表一个映射，则在特征空间中的划分函数变为： 按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出： （1）原对偶问题变为： （2）原分类函数变为： 求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了****核函数****（Kernel）的概念。 因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效****（低维计算，高维表现）****，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为： （1）对偶问题： （2）分类函数： 因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件： 由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数： *6.5 软间隔支持向量机*前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有****噪声*的情形，噪声数据（*outlier****）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。 为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了****“软间隔”支持向量机****的概念 * 允许某些数据点不满足约束y(w’x+b)≥1； * 同时又使得不满足约束的样本尽可能少。 这样优化目标变为： 如同阶跃函数，0&#x2F;1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。 支持向量机中的损失函数为****hinge损失*，引入*“松弛变量”****，目标函数与约束条件可以写为： 其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到： 按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有： 将w代入L化简，便得到其对偶问题： 将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。 *《机器学习》 学习笔记（8）–贝叶斯分类器* 上篇主要介绍和讨论了支持向量机。从最初的分类函数，通过最大化分类间隔，max(1&#x2F;||w||)，min(1&#x2F;2||w||^2)，凸二次规划，拉格朗日函数，对偶问题，一直到最后的SMO算法求解，都为寻找一个最优解。接着引入核函数将低维空间映射到高维特征空间，解决了非线性可分的情形。最后介绍了软间隔支持向量机，解决了outlier挤歪超平面的问题。本篇将讨论一个经典的统计学习算法–****贝叶斯分类器****。 *7、贝叶斯分类器*贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下贝叶斯公式。 *7.1 贝叶斯决策论*若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“****条件风险****”（conditional risk）。 我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了****贝叶斯判定准则****（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。 若损失函数λ取0-1损失，则有： 即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。一般这里有两种策略来对后验概率进行估计： * 判别式模型：直接对 P（c | x）进行建模求解。例如我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。 * 生成式模型：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。 贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换P（c | x）变身： 对于给定的样本x，P（x）与类标无关，P（c）称为类先验概率，p（x | c ）称为类条件概率。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，普及一下它们的基本概念。 * 先验概率： 根据以往经验和分析得到的概率。 * 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。 实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率 。 回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，因此这里一般采用极大似然法进行估计。 *7.2 极大似然法*极大似然估计（Maximum Likelihood Estimation，简称MLE），是一种根据数据采样来估计概率分布的经典方法。常用的策略是先假定总体具有某种确定的概率分布，再基于训练样本对概率分布的参数进行估计。运用到类条件概率p（x | c ）中，假设p（x | c ）服从一个参数为θ的分布，问题就变为根据已知的训练样本来估计θ。极大似然法的核心思想就是：估计出的参数使得已知样本出现的概率最大，即使得训练数据的似然最大。 所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤： * 1.写出似然函数； * 2.对似然函数取对数，并整理； * 3.求导数，令偏导数为0，得到似然方程组； * 4.解似然方程组，得到所有参数即为所求。 例如：假设样本属性都是连续值，p（x | c ）服从一个多维高斯分布，则通过MLE计算出的参数刚好分别为： 上述结果看起来十分合乎实际，但是采用最大似然法估计参数的效果很大程度上依赖于作出的假设是否合理，是否符合潜在的真实数据分布。 *7.3 朴素贝叶斯分类器*不难看出：原始的贝叶斯分类器最大的问题在于联合概率密度函数的估计，首先需要根据经验来假设联合概率分布，其次当属性很多时，训练样本往往覆盖不够，参数的估计会出现很大的偏差。为了避免这个问题，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为： 这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。 相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下： 当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。 *《机器学习》 学习笔记（9）–EM算法* 上篇主要介绍了贝叶斯分类器，从贝叶斯公式到贝叶斯决策论，再到通过极大似然法估计类条件概率，贝叶斯分类器的训练就是参数估计的过程。朴素贝叶斯则是“属性条件独立性假设”下的特例，它避免了假设属性联合分布过于经验性和训练集不足引起参数估计较大偏差两个大问题，最后介绍的拉普拉斯修正将概率值进行平滑处理。本篇将介绍另一个当选为数据挖掘十大算法之一的****EM算法****。 *8、EM算法*EM（Expectation-Maximization）算法是一种常用的估计参数隐变量的利器，也称为“期望最大算法”，是数据挖掘的十大经典算法之一。EM算法主要应用于训练集样本不完整即存在隐变量时的情形（例如某个属性值未知），通过其独特的“两步走”策略能较好地估计出隐变量的值。 *8.1 EM算法思想*EM是一种迭代式的方法，它的基本思想就是：若样本服从的分布参数θ已知，则可以根据已观测到的训练样本推断出隐变量Z的期望值（E步），若Z的值已知则运用最大似然法估计出新的θ值（M步）。重复这个过程直到Z和θ值不再发生变化。 简单来讲：假设我们想估计A和B这两个参数，在开始状态下二者都是未知的，但如果知道了A的信息就可以得到B的信息，反过来知道了B也就得到了A。可以考虑首先赋予A某种初值，以此得到B的估计值，然后从B的当前值出发，重新估计A的取值，这个过程一直持续到收敛为止。 现在再来回想聚类的代表算法K-Means：【首先随机选择类中心&#x3D;&gt;将样本点划分到类簇中&#x3D;&gt;重新计算类中心&#x3D;&gt;不断迭代直至收敛】，不难发现这个过程和EM迭代的方法极其相似，事实上，若将样本的类别看做为“隐变量”（latent variable）Z，类中心看作样本的分布参数θ，K-Means就是通过EM算法来进行迭代的，与我们这里不同的是，K-Means的目标是最小化样本点到其对应类中心的距离和，上述为极大化似然函数。 *8.2 EM算法数学推导*在上篇极大似然法中，当样本属性值都已知时，我们很容易通过极大化对数似然，接着对每个参数求偏导计算出参数的值。但当存在隐变量时，就无法直接求解，此时我们通常最大化已观察数据的对数“边际似然”（marginal likelihood）。 这时候，通过边缘似然将隐变量Z引入进来，对于参数估计，现在与最大似然不同的只是似然函数式中多了一个未知的变量Z，也就是说我们的目标是找到适合的θ和Z让L(θ)最大，这样我们也可以分别对未知的θ和Z求偏导，再令其等于0。 然而观察上式可以发现，和的对数（ln(x1+x2+x3)）求导十分复杂，那能否通过变换上式得到一种求导简单的新表达式呢？这时候 Jensen不等式就派上用场了，先回顾一下高等数学凸函数的内容： ****Jensen’s inequality****：过一个凸函数上任意两点所作割线一定在这两点间的函数图象的上方。理解起来也十分简单，对于凸函数f(x)”&gt;0，即曲线的变化率是越来越大单调递增的，所以函数越到后面增长越厉害，这样在一个区间下，函数的均值就会大一些了。 因为ln(*)函数为凹函数，故可以将上式“和的对数”变为“对数的和”，这样就很容易求导了。 接着求解Qi和θ：首先固定θ（初始值），通过求解Qi使得J（θ，Q）在θ处与L（θ）相等，即求出L（θ）的下界；然后再固定Qi，调整θ，最大化下界J（θ，Q）。不断重复两个步骤直到稳定。通过jensen不等式的性质，Qi的计算公式实际上就是后验概率： 通过数学公式的推导，简单来理解这一过程：固定θ计算Q的过程就是在建立L（θ）的下界，即通过jenson不等式得到的下界（E步）；固定Q计算θ则是使得下界极大化（M步），从而不断推高边缘似然L（θ）。从而循序渐进地计算出L（θ）取得极大值时隐变量Z的估计值。 EM算法也可以看作一种“坐标下降法”，首先固定一个值，对另外一个值求极值，不断重复直到收敛。这时候也许大家就有疑问，问什么不直接这两个家伙求偏导用梯度下降呢？这时候就是坐标下降的优势，有些特殊的函数，例如曲线函数z&#x3D;y^2+x^2+x^2y+xy+…，无法直接求导，这时如果先固定其中的一个变量，再对另一个变量求极值，则变得可行。 *8.3 EM算法流程*看完数学推导，算法的流程也就十分简单了，这里有两个版本，版本一来自西瓜书，周天使的介绍十分简洁；版本二来自于大牛的博客。结合着数学推导，自认为版本二更具有逻辑性，两者唯一的区别就在于版本二多出了红框的部分 *版本一：* *版本二：* *《机器学习》 学习笔记（10）–集成学习* 上篇主要介绍了鼎鼎大名的EM算法，从算法思想到数学公式推导（边际似然引入隐变量，Jensen不等式简化求导），EM算法实际上可以理解为一种坐标下降法，首先固定一个变量，接着求另外变量的最优解，通过其优美的“两步走”策略能较好地估计隐变量的值。本篇将继续讨论下一类经典算法–集成学习。 *9、集成学习*顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生…，即其泛化性能要能优于其中任何一个学习器。 *9.1 个体与集成*集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示： 在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。 ****同质集成****：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。****异质集成****：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。 上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：****准确性*和*多样性****（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。 现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为： 此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A&#x3D;1 | B&#x3D;1）&gt; P（A&#x3D;1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，****个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量*，准确性高意味着牺牲多样性，所以产生“*好而不同****”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。 *9.2 Boosting*Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。 Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是****指数损失函数*，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，*不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数****：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。 定义基学习器的集成为加权结合，则有： AdaBoost算法的指数损失函数定义为： 具体说来，整个Adaboost 迭代算法分为3步： · 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1&#x2F;N。 · 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。 · 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。 整个AdaBoost的算法流程如下所示： 可以看出：****AdaBoost的核心步骤就是计算基学习器权重和样本权重分布*，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为*大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到****，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开,建议掌握。（见原书：p174-175） Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下： *重赋权法* : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。*重采样法* : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。 从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。 *9.3 Bagging与Random Forest*相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。 *9.3.1 Bagging*Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序，可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。 Bagging算法的流程如下所示： 可以看出Bagging主要通过*样本的扰动*来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：*AdaBoost关注于降低偏差，而Bagging关注于降低方差。* ** *9.3.2 随机森林*随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种****属性扰动****，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K&#x3D;log2（d）。 这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。 *9.4 结合策略*结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略： *9.4.1 平均法（回归问题）**、* 易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，****一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法****。 *9.4.2 投票法（分类问题）* 绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。 一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，****一般基于类概率进行结合往往比基于类标记进行结合的效果更好****，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。 *9.4.3 学习法*学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个mT的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：***投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果****。 *9.5 多样性（diversity）*在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。 ****数据样本扰动****，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。****输入属性扰动****，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。****输出表示扰动****，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。****算法参数扰动****，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。 在此，集成学习就介绍完毕，看到这里，大家也会发现集成学习实质上是一种通用框架，可以使用任何一种基学习器，从而改进单个学习器的泛化性能。据说数据挖掘竞赛KDDCup历年的冠军几乎都使用了集成学习，看来的确是个好东西~ *《机器学习》 学习笔记（11）–聚类* 上篇主要介绍了一种机器学习的通用框架–集成学习方法，首先从准确性和差异性两个重要概念引出集成学习“****好而不同****”的四字真言，接着介绍了现阶段主流的三种集成学习方法：AdaBoost、Bagging及Random Forest，AdaBoost采用最小化指数损失函数迭代式更新样本分布权重和计算基学习器权重，Bagging通过自助采样引入样本扰动增加了基学习器之间的差异性，随机森林则进一步引入了属性扰动，最后简单概述了集成模型中的三类结合策略：平均法、投票法及学习法，其中Stacking是学习法的典型代表。本篇将讨论无监督学习中应用最为广泛的学习算法–聚类。 *10、聚类算法*聚类是一种经典的****无监督学习*方法，*无监督学习的目标是通过对无标记训练样本的学习，发掘和揭示数据集本身潜在的结构与规律****，即不依赖于训练数据集的类标记信息。聚类则是试图将数据集的样本划分为若干个互不相交的类簇，从而每个簇对应一个潜在的类别。 聚类直观上来说是将相似的样本聚在一起，从而形成一个****类簇（cluster）*。那首先的问题是如何来*度量相似性*（similarity measure）呢？这便是*距离度量*，在生活中我们说差别小则相似，对应到多维样本，每个样本可以对应于高维空间中的一个数据点，若它们的距离相近，我们便可以称它们相似。那接着如何来评价聚类结果的好坏呢？这便是*性能度量****，性能度量为评价聚类结果的好坏提供了一系列有效性指标。 *10.1 距离度量*谈及距离度量，最熟悉的莫过于欧式距离了，从年头一直用到年尾的距离计算公式：即对应属性之间相减的平方和再开根号。度量距离还有其它的很多经典方法，通常它们需要满足一些基本性质： 最常用的距离度量方法是****“闵可夫斯基距离”（Minkowski distance)****： 当p&#x3D;1时，闵可夫斯基距离即****曼哈顿距离（Manhattan distance）****： 当p&#x3D;2时，闵可夫斯基距离即****欧氏距离（Euclidean distance）****： 我们知道属性分为两种：****连续属性*和*离散属性****（有限个取值）。对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；而对于离散值的属性，需要作下面进一步的处理： 若属性值之间****存在序关系*，则可以将其转化为连续值，例如：身高属性“高”“中等”“矮”，可转化为{1, 0.5, 0}。若属性值之间*不存在序关系****，则通常将其转化为向量的形式，例如：性别属性“男”“女”，可转化为{（1,0），（0,1）}。 在进行距离度量时，易知****连续属性和存在序关系的离散属性都可以直接参与计算*，因为它们都可以反映一种程度，我们称其为“*有序属性*”；而对于不存在序关系的离散属性，我们称其为：“*无序属性****”，显然无序属性再使用闵可夫斯基距离就行不通了。 ****对于无序属性，我们一般采用VDM进行距离的计算****，例如：对于离散属性的两个取值a和b，定义（p200）： 于是，在计算两个样本之间的距离时，我们可以将闵可夫斯基距离和VDM混合在一起进行计算： 若我们定义的距离计算方法是用来度量相似性，例如下面将要讨论的聚类问题，即距离越小，相似性越大，反之距离越大，相似性越小。这时距离的度量方法并不一定需要满足前面所说的四个基本性质，这样的方法称为：****非度量距离（non-metric distance）****。 *10.2 性能度量*由于聚类算法不依赖于样本的真实类标，就不能像监督学习的分类那般，通过计算分对分错（即精确度或错误率）来评价学习器的好坏或作为学习过程中的优化目标。一般聚类有两类性能度量指标：****外部指标*和*内部指标****。 *10.2.1 外部指标*即将聚类结果与某个参考模型的结果进行比较，****以参考模型的输出作为标准，来评价聚类好坏***。假设聚类给出的结果为λ，参考模型给出的结果是λ，则我们将样本进行两两配对，定义： 显然a和b代表着聚类结果好坏的正能量，b和c则表示参考结果和聚类结果相矛盾，基于这四个值可以导出以下常用的外部评价指标： *10.2.2 内部指标*内部指标即不依赖任何外部模型，直接对聚类的结果进行评估，聚类的目的是想将那些相似的样本尽可能聚在一起，不相似的样本尽可能分开，直观来说：****簇内高内聚紧紧抱团，簇间低耦合老死不相往来****。定义： 基于上面的四个距离，可以导出下面这些常用的内部评价指标： *10.3 原型聚类*原型聚类即“****基于原型的聚类****”（prototype-based clustering），原型表示模板的意思，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程，常见的K-Means便是基于簇中心来实现聚类，混合高斯聚类则是基于簇分布来实现聚类。 *10.3.1 K-Means*K-Means的思想十分简单，****首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛*。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过*EM算法****的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E： K-Means的算法流程如下所示： *10.3.2 学习向量量化（LVQ）*LVQ也是基于原型的聚类算法，与K-Means不同的是，****LVQ使用样本真实类标记辅助聚类*，首先LVQ根据样本的类标记，从各类中分别随机选出一个样本作为该类簇的原型，从而组成了一个*原型特征向量组****，接着从样本集中随机挑选一个样本，计算其与原型向量组中每个向量的距离，并选取距离最小的原型向量所在的类簇作为它的划分结果，再与真实类标比较。 *若划分结果正确，则对应原型向量向这个样本靠近一些**若划分结果不正确，则对应原型向量向这个样本远离一些* LVQ算法的流程如下所示： *10.3.3 高斯混合聚类*现在可以看出K-Means与LVQ都试图以类中心作为原型指导聚类，高斯混合聚类则采用高斯分布来描述原型。现假设****每个类簇中的样本都服从一个多维高斯分布，那么空间中的样本可以看作由k个多维高斯分布混合而成****。 对于多维高斯分布，其概率密度函数如下所示： 其中u表示均值向量，∑表示协方差矩阵，可以看出一个多维高斯分布完全由这两个参数所确定。接着定义高斯混合分布为： α称为混合系数，这样空间中样本的采集过程则可以抽象为：****（1）先选择一个类簇（高斯分布），（2）再根据对应高斯分布的密度函数进行采样****，这时候贝叶斯公式又能大展身手了： 此时只需要选择PM最大时的类簇并将该样本划分到其中，看到这里很容易发现：这和那个传说中的贝叶斯分类不是神似吗，都是通过贝叶斯公式展开，然后计算类先验概率和类条件概率。但遗憾的是：****这里没有真实类标信息，对于类条件概率，并不能像贝叶斯分类那样通过最大似然法计算出来****，因为这里的样本可能属于所有的类簇，这里的似然函数变为： 可以看出：简单的最大似然法根本无法求出所有的参数，这样PM也就没法计算。****这里就要召唤出之前的EM大法，首先对高斯分布的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新****。 高斯混合聚类的算法流程如下图所示： *10.4 密度聚类*密度聚类则是基于密度的聚类，它从样本分布的角度来考察样本之间的可连接性，并基于可连接性（密度可达）不断拓展疆域（类簇）。其中最著名的便是****DBSCAN****算法，首先定义以下概念： 简单来理解DBSCAN便是：****找出一个核心对象所有密度可达的样本集合形成簇****。首先从数据集中任选一个核心对象A，找出所有A密度可达的样本集合，将这些样本形成一个密度相连的类簇，直到所有的核心对象都遍历完。DBSCAN算法的流程如下图所示： *10.5 层次聚类*层次聚类是一种基于树形结构的聚类方法，常用的是****自底向上*的结合策略（*AGNES算法****）。假设有N个待聚类的样本，其基本步骤是： 1.初始化–&gt;把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；2.寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；3.重新计算新生成的这个****类与各个旧类之间的相似度****；4.重复2和3直到所有样本点都归为一类，结束。 可以看出其中最关键的一步就是****计算两个类簇的相似度****，这里有多种度量方法： * 单链接（single-linkage）:取类间最小距离。 * 全链接（complete-linkage）:取类间最大距离 * 均链接（average-linkage）:取类间两两的平均距离 很容易看出：****单链接的包容性极强，稍微有点暧昧就当做是自己人了，全链接则是坚持到底，只要存在缺点就坚决不合并，均连接则是从全局出发顾全大局****。层次聚类法的算法流程如下所示： *《机器学习》 学习笔记（12）–降维与度量学习* 上篇主要介绍了几种常用的聚类算法，首先从距离度量与性能评估出发，列举了常见的距离计算公式与聚类评价指标，接着分别讨论了K-Means、LVQ、高斯混合聚类、密度聚类以及层次聚类算法。K-Means与LVQ都试图以类簇中心作为原型指导聚类，其中K-Means通过EM算法不断迭代直至收敛，LVQ使用真实类标辅助聚类；高斯混合聚类采用高斯分布来描述类簇原型；密度聚类则是将一个核心对象所有密度可达的样本形成类簇，直到所有核心对象都遍历完；最后层次聚类是一种自底向上的树形聚类方法，不断合并最相近的两个小类簇。本篇将讨论机器学习常用的方法–降维与度量学习。 *11、降维与度量学习*样本的特征数称为****维数*（dimensionality），当维数非常大时，也就是现在所说的“*维数灾难*”，具体表现在：在高维情形下，*数据样本将变得十分稀疏*，因为此时要满足训练样本为“*密采样*”的总体样本数目是一个触不可及的天文数字，谓可远观而不可亵玩焉…*训练样本的稀疏使得其代表总体分布的能力大大减弱，从而消减了学习器的泛化能力*；同时当维数很高时，*计算距离也变得十分复杂*，甚至连计算内积都不再容易，这也是为什么支持向量机（SVM）使用核函数*“低维计算，高维表现”****的原因。 缓解维数灾难的一个重要途径就是****降维，即通过某种数学变换将原始高维空间转变到一个低维的子空间*。在这个子空间中，样本的密度将大幅提高，同时距离计算也变得容易。这时也许会有疑问，这样降维之后不是会丢失原始数据的一部分信息吗？这是因为在很多实际的问题中，虽然训练数据是高维的，但是与学习任务相关也许仅仅是其中的一个低维子空间，也称为一个*低维嵌入*，例如：数据属性中存在噪声属性、相似属性或冗余属性等，*对高维数据进行降维能在一定程度上达到提炼低维优质属性或降噪的效果****。 *11.1 K近邻学习*k近邻算法简称****kNN（k-Nearest Neighbor）*，是一种经典的监督学习方法，同时也实力担当入选数据挖掘十大算法。其工作机制十分简单粗暴：给定某个测试样本，kNN基于某种*距离度量****在训练集中找出与其距离最近的k个带有真实标记的训练样本，然后给基于这k个邻居的真实标记来进行预测，类似于前面集成学习中所讲到的基学习器结合策略：分类任务采用投票法，回归任务则采用平均法。接下来本篇主要就kNN分类进行讨论。 从左图中我们可以看到，图中有两种类型的样本，一类是蓝色正方形，另一类是红色三角形。而那个绿色圆形是我们待分类的样本。基于kNN算法的思路，我们很容易得到以下结论： 如果K&#x3D;3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。如果K&#x3D;5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。 可以发现：****kNN虽然是一种监督学习方法，但是它却没有显式的训练过程*，而是当有新样本需要预测时，才来计算出最近的k个邻居，因此*kNN是一种典型的懒惰学习方法*，再来回想一下朴素贝叶斯的流程，训练的过程就是参数估计，因此朴素贝叶斯也可以懒惰式学习，此类技术在*训练阶段开销为零*，待收到测试样本后再进行计算。相应地我们称那些一有训练数据立马开工的算法为“*急切学习****”，可见前面我们学习的大部分算法都归属于急切学习。 很容易看出：****kNN算法的核心在于k值的选取以及距离的度量*。k值选取太小，模型很容易受到噪声数据的干扰，例如：极端地取k&#x3D;1，若待分类样本正好与一个噪声数据距离最近，就导致了分类错误；若k值太大， 则在更大的邻域内进行投票，此时模型的预测能力大大减弱，例如：极端取k&#x3D;训练样本数，就相当于模型根本没有学习，所有测试样本的预测结果都是一样的。*一般地我们都通过交叉验证法来选取一个适当的k值****。 对于距离度量，****不同的度量方法得到的k个近邻不尽相同，从而对最终的投票结果产生了影响*，因此选择一个合适的距离度量方法也十分重要。在上一篇聚类算法中，在度量样本相似性时介绍了常用的几种距离计算方法，包括*闵可夫斯基距离，曼哈顿距离，VDM*等。在实际应用中，*kNN的距离度量函数一般根据样本的特性来选择合适的距离度量，同时应对数据进行去量纲&#x2F;归一化处理来消除大量纲属性的强权政治影响****。 *11.2 MDS算法*不管是使用核函数升维还是对数据降维，我们都希望*原始空间样本点之间的距离在新空间中基本保持不变*，这样才不会使得原始空间样本之间的关系及总体分布发生较大的改变。*“多维缩放”（MDS**： multiple Dimession Scaling* ****）*正是基于这样的思想，*MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持****。 假定m个样本在原始空间中任意两两样本之间的距离矩阵为D∈R(mm)，我们的目标便是获得样本在低维空间中的表示Z∈R(d’m , d’&lt; d)，且任意两个样本在低维空间中的欧式距离等于原始空间中的距离，即||zi-zj||&#x3D;Dist(ij)。因此接下来我们要做的就是根据已有的距离矩阵D来求解出降维后的坐标矩阵Z。 令降维后的样本坐标矩阵Z被中心化，****中心化是指将每个样本向量减去整个样本集的均值向量，故所有样本向量求和得到一个零向量****。这样易知：矩阵B的每一列以及每一列求和均为0，因为提取公因子后都有一项为所有样本向量的和向量。 根据上面矩阵B的特征，我们很容易得到等式（2）、（3）以及（4）： 这时根据(1)–(4)式我们便可以计算出bij，即*bij&#x3D;(1)-(2)***(1&#x2F;m)-(3)********(1&#x2F;m)+(4)*(1&#x2F;(m^2))***，再逐一地计算每个b(ij)，就得到了降维后低维空间中的内积矩阵B(B&#x3D;Z’Z)，只需对B进行特征值分解便可以得到Z。MDS的算法流程如下图所示： *11.3 主成分分析（PCA）*不同于MDS采用距离保持的方法，*主成分分析（PCA**: Principle Component Analysis**）直接通过一个线性变换，将原始空间中的样本投影到新的低维空间中*。简单来理解这一过程便是：*PCA采用一组新的基来表示样本点，其中每一个基向量都是原来基向量的线性组合，通过使用尽可能少的新基向量来表出样本，从而达到降维的目的。* 假设使用d’个新基向量来表示原来样本，实质上是将样本投影到一个由d’个基向量确定的一个****超平面*上（*即舍弃了一些维度*），要用一个超平面对空间中所有高维样本进行恰当的表达，最理想的情形是：*若这些样本点都能在超平面上表出且这些表出在超平面上都能够很好地分散开来****。但是一般使用较原空间低一些维度的超平面来做到这两点十分不容易，因此我们退一步海阔天空，要求这个超平面应具有如下两个性质： ****最近重构性****：样本点到超平面的距离足够近，即尽可能在超平面附近；****最大可分性****：样本点在超平面上的投影尽可能地分散开来，即投影后的坐标具有区分性。 这里十分神奇的是：****最近重构性与最大可分性虽然从不同的出发点来定义优化问题中的目标函数，但最终这两种特性得到了完全相同的优化问题****： 接着使用拉格朗日乘子法求解上面的优化问题，得到： 因此只需对协方差矩阵进行特征值分解即可求解出W，PCA算法的整个流程如下图所示： 一篇博客给出更通俗更详细的理解：http://blog.csdn.net/u011826404/article/details/57472730 *11.4 核化线性降维*说起机器学习你中有我&#x2F;我中有你&#x2F;水乳相融…在这里能够得到很好的体现。正如SVM在处理非线性可分时，通过引入核函数将样本投影到高维特征空间，接着在高维空间再对样本点使用超平面划分。这里也是相同的问题：若我们的样本数据点本身就不是线性分布，那还如何使用一个超平面去近似表出呢？因此也就引入了核函数，****即先将样本映射到高维空间，再在高维空间中使用线性降维的方法*。下面主要介绍*核化主成分分析（KPCA）****的思想。 若核函数的形式已知，即我们知道如何将低维的坐标变换为高维坐标，这时我们只需先将数据映射到高维特征空间，再在高维空间中运用PCA即可。但是一般情况下，我们并不知道核函数具体的映射规则，例如：Sigmoid、高斯核等，我们只知道如何计算高维空间中的样本内积，这时就引出了KPCA的一个重要创新之处：****即空间中的任一向量，都可以由该空间中的所有样本线性表示****。证明过程也十分简单： 这样我们便可以将高维特征空间中的投影向量wi使用所有高维样本点线性表出，接着代入PCA的求解问题，得到： 化简到最后一步，发现结果十分的美妙，只需对核矩阵K进行特征分解，便可以得出投影向量wi对应的系数向量α，因此选取特征值前d’大对应的特征向量便是d’个系数向量。这时对于需要降维的样本点，只需按照以下步骤便可以求出其降维后的坐标。可以看出：KPCA在计算降维后的坐标表示时，需要与所有样本点计算核函数值并求和，因此该算法的计算开销十分大。 *11.5 流形学习*****流形学习（manifold learning）是一种借助拓扑流形概念的降维方法*，*流形是指在局部与欧式空间同胚的空间*，即在局部与欧式空间具有相同的性质，能用欧氏距离计算样本之间的距离。这样即使高维空间的分布十分复杂，但是在局部上依然满足欧式空间的性质，基于流形学习的降维正是这种*“邻域保持”*的思想。其中*等度量映射（Isomap）试图在降维前后保持邻域内样本之间的距离，而局部线性嵌入（LLE）则是保持邻域内样本之间的线性关系****，下面将分别对这两种著名的流行学习方法进行介绍。 *11.5.1 等度量映射（Isomap）*等度量映射的基本出发点是：高维空间中的直线距离具有误导性，因为有时高维空间中的直线距离在低维空间中是不可达的。****因此利用流形在局部上与欧式空间同胚的性质，可以使用近邻距离来逼近测地线距离*，即对于一个样本点，它与近邻内的样本点之间是可达的，且距离使用欧式距离计算，这样整个样本空间就形成了一张近邻图，高维空间中两个样本之间的距离就转为最短路径问题。可采用著名的*Dijkstra算法*或*Floyd算法****计算最短距离，得到高维空间中任意两点之间的距离后便可以使用MDS算法来其计算低维空间中的坐标。 从MDS算法的描述中我们可以知道：MDS先求出了低维空间的内积矩阵B，接着使用特征值分解计算出了样本在低维空间中的坐标，但是并没有给出通用的投影向量w，因此对于需要降维的新样本无从下手，给出的权宜之计是利用已知高&#x2F;低维坐标的样本作为训练集学习出一个“投影器”，便可以用高维坐标预测出低维坐标。Isomap算法流程如下图： 对于近邻图的构建，常用的有两种方法：****一种是指定近邻点个数*，像kNN一样选取k个最近的邻居；*另一种是指定邻域半径****，距离小于该阈值的被认为是它的近邻点。但两种方法均会出现下面的问题： 若****邻域范围指定过大，则会造成“短路问题”*，即本身距离很远却成了近邻，将距离近的那些样本扼杀在摇篮。若*邻域范围指定过小，则会造成“断路问题”****，即有些样本点无法可达了，整个世界村被划分为互不可达的小部落。 *11.5.2 局部线性嵌入(LLE)*不同于Isomap算法去保持邻域距离，LLE算法试图去保持邻域内的线性关系，假定样本xi的坐标可以通过它的邻域样本线性表出： LLE算法分为两步走，****首先第一步根据近邻关系计算出所有样本的邻域重构系数w****： ****接着根据邻域重构系数不变，去求解低维坐标****： 这样利用矩阵M，优化问题可以重写为： M特征值分解后最小的d’个特征值对应的特征向量组成Z，LLE算法的具体流程如下图所示： *11.6 度量学习*本篇一开始就提到维数灾难，即在高维空间进行机器学习任务遇到样本稀疏、距离难计算等诸多的问题，因此前面讨论的降维方法都试图将原空间投影到一个合适的低维空间中，接着在低维空间进行学习任务从而产生较好的性能。事实上，不管高维空间还是低维空间都潜在对应着一个距离度量，那可不可以直接学习出一个距离度量来等效降维呢？例如：****咋们就按照降维后的方式来进行距离的计算，这便是度量学习的初衷****。 ****首先要学习出距离度量必须先定义一个合适的距离度量形式****。对两个样本xi与xj，它们之间的平方欧式距离为： 若各个属性重要程度不一样即都有一个权重，则得到加权的平方欧式距离： 此时各个属性之间都是相互独立无关的，但现实中往往会存在属性之间有关联的情形，例如：身高和体重，一般人越高，体重也会重一些，他们之间存在较大的相关性。这样计算距离就不能分属性单独计算，于是就引入经典的*马氏距离(Mahalanobis distance)*: ****标准的马氏距离中M是协方差矩阵的逆，马氏距离是一种考虑属性之间相关性且尺度无关（即无须去量纲）的距离度量****。 ****矩阵M也称为“度量矩阵”，为保证距离度量的非负性与对称性，M必须为(半)正定对称矩阵*，这样就为度量学习定义好了距离度量的形式，换句话说：*度量学习便是对度量矩阵进行学习*。现在来回想一下前面我们接触的机器学习不难发现：*机器学习算法几乎都是在优化目标函数，从而求解目标函数中的参数****。同样对于度量学习，也需要设置一个优化目标，书中简要介绍了错误率和相似性两种优化目标，此处限于篇幅不进行展开。 在此，降维和度量学习就介绍完毕。*降维是将原高维空间嵌入到一个合适的低维子空间中，接着在低维空间中进行学习任务；度量学习则是试图去学习出一个距离度量来等效降维的效果*，两者都是为了解决维数灾难带来的诸多问题。也许大家最后心存疑惑，那kNN呢，为什么一开头就说了kNN算法，但是好像和后面没有半毛钱关系？正是因为在降维算法中，低维子空间的维数d’通常都由人为指定，因此我们需要使用一些低开销的学习器来选取合适的d’，*kNN这家伙懒到家了根本无心学习，在训练阶段开销为零，测试阶段也只是遍历计算了距离，因此拿kNN来进行交叉验证就十分有优势了~同时降维后样本密度增大同时距离计算变易，更为kNN来展示它独特的十八般手艺提供了用武之地。* *《机器学习》 学习笔记（13）–特征选择与稀疏学习* 上篇主要介绍了经典的降维方法与度量学习，首先从“维数灾难”导致的样本稀疏以及距离难计算两大难题出发，引出了降维的概念，即通过某种数学变换将原始高维空间转变到一个低维的子空间，接着分别介绍了kNN、MDS、PCA、KPCA以及两种经典的流形学习方法，k近邻算法的核心在于k值的选取以及距离的度量，MDS要求原始空间样本之间的距离在降维后的低维空间中得以保持，主成分分析试图找到一个低维超平面来表出原空间样本点，核化主成分分析先将样本点映射到高维空间，再在高维空间中使用线性降维的方法，从而解决了原空间样本非线性分布的情形，基于流形学习的降维则是一种“邻域保持”的思想，最后度量学习试图去学习出一个距离度量来等效降维的效果。本篇将讨论另一种常用方法–特征选择与稀疏学习。 *12、特征选择与稀疏学习*对于数据集中的一个对象及组成对象的零件元素： 统计学家常称它们为*观测*（*observation*）和*变量*（*variable*）；数据库分析师则称其为*记录*（*record*）和*字段*（*field*）；数据挖掘&#x2F;机器学习学科的研究者则习惯把它们叫做*样本*&#x2F;*示例*（*example*&#x2F;*instance*）和*属性*&#x2F;*特征*（*attribute*&#x2F;****feature****）。 回归正题，在机器学习中特征选择是一个重要的“*数据预处理*”（*data* ****preprocessing****）过程，即试图从数据集的所有特征中挑选出与当前学习任务相关的特征子集，接着再利用数据子集来训练学习器；稀疏学习则是围绕着稀疏矩阵的优良性质，来完成相应的学习任务。 *12.1 子集搜索与评价*一般地，我们可以用很多属性&#x2F;特征来描述一个示例，例如对于一个人可以用性别、身高、体重、年龄、学历、专业等属性来描述，那现在想要训练出一个学习器来预测人的收入。根据生活经验易知：并不是所有的特征都与学习任务相关，例如年龄&#x2F;学历&#x2F;专业可能很大程度上影响了收入，身高&#x2F;体重这些外貌属性也有较小的可能性影响收入。因此我们只需要那些与学习任务紧密相关的特征，****特征选择便是从给定的特征集合中选出相关特征子集的过程****。 与上篇中降维技术有着异曲同工之处的是，特征选择也可以有效地解决维数灾难的难题。具体而言：****降维从一定程度起到了提炼优质低维属性和降噪的效果，特征选择则是直接剔除那些与学习任务无关的属性而选择出最佳特征子集*。若直接遍历所有特征子集，显然当维数过多时遭遇指数爆炸就行不通了；若采取从候选特征子集中不断迭代生成更优候选子集的方法，则时间复杂度大大减小。这时就涉及到了两个关键环节：*1.如何生成候选子集；2.如何评价候选子集的好坏****，这便是早期特征选择的常用方法。书本上介绍了贪心算法，分为三种策略： ****前向搜索****：初始将每个特征当做一个候选特征子集，然后从当前所有的候选子集中选择出最佳的特征子集；接着在上一轮选出的特征子集中添加一个新的特征，同样地选出最佳特征子集；最后直至选不出比上一轮更好的特征子集。****后向搜索****：初始将所有特征作为一个候选特征子集；接着尝试去掉上一轮特征子集中的一个特征并选出当前最优的特征子集；最后直到选不出比上一轮更好的特征子集。****双向搜索****：将前向搜索与后向搜索结合起来，即在每一轮中既有添加操作也有剔除操作。 对于特征子集的评价，书中给出了一些想法及基于信息熵的方法。假设数据集的属性皆为离散属性，这样给定一个特征子集，便可以通过这个特征子集的取值将数据集合划分为V个子集。例如：A1&#x3D;{男,女}，A2&#x3D;{本科,硕士}就可以将原数据集划分为2*2&#x3D;4个子集，其中每个子集的取值完全相同。这时我们就可以像决策树选择划分属性那样，通过计算信息增益来评价该属性子集的好坏。 此时，信息增益越大表示该属性子集包含有助于分类的特征越多，使用上述这种****子集搜索与子集评价相结合的机制，便可以得到特征选择方法****。值得一提的是若将前向搜索策略与信息增益结合在一起，与前面我们讲到的ID3决策树十分地相似。事实上，决策树也可以用于特征选择，树节点划分属性组成的集合便是选择出的特征子集。 *12.2 过滤式选择（Relief）*过滤式方法是一种将特征选择与学习器训练相分离的特征选择技术，即首先将相关特征挑选出来，再使用选择出的数据子集来训练学习器。Relief是其中著名的代表性算法，它使用一个“****相关统计量****”来度量特征的重要性，该统计量是一个向量，其中每个分量代表着相应特征的重要性，因此我们最终可以根据这个统计量各个分量的大小来选择出合适的特征子集。 易知Relief算法的核心在于如何计算出该相关统计量。对于数据集中的每个样例xi，Relief首先找出与xi同类别的最近邻与不同类别的最近邻，分别称为****猜中近邻（near-hit）*与*猜错近邻（near-miss）****，接着便可以分别计算出相关统计量中的每个分量。对于j分量： 直观上理解：对于猜中近邻，两者j属性的距离越小越好，对于猜错近邻，j属性距离越大越好。更一般地，若xi为离散属性，diff取海明距离，即相同取0，不同取1；若xi为连续属性，则diff为曼哈顿距离，即取差的绝对值。分别计算每个分量，最终取平均便得到了整个相关统计量。 标准的Relief算法只用于二分类问题，后续产生的拓展变体Relief-F则解决了多分类问题。对于j分量，新的计算公式如下： 其中pl表示第l类样本在数据集中所占的比例，易知两者的不同之处在于：****标准Relief 只有一个猜错近邻，而Relief-F有多个猜错近邻****。 *12.3 包裹式选择（LVW）*与过滤式选择不同的是，包裹式选择将后续的学习器也考虑进来作为特征选择的评价准则。因此包裹式选择可以看作是为某种学习器****量身定做*的特征选择方法，由于在每一轮迭代中，包裹式选择都需要训练学习器，因此在获得较好性能的同时也产生了较大的开销。下面主要介绍一种经典的包裹式特征选择方法 –LVW（Las Vegas Wrapper）*，它在拉斯维加斯框架下使用随机策略来进行特征子集的搜索。怀着好奇科普一下，结果又顺带了一个赌场： ****蒙特卡罗算法****：采样越多，越近似最优解，一定会给出解，但给出的解不一定是正确解；****拉斯维加斯算法****：采样越多，越有机会找到最优解，不一定会给出解，且给出的解一定是正确解。 举个例子，假如筐里有100个苹果，让我每次闭眼拿1个，挑出最大的。于是我随机拿1个，再随机拿1个跟它比，留下大的，再随机拿1个……我每拿一次，留下的苹果都至少不比上次的小。拿的次数越多，挑出的苹果就越大，但我除非拿100次，否则无法肯定挑出了最大的。这个挑苹果的算法，就属于蒙特卡罗算法——尽量找较好的，但不保证是最好的。 而拉斯维加斯算法，则是另一种情况。假如有一把锁，给我100把钥匙，只有1把是对的。于是我每次随机拿1把钥匙去试，打不开就再换1把。我试的次数越多，打开（正确解）的机会就越大，但在打开之前，那些错的钥匙都是没有用的。这个试钥匙的算法，就是拉斯维加斯的——尽量找最好的，但不保证能找到。 LVW算法的具体流程如下所示，其中比较特别的是停止条件参数T的设置，即在每一轮寻找最优特征子集的过程中，若随机T次仍没找到，算法就会停止，从而保证了算法运行时间的可行性。 *12.4 嵌入式选择与正则化*前面提到了的两种特征选择方法：****过滤式中特征选择与后续学习器完全分离，包裹式则是使用学习器作为特征选择的评价准则；嵌入式是一种将特征选择与学习器训练完全融合的特征选择方法，即将特征选择融入学习器的优化过程中*。在之前《经验风险与结构风险》中已经提到：经验风险指的是模型与训练数据的契合度，结构风险则是模型的复杂程度，机器学习的核心任务就是：*在模型简单的基础上保证模型的契合度****。例如：岭回归就是加上了L2范数的最小二乘法，有效地解决了奇异矩阵、过拟合等诸多问题，下面的嵌入式特征选择则是在损失函数后加上了L1范数。 L1范数美名又约****Lasso Regularization****，指的是向量中每个元素的绝对值之和，这样在优化目标函数的过程中，就会使得w尽可能地小，在一定程度上起到了防止过拟合的作用，同时与L2范数（Ridge Regularization ）不同的是，L1范数会使得部分w变为0， 从而达到了特征选择的效果。 总的来说：****L1范数会趋向产生少量的特征，其他特征的权值都是0；L2会选择更多的特征，这些特征的权值都会接近于0****。这样L1范数在特征选择上就十分有用，而L2范数则具备较强的控制过拟合能力。可以从下面两个方面来理解： （1）****下降速度****：L1范数按照绝对值函数来下降，L2范数按照二次函数来下降。因此在0附近，L1范数的下降速度大于L2范数，故L1范数能很快地下降到0，而L2范数在0附近的下降速度非常慢，因此较大可能收敛在0的附近。 （2）****空间限制****：L1范数与L2范数都试图在最小化损失函数的同时，让权值W也尽可能地小。我们可以将原优化问题看做为下面的问题，即让后面的规则则都小于某个阈值。这样从图中可以看出：L1范数相比L2范数更容易得到稀疏解。 *12.5 稀疏表示与字典学习*当样本数据是一个稀疏矩阵时，对学习任务来说会有不少的好处，例如很多问题变得线性可分，储存更为高效等。这便是稀疏表示与字典学习的基本出发点。稀疏矩阵即矩阵的每一行&#x2F;列中都包含了大量的零元素，且这些零元素没有出现在同一行&#x2F;列，对于一个给定的稠密矩阵，若我们能****通过某种方法找到其合适的稀疏表示*，则可以使得学习任务更加简单高效，我们称之为*稀疏编码（sparse coding）*或*字典学习（dictionary learning）****。 给定一个数据集，字典学习&#x2F;稀疏编码指的便是通过一个字典将原数据转化为稀疏表示，因此最终的目标就是求得字典矩阵B及稀疏表示α，书中使用变量交替优化的策略能较好地求得解，深感陷进去短时间无法自拔，故先不进行深入… *12.6 压缩感知*压缩感知与特征选择、稀疏表示不同的是：它关注的是通过欠采样信息来恢复全部信息。在实际问题中，为了方便传输和存储，我们一般将数字信息进行压缩，这样就有可能损失部分信息，如何根据已有的信息来重构出全部信号，这便是压缩感知的来历，压缩感知的前提是已知的信息具有稀疏表示。下面是关于压缩感知的一些背景： 在很多实际情形中，选了好的特征比选了好的模型更为重要. *《机器学习》 学习笔记（14）–计算学习理论* 上篇主要介绍了常用的特征选择方法及稀疏学习。首先从相关&#x2F;无关特征出发引出了特征选择的基本概念，接着分别介绍了子集搜索与评价、过滤式、包裹式以及嵌入式四种类型的特征选择方法。子集搜索与评价使用的是一种优中生优的贪婪算法，即每次从候选特征子集中选出最优子集；过滤式方法计算一个相关统计量来评判特征的重要程度；包裹式方法将学习器作为特征选择的评价准则；嵌入式方法则是通过L1正则项将特征选择融入到学习器参数优化的过程中。最后介绍了稀疏表示与压缩感知的核心思想：稀疏表示利用稀疏矩阵的优良性质，试图通过某种方法找到原始稠密矩阵的合适稀疏表示；压缩感知则试图利用可稀疏表示的欠采样信息来恢复全部信息。本篇将讨论一种为机器学习提供理论保证的学习方法–计算学习理论。 *13、计算学习理论*计算学习理论（computational learning theory）是通过“计算”来研究机器学习的理论，简而言之，其目的是分析学习任务的本质，例如：****在什么条件下可进行有效的学习，需要多少训练样本能获得较好的精度等，从而为机器学习算法提供理论保证****。 首先来谈谈经验误差和泛化误差。假设给定训练集D，其中所有的训练样本都服从一个未知的分布T，且它们都是在总体分布T中独立采样得到，即****独立同分布****（independent and identically distributed，i.i.d.），在《贝叶斯分类器》中我们已经提到：独立同分布是很多统计学习算法的基础假设，例如最大似然法，贝叶斯分类器，高斯混合聚类等，简单来理解独立同分布：每个样本都是从总体分布中独立采样得到水。 ****泛化误差指的是学习器在总体上的预测误差，经验误差则是学习器在某个特定数据集D上的预测误差****。在实际问题中，往往我们并不能得到总体且数据集D是通过独立同分布采样得到的，因此我们常常使用经验误差作为泛化误差的近似。 *13.1 PAC学习*****对于机器学习算法，学习器也正是为了寻找合适的映射规则*，即如何从条件属性得到目标属性。从样本空间到标记空间存在着很多的映射，我们将每个映射称之为*概念****（concept），定义： 若概念c对任何样本x满足c(x)&#x3D;y，则称c为****目标概念*，即最理想的映射，所有的目标概念构成的集合称为*“概念类”*；给定学习算法，它所有可能映射&#x2F;概念的集合称为*“假设空间”*，其中单个的概念称为*“假设”*（hypothesis）；若一个算法的假设空间包含目标概念，则称该数据集对该算法是*可分*（separable）的，亦称*一致*（consistent）的；若一个算法的假设空间不包含目标概念，则称该数据集对该算法是*不可分*（non-separable）的，或称*不一致****（non-consistent）的。 举个简单的例子：对于非线性分布的数据集，若使用一个线性分类器，则该线性分类器对应的假设空间就是空间中所有可能的超平面，显然假设空间不包含该数据集的目标概念，所以称数据集对该学习器是不可分的。给定一个数据集D，我们希望模型学得的假设h尽可能地与目标概念一致，这便是*概率近似正确*(Probably Approximately Correct，简称PAC)的来源，即以较大的概率学得模型满足误差的预设上限。 上述关于PAC的几个定义层层相扣：定义12.1表达的是对于某种学习算法，如果能以一个置信度学得假设满足泛化误差的预设上限，则称该算法能PAC辨识概念类，即该算法的输出假设已经十分地逼近目标概念。定义12.2则将样本数量考虑进来，当样本超过一定数量时，学习算法总是能PAC辨识概念类，则称概念类为PAC可学习的。定义12.3将学习器运行时间也考虑进来，若运行时间为多项式时间，则称PAC学习算法。 显然，PAC学习中的一个关键因素就是****假设空间的复杂度*，对于某个学习算法，*若假设空间越大，则其中包含目标概念的可能性也越大，但同时找到某个具体概念的难度也越大****，一般假设空间分为有限假设空间与无限假设空间。 *13.2 有限假设空间**13.2.1 可分情形*可分或一致的情形指的是：****目标概念包含在算法的假设空间中*。对于目标概念，在训练集D中的经验误差一定为0，因此首先我们可以想到的是：不断地剔除那些出现预测错误的假设，直到找到经验误差为0的假设即为目标概念。但*由于样本集有限，可能会出现多个假设在D上的经验误差都为0，因此问题转化为：需要多大规模的数据集D才能让学习算法以置信度的概率从这些经验误差都为0的假设中找到目标概念的有效近似****。 通过上式可以得知：*对于可分情形的有限假设空间，目标概念都是PAC可学习的，即当样本数量满足上述条件之后，在与训练集一致的假设中总是可以在1-σ概率下找到目标概念的有效近似。* *13.2.2 不可分情形*不可分或不一致的情形指的是：*目标概念不存在于假设空间中*，这时我们就不能像可分情形时那样从假设空间中寻找目标概念的近似。但*当假设空间给定时，必然存一个假设的泛化误差最小，若能找出此假设的有效近似也不失为一个好的目标，这便是不可知学习(agnostic learning)的来源。* 这时候便要用到****Hoeffding不等式****： 对于假设空间中的所有假设，出现泛化误差与经验误差之差大于e的概率和为： 因此，可令不等式的右边小于（等于）σ，便可以求出满足泛化误差与经验误差相差小于e所需的最少样本数，同时也可以求出泛化误差界。 *13.3 VC维*现实中的学习任务通常都是无限假设空间，例如d维实数域空间中所有的超平面等，因此要对此种情形进行可学习研究，需要度量****假设空间的复杂度*。这便是*VC维****（Vapnik-Chervonenkis dimension）的来源。在介绍VC维之前，需要引入两个概念： *增长函数*：对于给定数据集D，假设空间中的每个假设都能对数据集的样本赋予标记，因此一个假设对应着一种打标结果，不同假设对D的打标结果可能是相同的，也可能是不同的。随着样本数量m的增大，假设空间对样本集D的打标结果也会增多，增长函数则表示假设空间对m个样本的数据集D打标的最大可能结果数，因此*增长函数描述了假设空间的表示能力与复杂度。* ****打散*：例如对二分类问题来说，m个样本最多有2^m个可能结果，每种可能结果称为一种*“对分”****，若假设空间能实现数据集D的所有对分，则称数据集能被该假设空间打散。 ****因此尽管假设空间是无限的，但它对特定数据集打标的不同结果数是有限的，假设空间的VC维正是它能打散的最大数据集大小****。通常这样来计算假设空间的VC维：若存在大小为d的数据集能被假设空间打散，但不存在任何大小为d+1的数据集能被假设空间打散，则其VC维为d。 同时给出了假设空间VC维与增长函数的两个关系： 直观来理解（1）式也十分容易： 首先假设空间的VC维是d，说明当m&lt;&#x3D;d时，增长函数与2^m相等，例如：当m&#x3D;d时，右边的组合数求和刚好等于2^d；而当m&#x3D;d+1时，右边等于2^(d+1)-1，十分符合VC维的定义，同时也可以使用数学归纳法证明；（2）式则是由（1）式直接推导得出。 在有限假设空间中，根据Hoeffding不等式便可以推导得出学习算法的泛化误差界；但在无限假设空间中，由于假设空间的大小无法计算，只能通过增长函数来描述其复杂度，因此无限假设空间中的泛化误差界需要引入增长函数。 上式给出了基于VC维的泛化误差界，同时也可以计算出满足条件需要的样本数（样本复杂度）。若学习算法满足*经验风险最小化原则（ERM）*，即学习算法的输出假设h在数据集D上的经验误差最小，可证明：*任何VC维有限的假设空间都是（不可知）PAC可学习的，换而言之：若假设空间的最小泛化误差为0即目标概念包含在假设空间中，则是PAC可学习，若最小泛化误差不为0，则称为不可知PAC可学习。* *13.4 稳定性*稳定性考察的是当算法的输入发生变化时，输出是否会随之发生较大的变化，输入的数据集D有以下两种变化： 若对数据集中的任何样本z，满足： 即原学习器和剔除一个样本后生成的学习器对z的损失之差保持β稳定，称学习器关于损失函数满足****β-均匀稳定性****。同时若损失函数有上界，即原学习器对任何样本的损失函数不超过M，则有如下定理： 事实上，****若学习算法符合经验风险最小化原则（ERM）且满足β-均匀稳定性，则假设空间是可学习的****。稳定性通过损失函数与假设空间的可学习联系在了一起，区别在于：假设空间关注的是经验误差与泛化误差，需要考虑到所有可能的假设；而稳定性只关注当前的输出假设。 《机器学习》 学习笔记（15）–半监督学习 上篇主要介绍了机器学习的理论基础，首先从独立同分布引入泛化误差与经验误差，接着介绍了PAC可学习的基本概念，即以较大的概率学习出与目标概念近似的假设（泛化误差满足预设上限），对于有限假设空间：（1）可分情形时，假设空间都是PAC可学习的，即当样本满足一定的数量之后，总是可以在与训练集一致的假设中找出目标概念的近似；（2）不可分情形时，假设空间都是不可知PAC可学习的，即以较大概率学习出与当前假设空间中泛化误差最小的假设的有效近似（Hoeffding不等式）。对于无限假设空间，通过增长函数与VC维来描述其复杂度，若学习算法满足经验风险最小化原则，则任何VC维有限的假设空间都是（不可知）PAC可学习的，同时也给出了泛化误差界与样本复杂度。稳定性则考察的是输入发生变化时输出的波动，稳定性通过损失函数与假设空间的可学习理论联系在了一起。本篇将讨论一种介于监督与非监督学习之间的学习算法–半监督学习。 *14、半监督学习*前面我们一直围绕的都是监督学习与无监督学习，监督学习指的是训练样本包含标记信息的学习任务，例如：常见的分类与回归算法；无监督学习则是训练样本不包含标记信息的学习任务，例如：聚类算法。在实际生活中，常常会出现一部分样本有标记和较多样本无标记的情形，例如：做网页推荐时需要让用户标记出感兴趣的网页，但是少有用户愿意花时间来提供标记。若直接丢弃掉无标记样本集，使用传统的监督学习方法，常常会由于训练样本的不充足，使得其刻画总体分布的能力减弱，从而影响了学习器泛化性能。那如何利用未标记的样本数据呢？ 一种简单的做法是通过专家知识对这些未标记的样本进行打标，但随之而来的就是巨大的人力耗费。若我们先使用有标记的样本数据集训练出一个学习器，再基于该学习器对未标记的样本进行预测，从中****挑选出不确定性高或分类置信度低的样本来咨询专家并进行打标*，最后使用扩充后的训练集重新训练学习器，这样便能大幅度降低标记成本，这便是*主动学习*（active learning），其目标是*使用尽量少的&#x2F;有价值的咨询来获得更好的性能****。 显然，****主动学习需要与外界进行交互&#x2F;查询&#x2F;打标，其本质上仍然属于一种监督学习*。事实上，无标记样本虽未包含标记信息，但它们与有标记样本一样都是从总体中独立同分布采样得到，因此*它们所包含的数据分布信息对学习器的训练大有裨益*。如何让学习过程不依赖外界的咨询交互，自动利用未标记样本所包含的分布信息的方法便是*半监督学习*（semi-supervised learning），*即训练集同时包含有标记样本数据和未标记样本数据****。 此外，半监督学习还可以进一步划分为****纯半监督学习*和*直推学习****，两者的区别在于：前者假定训练数据集中的未标记数据并非待预测数据，而后者假定学习过程中的未标记数据就是待预测数据。主动学习、纯半监督学习以及直推学习三者的概念如下图所示： *14.1 生成式方法*****生成式方法*（generative methods）是基于生成式模型的方法，即先对联合分布P（x,c）建模，从而进一步求解 P（c | x），*此类方法假定样本数据服从一个潜在的分布，因此需要充分可靠的先验知识****。例如：前面已经接触到的贝叶斯分类器与高斯混合聚类，都属于生成式模型。现假定总体是一个高斯混合分布，即由多个高斯分布组合形成，从而一个子高斯分布就代表一个类簇（类别）。高斯混合分布的概率密度函数如下所示： 不失一般性，假设类簇与真实的类别按照顺序一一对应，即第i个类簇对应第i个高斯混合成分。与高斯混合聚类类似地，这里的主要任务也是估计出各个高斯混合成分的参数以及混合系数，不同的是：对于有标记样本，不再是可能属于每一个类簇，而是只能属于真实类标对应的特定类簇。 直观上来看，****基于半监督的高斯混合模型有机地整合了贝叶斯分类器与高斯混合聚类的核心思想****，有效地利用了未标记样本数据隐含的分布信息，从而使得参数的估计更加准确。同样地， 用EM进行求解，首先对各个高斯混合成分的参数及混合系数进行随机初始化，计算出各个PM（即γji，第i个样本属于j类，有标记样本则直接属于特定类），再最大化似然函数（即LL（D）分别对α、u和∑求偏导 ），对参数进行迭代更新。 当参数迭代更新收敛后，对于待预测样本x，便可以像贝叶斯分类器那样计算出样本属于每个类簇的后验概率，接着找出概率最大的即可： 可以看出：基于生成式模型的方法十分依赖于对潜在数据分布的假设，即假设的分布要能和真实分布相吻合，否则利用未标记的样本数据反倒会在错误的道路上渐行渐远，从而降低学习器的泛化性能。 *14.2 半监督SVM*监督学习中的SVM（Semi-Supervised Support Vector Machine）试图找到一个划分超平面，使得两侧支持向量之间的间隔最大，即“****最大划分间隔****”思想。对于半监督学习，S3VM则考虑超平面需穿过数据低密度的区域。TSVM是半监督支持向量机中的最著名代表，其核心思想是：尝试为未标记样本找到合适的标记指派，使得超平面划分后的间隔最大化。TSVM(Transductive SVM)采用局部搜索的策略来进行迭代求解，即首先使用有标记样本集训练出一个初始SVM，接着使用该学习器对未标记样本进行打标，这样所有样本都有了标记，并基于这些有标记的样本重新训练SVM，之后再寻找易出错样本不断调整。整个算法流程如下所示： 缺图半监督学习 *14.3 基于分歧的方法*基于分歧的方法通过多个学习器之间的*分歧（disagreement）&#x2F;多样性（diversity）*来利用未标记样本数据，协同训练就是其中的一种经典方法。*协同训练最初是针对于多视图（**multi-view*****）数据而设计的，多视图数据指的是样本对象具有多个属性集，每个属性集则对应一个试图****。例如：电影数据中就包含画面类属性和声音类属性，这样画面类属性的集合就对应着一个视图。首先引入两个关于视图的重要性质： ****相容性****：即使用单个视图数据训练出的学习器的输出空间是一致的。例如都是{好，坏}、{+1,-1}等。****互补性****：即不同视图所提供的信息是互补&#x2F;相辅相成的，实质上这里体现的就是集成学习的思想。 协同训练正是很好地利用了多视图数据的“****相容互补性*”，其基本的思想是：首先基于有标记样本数据在每个视图上都训练一个初始分类器，然后让每个分类器去挑选分类置信度最高的样本并赋予标记，并将带有伪标记的样本数据传给另一个分类器去学习，从而*共同进步****。 *14.4 半监督聚类*前面提到的几种方法都是借助无标记样本数据来辅助监督学习的训练过程，从而使得学习更加充分&#x2F;泛化性能得到提升；半监督聚类则是借助已有的监督信息来辅助聚类的过程。一般而言，监督信息大致有两种类型： ****必连与勿连约束****：必连指的是两个样本必须在同一个类簇，勿连则是必不在同一个类簇。****标记信息****：少量的样本带有真实的标记。 下面主要介绍两种基于半监督的K-Means聚类算法：第一种是数据集包含一些必连与勿连关系，另外一种则是包含少量带有标记的样本。两种算法的基本思想都十分的简单：对于带有约束关系的k-均值算法，在迭代过程中对每个样本划分类簇时，需要****检测当前划分是否满足约束关系****，若不满足则会将该样本划分到距离次小对应的类簇中，再继续检测是否满足约束关系，直到完成所有样本的划分。算法流程如下图所示： 对于带有少量标记样本的k-均值算法，则可以****利用这些有标记样本进行类中心的指定，同时在对样本进行划分时，不需要改变这些有标记样本的簇隶属关系****，直接将其划分到对应类簇即可。算法流程如下所示： 半监督学习将前面许多知识模块联系在了一起，足以体现了作者编排的用心。 *《机器学习》 学习笔记（16）–概率图模型* 上篇主要介绍了半监督学习，首先从如何利用未标记样本所蕴含的分布信息出发，引入了半监督学习的基本概念，即训练数据同时包含有标记样本和未标记样本的学习方法；接着分别介绍了几种常见的半监督学习方法：生成式方法基于对数据分布的假设，利用未标记样本隐含的分布信息，使得对模型参数的估计更加准确；TSVM给未标记样本赋予伪标记，并通过不断调整易出错样本的标记得到最终输出；基于分歧的方法结合了集成学习的思想，通过多个学习器在不同视图上的协作，有效利用了未标记样本数据 ；最后半监督聚类则是借助已有的监督信息来辅助聚类的过程，带约束k-均值算法需检测当前样本划分是否满足约束关系，带标记k-均值算法则利用有标记样本指定初始类中心。本篇将讨论一种基于图的学习算法–概率图模型。 *15、概率图模型*现在来谈谈机器学习的核心价值观，可以通俗地理解为：****根据一些已观察到的证据来推断未知*。其中*基于概率的模型将学习任务归结为计算变量的概率分布****，正如之前已经提到的：生成式模型先对联合分布进行建模，从而再来求解后验概率，例如：贝叶斯分类器先对联合分布进行最大似然估计，从而便可以计算类条件概率；判别式模型则是直接对条件分布进行建模。 ****概率图模型*（probabilistic graphical model）是一类用*图结构*来表达各属性之间相关关系的概率模型，一般而言：*图中的一个结点表示一个或一组随机变量，结点之间的边则表示变量间的相关关系*，从而形成了一张“*变量关系图*”。若使用有向的边来表达变量之间的依赖关系，这样的有向关系图称为*贝叶斯网*（Bayesian nerwork）或有向图模型；若使用无向边，则称为*马尔可夫网****（Markov network）或无向图模型。 *15.1 隐马尔可夫模型(HMM)*隐马尔可夫模型（Hidden Markov Model，简称HMM）是结构最简单的一种贝叶斯网，在语音识别与自然语言处理领域上有着广泛的应用。HMM中的变量分为两组：****状态变量*与*观测变量*，其中状态变量一般是未知的，因此又称为“*隐变量****”，观测变量则是已知的输出值。在隐马尔可夫模型中，变量之间的依赖关系遵循如下两个规则： ****1. 观测变量的取值仅依赖于状态变量****；****2. 下一个状态的取值仅依赖于当前状态*，通俗来讲：*现在决定未来，未来与过去无关*，这就是著名的*马尔可夫性****。 基于上述变量之间的依赖关系，我们很容易写出隐马尔可夫模型中所有变量的联合概率分布： 易知：****欲确定一个HMM模型需要以下三组参数****： 当确定了一个HMM模型的三个参数后，便按照下面的规则来生成观测值序列： 在实际应用中，HMM模型的发力点主要体现在下述三个问题上： *15.1.1 HMM评估问题*HMM评估问题指的是：****给定了模型的三个参数与观测值序列，求该观测值序列出现的概率*。例如：对于赌场问题，便可以依据骰子掷出的结果序列来计算该结果序列出现的可能性，若小概率的事件发生了则可认为赌场的骰子有作弊的可能。解决该问题使用的是*前向算法*，即步步为营，自底向上的方式逐步增加序列的长度，直到获得目标概率值。在前向算法中，定义了一个*前向变量****，即给定观察值序列且t时刻的状态为Si的概率： 基于前向变量，很容易得到该问题的递推关系及终止条件： 因此可使用动态规划法，从最小的子问题开始，通过填表格的形式一步一步计算出目标结果。 *15.1.2 HMM解码问题*HMM解码问题指的是：****给定了模型的三个参数与观测值序列，求可能性最大的状态序列*。例如：在语音识别问题中，人说话形成的数字信号对应着观测值序列，对应的具体文字则是状态序列，从数字信号转化为文字正是对应着根据观测值序列推断最有可能的状态值序列。解决该问题使用的是*Viterbi算法*，与前向算法十分类似地，Viterbi算法定义了一个*Viterbi变量****，也是采用动态规划的方法，自底向上逐步求解。 *15.1.3 HMM学习问题*HMM学习问题指的是：****给定观测值序列，如何调整模型的参数使得该序列出现的概率最大*。这便转化成了机器学习问题，即从给定的观测值序列中学习出一个HMM模型，*该问题正是EM算法的经典案例之一****。其思想也十分简单：对于给定的观测值序列，如果我们能够按照该序列潜在的规律来调整模型的三个参数，则可以使得该序列出现的可能性最大。假设状态值序列也已知，则很容易计算出与该序列最契合的模型参数： 但一般状态值序列都是不可观测的，且****即使给定观测值序列与模型参数，状态序列仍然遭遇组合爆炸****。因此上面这种简单的统计方法就行不通了，若将状态值序列看作为隐变量，这时便可以考虑使用EM算法来对该问题进行求解： 【1】首先对HMM模型的三个参数进行随机初始化；【2】根据模型的参数与观测值序列，计算t时刻状态为i且t+1时刻状态为j的概率以及t时刻状态为i的概率。 【3】接着便可以对模型的三个参数进行重新估计： 【4】重复步骤2-3，直至三个参数值收敛，便得到了最终的HMM模型。 *15.2 马尔可夫随机场（MRF）*马尔可夫随机场（Markov Random Field）是一种典型的马尔可夫网，即使用无向边来表达变量间的依赖关系。在马尔可夫随机场中，对于关系图中的一个子集，****若任意两结点间都有边连接，则称该子集为一个团；若再加一个结点便不能形成团，则称该子集为极大团*。MRF使用*势函数*来定义多个变量的概率分布函数，其中*每个（极大）团对应一个势函数****，一般团中的变量关系也体现在它所对应的极大团中，因此常常基于极大团来定义变量的联合概率分布函数。具体而言，若所有变量构成的极大团的集合为C，则MRF的联合概率函数可以定义为： 对于条件独立性，****马尔可夫随机场通过分离集来实现条件独立****，若A结点集必须经过C结点集才能到达B结点集，则称C为分离集。书上给出了一个简单情形下的条件独立证明过程，十分贴切易懂，此处不再展开。基于分离集的概念，得到了MRF的三个性质： ****全局马尔可夫性****：给定两个变量子集的分离集，则这两个变量子集条件独立。****局部马尔可夫性****：给定某变量的邻接变量，则该变量与其它变量条件独立。****成对马尔可夫性****：给定所有其他变量，两个非邻接变量条件独立。 对于MRF中的势函数，势函数主要用于描述团中变量之间的相关关系，且要求为非负函数，直观来看：势函数需要在偏好的变量取值上函数值较大，例如：若x1与x2成正相关，则需要将这种关系反映在势函数的函数值中。一般我们常使用指数函数来定义势函数： *15.3 条件随机场（CRF）*前面所讲到的****隐马尔可夫模型和马尔可夫随机场都属于生成式模型，即对联合概率进行建模，条件随机场则是对条件分布进行建模*。CRF（Conditional Random Field）试图在给定观测值序列后，对状态序列的概率分布进行建模，即P(y | x)。直观上看：CRF与HMM的解码问题十分类似，都是在给定观测值序列后，研究状态序列可能的取值。CRF可以有多种结构，只需保证状态序列满足马尔可夫性即可，一般我们常使用的是*链式条件随机场****： 与马尔可夫随机场定义联合概率类似地，CRF也通过团以及势函数的概念来定义条件概率P(y | x)。在给定观测值序列的条件下，链式条件随机场主要包含两种团结构：单个状态团及相邻状态团，通过引入两类特征函数便可以定义出目标条件概率： 以词性标注为例，如何判断给出的一个标注序列靠谱不靠谱呢？****转移特征函数主要判定两个相邻的标注是否合理*，例如：动词+动词显然语法不通；*状态特征函数则判定观测值与对应的标注是否合理*，例如： ly结尾的词–&gt;副词较合理。因此我们可以定义一个特征函数集合，用这个特征函数集合来为一个标注序列打分，并据此选出最靠谱的标注序列。也就是说，每一个特征函数（对应一种规则）都可以用来为一个标注序列评分，把集合中所有特征函数对同一个标注序列的评分综合起来，就是这个标注序列最终的评分值。可以看出：*特征函数是一些经验的特性****。 *15.4 学习与推断*对于生成式模型，通常我们都是先对变量的联合概率分布进行建模，接着再求出目标变量的****边际分布*（marginal distribution），那如何从联合概率得到边际分布呢？这便是学习与推断。下面主要介绍两种精确推断的方法：*变量消去*与*信念传播****。 *15.4.1 变量消去*变量消去利用条件独立性来消减计算目标概率值所需的计算量，它通过运用****乘法与加法的分配率*，将对变量的积的求和问题转化为对部分变量交替进行求积与求和的问题，从而将每次的*运算控制在局部****，达到简化运算的目的。 *15.4.2 信念传播*若将变量求和操作看作是一种消息的传递过程，信念传播可以理解成：****一个节点在接收到所有其它节点的消息后才向另一个节点发送消息****，同时当前节点的边际概率正比于他所接收的消息的乘积： 因此只需要经过下面两个步骤，便可以完成所有的消息传递过程。利用动态规划法的思想记录传递过程中的所有消息，当计算某个结点的边际概率分布时，只需直接取出传到该结点的消息即可，从而避免了计算多个边际分布时的冗余计算问题。 1.指定一个根节点，从所有的叶节点开始向根节点传递消息，直到根节点收到所有邻接结点的消息****（从叶到根）*；2.从根节点开始向叶节点传递消息，直到所有叶节点均收到消息*（从根到叶）****。 *15.5 LDA话题模型*话题模型主要用于处理文本类数据，其中****隐狄利克雷分配模型****（Latent Dirichlet Allocation，简称LDA）是话题模型的杰出代表。在话题模型中，有以下几个基本概念：词（word）、文档（document）、话题（topic）。 ****词****：最基本的离散单元；****文档****：由一组词组成，词在文档中不计顺序；****话题****：由一组特定的词组成，这组词具有较强的相关关系。 在现实任务中，一般我们可以得出一个文档的词频分布，但不知道该文档对应着哪些话题，LDA话题模型正是为了解决这个问题。具体来说：****LDA认为每篇文档包含多个话题，且其中每一个词都对应着一个话题****。因此可以假设文档是通过如下方式生成： 这样一个文档中的所有词都可以认为是通过话题模型来生成的，当已知一个文档的词频分布后（即一个N维向量，N为词库大小），则可以认为：****每一个词频元素都对应着一个话题，而话题对应的词频分布则影响着该词频元素的大小****。因此很容易写出LDA模型对应的联合概率函数： 从上图可以看出，LDA的三个表示层被三种颜色表示出来： *corpus-level（红色）：* α和β表示语料级别的参数，也就是每个文档都一样，因此生成过程只采样一次。*document-level（橙色）：* θ是文档级别的变量，每个文档对应一个θ。*word-level（绿色）：* z和w都是单词级别变量，z由θ生成，w由z和β共同生成，一个单词w对应一个主题z。 通过上面对LDA生成模型的讨论，可以知道****LDA模型主要是想从给定的输入语料中学习训练出两个控制参数α和β****，当学习出了这两个控制参数就确定了模型，便可以用来生成文档。其中α和β分别对应以下各个信息： *α：分布p(θ)需要一个向量参数，即Dirichlet分布的参数，用于生成一个主题θ向量；* ***β：各个主题对应的单词概率分布矩阵p(w|z)。* 把w当做观察变量，θ和z当做隐藏变量，就可以通过EM算法学习出α和β，求解过程中遇到后验概率p(θ,z|w)无法直接求解，需要找一个似然函数下界来近似求解，原作者使用基于分解（factorization）假设的变分法（varialtional inference）进行计算，用到了EM算法。每次E-step输入α和β，计算似然函数，M-step最大化这个似然函数，算出α和β，不断迭代直到收敛。 《机器学习》 学习笔记（17）–强化学习上篇主要介绍了概率图模型，首先从生成式模型与判别式模型的定义出发，引出了概率图模型的基本概念，即利用图结构来表达变量之间的依赖关系；接着分别介绍了隐马尔可夫模型、马尔可夫随机场、条件随机场、精确推断方法以及LDA话题模型：HMM主要围绕着评估&#x2F;解码&#x2F;学习这三个实际问题展开论述；MRF基于团和势函数的概念来定义联合概率分布；CRF引入两种特征函数对状态序列进行评价打分；变量消去与信念传播在给定联合概率分布后计算特定变量的边际分布；LDA话题模型则试图去推断给定文档所蕴含的话题分布。本篇将介绍最后一种学习算法–强化学习。 *16、强化学习*****强化学习*（Reinforcement Learning，简称*RL*）是机器学习的一个重要分支。在强化学习中，包含两种基本的元素：*状态*与*动作*，*在某个状态下执行某种动作，这便是一种策略****，学习器要做的就是通过不断地探索学习，从而获得一个好的策略。例如：在围棋中，一种落棋的局面就是一种状态，若能知道每种局面下的最优落子动作，那就攻无不克&#x2F;百战不殆了~ 若将状态看作为属性，动作看作为标记，易知：****监督学习和强化学习都是在试图寻找一个映射，从已知属性&#x2F;状态推断出标记&#x2F;动作*，这样强化学习中的策略相当于监督学习中的分类&#x2F;回归器。但在实际问题中，*强化学习并没有监督学习那样的标记信息*，通常都是在*尝试动作后才能获得结果****，因此强化学习是通过反馈的结果信息不断调整之前的策略，从而算法能够学习到：在什么样的状态下选择什么样的动作可以获得最好的结果。 *16.1 基本要素*强化学习任务通常使用****马尔可夫决策过程*（Markov Decision Process，简称*MDP****）来描述，具体而言：机器处在一个环境中，每个状态为机器对当前环境的感知；机器只能通过动作来影响环境，当机器执行一个动作后，会使得环境按某种概率转移到另一个状态；同时，环境会根据潜在的奖赏函数反馈给机器一个奖赏。综合而言，强化学习主要包含四个要素：状态、动作、转移概率以及奖赏函数。 ****状态（X）****：机器对环境的感知，所有可能的状态称为状态空间；****动作（A）****：机器所采取的动作，所有能采取的动作构成动作空间；****转移概率（P）****：当执行某个动作后，当前状态会以某种概率转移到另一个状态；****奖赏函数（R）****：在状态转移的同时，环境给反馈给机器一个奖赏。 因此，****强化学习的主要任务就是通过在环境中不断地尝试，根据尝试获得的反馈信息调整策略，最终生成一个较好的策略π，机器根据这个策略便能知道在什么状态下应该执行什么动作****。常见的策略表示方法有以下两种： ****确定性策略****：π（x）&#x3D;a，即在状态x下执行a动作； ****随机性策略****：P&#x3D;π（x,a），即在状态x下执行a动作的概率。 ****一个策略的优劣取决于长期执行这一策略后的累积奖赏****，换句话说：可以使用累积奖赏来评估策略的好坏，最优策略则表示在初始状态下一直执行该策略后，最后的累积奖赏值最高。长期累积奖赏通常使用下述两种计算方法： *16.2 K摇摆赌博机*首先我们考虑强化学习最简单的情形：仅考虑一步操作，即在状态x下只需执行一次动作a便能观察到奖赏结果。易知：欲最大化单步奖赏，我们需要知道每个动作带来的期望奖赏值，这样便能选择奖赏值最大的动作来执行。若每个动作的奖赏值为确定值，则只需要将每个动作尝试一遍即可，但大多数情形下，一个动作的奖赏值来源于一个概率分布，因此需要进行多次的尝试。 单步强化学习实质上是****K-摇臂赌博机*（K-armed bandit）的原型，一般我们*尝试动作的次数是有限的****，那如何利用有限的次数进行有效地探索呢？这里有两种基本的想法： ****仅探索法****：将尝试的机会平均分给每一个动作，即轮流执行，最终将每个动作的平均奖赏作为期望奖赏的近似值。 ****仅利用法****：将尝试的机会分给当前平均奖赏值最大的动作，隐含着让一部分人先富起来的思想。 可以看出：上述****两种方法是相互矛盾的****，仅探索法能较好地估算每个动作的期望奖赏，但是没能根据当前的反馈结果调整尝试策略；仅利用法在每次尝试之后都更新尝试策略，符合强化学习的思（tao）维（lu），但容易找不到最优动作。因此需要在这两者之间进行折中。 *16.2.1 ε-贪心*****ε-贪心法基于一个概率来对探索和利用进行折中****，具体而言：在每次尝试时，以ε的概率进行探索，即以均匀概率随机选择一个动作；以1-ε的概率进行利用，即选择当前最优的动作。ε-贪心法只需记录每个动作的当前平均奖赏值与被选中的次数，便可以增量式更新。 *16.2.2 Softmax*****Softmax算法则基于当前每个动作的平均奖赏值来对探索和利用进行折中，Softmax函数将一组值转化为一组概率****，值越大对应的概率也越高，因此当前平均奖赏值越高的动作被选中的几率也越大。Softmax函数如下所示： *16.3 有模型学习*若学习任务中的四个要素都已知，即状态空间、动作空间、转移概率以及奖赏函数都已经给出，这样的情形称为“****有模型学习****”。假设状态空间和动作空间均为有限，即均为离散值，这样我们不用通过尝试便可以对某个策略进行评估。 *16.3.1 策略评估*前面提到：****在模型已知的前提下，我们可以对任意策略的进行评估****（后续会给出演算过程）。一般常使用以下两种值函数来评估某个策略的优劣： ****状态值函数（V）****：V（x），即从状态x出发，使用π策略所带来的累积奖赏； ****状态-动作值函数（Q）****：Q（x,a），即从状态x出发，执行动作a后再使用π策略所带来的累积奖赏。 根据累积奖赏的定义，我们可以引入T步累积奖赏与r折扣累积奖赏： 由于MDP具有马尔可夫性，即现在决定未来，将来和过去无关，我们很容易找到值函数的递归关系： 类似地，对于r折扣累积奖赏可以得到： 易知：****当模型已知时，策略的评估问题转化为一种动态规划问题****，即以填表格的形式自底向上，先求解每个状态的单步累积奖赏，再求解每个状态的两步累积奖赏，一直迭代逐步求解出每个状态的T步累积奖赏。算法流程如下所示： 对于状态-动作值函数，只需通过简单的转化便可得到： *16.3.2 策略改进*理想的策略应能使得每个状态的累积奖赏之和最大，简单来理解就是：不管处于什么状态，只要通过该策略执行动作，总能得到较好的结果。因此对于给定的某个策略，我们需要对其进行改进，从而得到****最优的值函数****。 最优Bellman等式改进策略的方式为：****将策略选择的动作改为当前最优的动作****，而不是像之前那样对每种可能的动作进行求和。易知：选择当前最优动作相当于将所有的概率都赋给累积奖赏值最大的动作，因此每次改进都会使得值函数单调递增。 将策略评估与策略改进结合起来，我们便得到了生成最优策略的方法：先给定一个随机策略，现对该策略进行评估，然后再改进，接着再评估&#x2F;改进一直到策略收敛、不再发生改变。这便是策略迭代算法，算法流程如下所示： 可以看出：策略迭代法在每次改进策略后都要对策略进行重新评估，因此比较耗时。若从最优化值函数的角度出发，即先迭代得到最优的值函数，再来计算如何改变策略，这便是值迭代算法，算法流程如下所示： *16.4 蒙特卡罗强化学习*在现实的强化学习任务中，****环境的转移函数与奖赏函数往往很难得知*，因此我们需要考虑在不依赖于环境参数的条件下建立强化学习模型，这便是*免模型学习****。蒙特卡罗强化学习便是其中的一种经典方法。 由于模型参数未知，状态值函数不能像之前那样进行全概率展开，从而运用动态规划法求解。一种直接的方法便是通过采样来对策略进行评估&#x2F;估算其值函数，****蒙特卡罗强化学习正是基于采样来估计状态-动作值函数*：对采样轨迹中的每一对状态-动作，记录其后的奖赏值之和，作为该状态-动作的一次累积奖赏，通过多次采样后，使用累积奖赏的平均作为状态-动作值的估计，并*引入ε-贪心策略保证采样的多样性****。 在上面的算法流程中，被评估和被改进的都是同一个策略，因此称为****同策略蒙特卡罗强化学习算法*。引入ε-贪心仅是为了便于采样评估，而在使用策略时并不需要ε-贪心，那能否仅在评估时使用ε-贪心策略，而在改进时使用原始策略呢？这便是*异策略蒙特卡罗强化学习算法****。","categories":[],"tags":[]},{"title":"","slug":"LI_001_introduce-linux","date":"2023-08-09T03:00:40.635Z","updated":"2023-08-07T08:49:47.000Z","comments":true,"path":"2023/08/09/LI_001_introduce-linux/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/LI_001_introduce-linux/","excerpt":"","text":"Linux Linux 是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统。Linux也是自由软件和开放源代码软件发展中最著名的例子。只要遵循GNU 通用公共许可证（GPL），任何个人和机构都可以自由地使用Linux的所有底层源代码，也可以自由地修改和再发布。大多数Linux系统还包括像提供GUI的X Window之类的程序。除了一部分专家之外，大多数人都是直接使用Linux 发行版，而不是自己选择每一样组件或自行设置。 Linux严格来说是单指操作系统的内核，因操作系统中包含了许多用户图形接口和其他实用工具。如今Linux常用来指基于Linux的完整操作系统，内核则改以Linux内核称之。由于这些支持用户空间的系统工具和库主要由理查德·斯托曼于1983年发起的GNU计划提供，自由软件基金会提议将其组合系统命名为GNU&#x2F;Linux，但Linux不属于GNU计划，这个名称并没有得到社区的一致认同。 Linux最初是作为支持英特尔x86架构的个人电脑的一个自由操作系统。目前Linux已经被移植到更多的计算机硬件平台，远远超出其他任何操作系统。Linux可以运行在服务器和其他大型平台之上，如大型计算机和超级计算机。世界上500个最快的超级计算机已100％运行Linux发行版或变种。Linux也广泛应用在嵌入式系统上，如手机（Mobile Phone）、平板电脑（Tablet）、路由器（Router）、电视（TV）和电子游戏机等。在移动设备上广泛使用的Android操作系统就是创建在Linux内核之上。 通常情况下，Linux被打包成供个人计算机和服务器使用的Linux发行版，一些流行的主流Linux发布版，包括Debian（及其派生版本Ubuntu、Linux Mint）、Fedora（及其相关版本Red Hat Enterprise Linux、CentOS）和openSUSE等。Linux发行版包含Linux内核和支撑内核的实用程序和库，通常还带有大量可以满足各类需求的应用程序。个人计算机使用的Linux发行版通常包含X Window和一个相应的桌面环境，如GNOME或KDE。桌面Linux操作系统常用的应用程序，包括Firefox网页浏览器、LibreOffice办公软件、GIMP图像处理工具等。由于Linux是自由软件，任何人都可以创建一个符合自己需求的Linux发行版。 1、什么是虚拟机（virtual machine）？ 通过软件技术 模拟出来的一台虚拟的计算机，使用起来与真实的计算机类似。 2、虚拟机软件 虚拟机软件可以生成虚拟机，且可以同时运行多个不同的操作系统。举个例子： 现有一个装有 Windows 系统的计算机，在上面安装了一个虚拟机软件（比如 VMware），VMware 里又装有 Linux、Mac OS等操作系统，则装有 Windows 系统被称为 宿主机，而 Linux、Mac OS 被称为 虚拟机。 3、Windows系统下 安装虚拟机 – VMware（1）Step1：下载虚拟机软件。 官网地址：https://www.vmware.com/products/workstation-pro/workstation-pro-evaluation.html 使用百度云盘下载亦可： 链接：https://pan.baidu.com/s/13qT3rTAVSUGKTuDRckNPVg 提取码：w4p4 （2）Step2：下载相关系统的镜像文件。– CentOS7 阿里云镜像地址： https://mirrors.aliyun.com/centos/8.3.2011/isos/x86_64/ 可以自行选择镜像：https://developer.aliyun.com/mirror/ 4、VMware 14 安装 Linux 系统（1）Step1：双击运行 VMware，输入密钥 或者 试用。 【密钥：】 CG54H-D8D0H-H8DHY-C6X7X-N2KG6 （2）Step2：创建新的虚拟机 （3）Step3：运行虚拟机，并安装、配置 CentOS7. （4）Step4：重启后，接受协议，登录虚拟机。 Linux环境模拟：http://cb.vu/ https://bellard.org/jslinux/ linux 为何物Linux 就是一个操作系统，就像你多少已经了解的 Windows（xp，7，8）和 Mac OS 。至于操作系统是什么，就不用过多解释了，如果你学习过前面的入门课程，应该会有个基本概念了，这里简单介绍一下操作系统在整个计算机系统中的角色。 我们的 Linux 主要是系统调用和内核那两层。当然直观地看，我们使用的操作系统还包含一些在其上运行的应用程序，比如文本编辑器、浏览器、电子邮件等。 linux历史简介操作系统始于二十世纪五十年代，当时的操作系统能运行批处理程序。批处理程序不需要用户的交互，它从文件或者穿孔卡片读取数据，然后输出到另外一个文件或者打印机。 二十世纪六十年代初，交互式操作系统开始流行。它不仅仅可以交互，还能使多个用户从不同的终端同时操作主机。这样的操作系统被称作分时操作系统，它的出现对批处理操作系统是个极大的挑战。许多人尝试开发分时操作系统， 其中包括一些大学的研究项目和商业项目。当时有个项目叫做 Multics ，它的技术在当时很具有创新性。 Multics 项目的开发并不顺利，它花费了远超过预计的资金，却没有在操作系统市场上占到多少份额。而参加该项目的一个开发团体——贝尔实验室退出了这个项目。他们在退出后开发了他们自己的一个操作系统—— UNIX 。 UNIX 最初免费发布并因此在大学里受到欢迎。后来，UNIX 实现了 TCP&#x2F;IP 协议栈，成为了早期工作站的操作系统的一个流行选择。 1990 年，UNIX 在服务器市场上尤其是大学校园中成为主流操作系统，许多校园都有 UNIX 主机，当然还包括一些研究它的计算机系的学生。这些学生都渴望能在自己的电脑上运行 UNIX 。不幸的是，从那时候开始，UNIX 开始变得商业化，它的价格也变得非常昂贵。而唯一低廉的选择就是 MINIX，这是一个功能有限的类似 UNIX 的操作系统，作者 Andrew Tanenbaum 开发它的目的是用于教学。 1991 年 10 月，Linus Torvalds（Linux 之父）在赫尔辛基大学接触 UNIX，他希望能在自己的电脑上运行一个类似的操作系统。可是 UNIX 的商业版本非常昂贵，于是他从 MINIX 开始入手，计划开发一个比 MINIX 性能更好的操作系统。很快他就开始了自己的开发工作。他第一次发行的版本迅速吸引了一些黑客。尽管最初的 Linux 并没有多少用处，但由于一些黑客的加入使它很快就具有了许多吸引人的特性，甚至一些对操作系统开发不感兴趣的人也开始关注它。 Linux 本身只是操作系统的内核。内核是使其它程序能够运行的基础。它实现了多任务和硬件管理，用户或者系统管理员交互运行的所有程序实际上都运行在内核之上。其中有些程序是必需的，比如说，命令行解释器（shell），它用于用户交互和编写 shell 脚本。 Linus 没有自己去开发这些应用程序，而是使用已有的自由软件。这减少了搭建开发环境所需花费的工作量。实际上，他经常改写内核，使得那些程序能够更容易地在 Linux 上运行。许多重要的软件，包括 C 编译器，都来自于自由软件基金 GNU 项目。GNU 项目开始于 1984 年，目的是为了开发一个完全类似于 UNIX 的免费操作系统。为了表扬 GNU 对 Linux 的贡献，许多人把 Linux 称为 GNU&#x2F;Linux（GNU 有自己的内核）。 1992－1993 年，Linux 内核具备了挑战 UNIX 的所有本质特性，包括 TCP&#x2F;IP 网络，图形界面系统（X window )，Linux 同样也吸引了许多行业的关注。一些小的公司开始开发和发行 Linux，有几十个 Linux 用户社区成立。1994 年，Linux 杂志也开始发行。 Linux 内核 1.0 在 1994 年 3 月发布，内核的发布要经历许多开发周期，直至达到一个稳定的版本。 下面列举一些 Linux 诞生大事件： 1965 年，Bell 实验室、MIT、GE（通用电气公司）准备开发 Multics 系统，为了同时支持 300 个终端访问主机，但是 1969 年失败了；那时候并没有鼠标、键盘，输入设备，只有卡片机。因此，如果要测试某个程序，则需要将读卡纸插入卡片机，如果有错误，还需要重新来过；Multics：Multiplexed Information and Computing Service； 1969 年，Ken Thompson（C 语言之父）利用汇编语言开发了 File Server System（Unics，即 UNIX 的原型）；因为汇编语言对于硬件的依赖性，因此只能针对特定硬件； 只是为了移植一款“太空旅游”的游戏； 1973 年，Dennis Ritchie 和 Ken Thompson 发明了 C 语言，而后写出了 UNIX 的内核；将 B 语言改成 C 语言，由此产生了 C 语言之父；90% 的代码是 C 语言写的，10% 的代码用汇编语言写的，因此移植时只要修改那 10% 的代码即可； 1977 年，Berkeley 大学的 Bill Joy 针对他的机器修改了 UNIX 源码，称为 BSD（Berkeley Software Distribution）；Bill Joy 是 Sun 公司的创始人； 1979 年，UNIX 发布 System V，用于个人计算机； 1984 年，因为 UNIX 规定“不能对学生提供源码”，Tanenbaum 老师自己编写兼容于 UNIX 的 Minix，用于教学； 1984 年，Stallman 开始 GNU（GNU’s Not Unix）项目，创办 FSF（Free Software Foundation）基金会；产品：GCC、Emacs、Bash Shell、GLIBC；倡导“自由软件”；GNU 的软件缺乏一个开放的平台运行，只能在 UNIX 上运行；自由软件指用户可以对软件做任何修改，甚至再发行，但是始终要挂着 GPL 的版权；自由软件是可以卖的，但是不能只卖软件，而是卖服务、手册等； 1985 年，为了避免 GNU 开发的自由软件被其他人用作专利软件，因此创建 GPL（General Public License）版权声明； 1988 年，MIT 为了开发 GUI，成立了研发 XFree86 的组织； 1991 年，芬兰赫尔辛基大学的研究生 Linus Torvalds 基于 gcc、bash 开发了针对 386 机器的 Linux 内核； 1994 年，Torvalds 发布 Linux-v1.0； 1996 年，Torvalds 发布 Linux-v2.0，确定了 Linux 的吉祥物：企鹅。 UNIX 进化史（UNIX 大家族族谱 1969-2013）： linux与 Windows 到底有哪些不同1. 免费与收费 最新正版 Windows 10，需要付费购买； Linux 免费或少许费用。 2. 软件与支持 Windows 平台：数量和质量的优势，不过大部分为收费软件；由微软官方提供重要支持和服务； Linux 平台：大都为开源自由软件，用户可以修改定制和再发布，由于基本免费没有资金支持，部分软件质量和体验欠缺；由全球所有的 Linux 开发者和自由软件社区提供支持。 3. 安全性 Windows 平台：三天两头打补丁安装系统安全更新，还是会中病毒木马； Linux 平台：要说 Linux 没有安全问题，那当然是不可能的，这一点仁者见仁智者见智，相对来说肯定比 Windows 平台要更加安全，使用 Linux 你也不用装某杀毒、某毒霸。 4. 使用习惯 Windows：普通用户基本都是纯图形界面下操作使用，依靠鼠标和键盘完成一切操作，用户上手容易，入门简单； Linux：兼具图形界面操作（需要使用带有桌面环境的发行版）和完全的命令行操作，可以只用键盘完成一切操作，新手入门较困难，需要一些学习和指导（这正是我们要做的事情），一旦熟练之后效率极高。 5. 可定制性 Windows：这些年之前算是全封闭的，系统可定制性很差； Linux：你想怎么做就怎么做，Windows 能做到得它都能，Windows 做不到的，它也能。 6. 应用范畴或许你之前不知道 Linux ，要知道，你之前在 Windows 使用百度、谷歌，上淘宝，聊 QQ 时，支撑这些软件和服务的，是后台成千上万的 Linux 服务器主机，它们时时刻刻都在忙碌地进行着数据处理和运算，可以说世界上大部分软件和服务都是运行在 Linux 之上的。 7. Windows 没有的 稳定的系统 安全性和漏洞的快速修补 多用户 用户和用户组的规划 相对较少的系统资源占用 可定制裁剪，移植到嵌入式平台（如安卓设备） 可选择的多种图形用户界面（如 GNOME，KDE） 8. Linux 没有的 特定的支持厂商 足够的游戏娱乐支持度 足够的专业软件支持度 如何学习Linux 明确目的：你是要用 Linux 来干什么，搭建服务器、做程序开发、日常办公，还是娱乐游戏； 面对现实：Linux 大都在命令行下操作，能否接受不用或少用图形界面； 是学习 Linux 操作系统本身还是某一个 Linux 发行版（Ubuntu，CentOS，Fedora，OpenSUSE，Debian，Mint 等等），如果你对发行版的概念或者它们之间的关系不明确的话可以参看 Linux 发行版。 注重基础，从头开始，可参考菜鸟教程 Shell通常在图形界面中对实际体验带来差异的不是上述的不同发行版的各种终端模拟器，而是这个 Shell（壳）。有壳就有核，这里的核就是指 UNIX&#x2F;Linux 内核，Shell 是指“提供给使用者使用界面”的软件（命令解析器），类似于 DOS 下的 command（命令行）和后来的 cmd.exe 。 普通意义上的 Shell 就是可以接受用户输入命令的程序。它之所以被称作 Shell 是因为它隐藏了操作系统底层的细节。同样的 UNIX&#x2F;Linux 下的图形用户界面 GNOME 和 KDE，有时也被叫做“虚拟 shell”或“图形 shell”。 UNIX&#x2F;Linux 操作系统下的 Shell 既是用户交互的界面，也是控制系统的脚本语言。当然这一点也有别于 Windows 下的命令行，虽然该命令行也提供了很简单的控制语句。在 Windows 操作系统下，有些用户从来都不会直接使用 Shell，然而在 UNIX 系列操作系统下，Shell 仍然是控制系统启动、X11 启动和很多其它实用工具的脚本解释程序。 在 UNIX&#x2F;Linux 中比较流行的常见的 Shell 有 bash、zsh、ksh、csh 等等，Ubuntu 终端默认使用的是 bash，默认的桌面环境是 GNOME 或者 Unity（基于 GNOME） 命令行操作体验在 linux 中，最最重要的就是命令，这就包含了 2 个过程，输入和输出 输入：输入当然就是打开终端，然后按键盘输入，然后按回车，输入格式一般就是这类的 #创建一个名为 file 的文件，touch是一个命令 touch file #进入一个目录，cd是一个命令 cd &#x2F;etc&#x2F; #查看当前所在目录 pwd 输出：输出会返回你想要的结果，比如你要看什么文件，就会返回文件的内容。如果只是执行，执行失败会告诉你哪里错了，如果执行成功那么会没有输出，因为 linux 的哲学就是：没有结果就是最好的结果 开始打开终端后系统会自动运行 Shell 程序，然后我们就可以输入命令让系统来执行了： echo “hello world” https://zhuanlan.zhihu.com/p/101478169 1) 重要快捷键真正学习命令行之前，你先要掌握几个十分有用、必需掌握的小技巧： [Tab]使用Tab键来进行命令补全，Tab键一般是在字母Q旁边，这个技巧给你带来的最大的好处就是当你忘记某个命令的全称时可以只输入它的开头的一部分，然后按下Tab键就可以得到提示或者帮助完成： 当然不止补全命令，补全目录、补全命令参数都是没问题的： [Ctrl+c]想想你有没有遇到过这种情况，当你在 Linux 命令行中无意输入了一个不知道的命令，或者错误地使用了一个命令，导致在终端里出现了你无法预料的情况，比如，屏幕上只有光标在闪烁却无法继续输入命令，或者不停地输出一大堆你不想要的结果。你想要立即停止并恢复到你可控的状态，那该怎么办呢？这时候你就可以使用Ctrl+c键来强行终止当前程序（你可以放心它并不会使终端退出）。 尝试输入以下命令： tail 然后你会发现你接下来的输入都没有任何反应了，只是将你输入的东西显示出来，现在你可以使用Ctrl+c，来中断这个你目前可能还不知道是什么的程序（在后续课程中我们会具体解释这个tail命令是什么）。 又或者输入： find &#x2F; 显然这不是你想的结果，可以使用Ctrl+c结束。 虽然这个按着很方便，但不要随便按，因为有时候，当你看到终端没有任何反应或提示，也不能接受你的输入时，可能只是运行的程序需要你耐心等一下，就不要急着按Ctrl+c了。 其他一些常用快捷键按键****作用 2) 学会利用历史输入命令很简单，你可以使用键盘上的方向上键↑，恢复你之前输入过的命令，你一试便知。 3) 学会使用通配符通配符是一种特殊语句，主要有星号（*）和问号（?），用来对字符串进行模糊匹配（比如文件名、参数名）。当查找文件夹时，可以使用它来代替一个或多个真正字符；当不知道真正字符或者懒得输入完整名字时，常常使用通配符代替一个或多个真正字符。 终端里面输入的通配符是由 Shell 处理的，不是由所涉及的命令语句处理的，它只会出现在命令的“参数值”里（它不能出现在命令名称里， 命令不记得，那就用Tab补全）。当 Shell 在“参数值”中遇到了通配符时，Shell 会将其当作路径或文件名在磁盘上搜寻可能的匹配：若符合要求的匹配存在，则进行代换（路径扩展）；否则就将该通配符作为一个普通字符传递给“命令”，然后再由命令进行处理。总之，通配符实际上就是一种 Shell 实现的路径扩展功能。在通配符被处理后， Shell 会先完成该命令的重组，然后继续处理重组后的命令，直至执行该命令。 首先回到用户家目录： cd &#x2F;home&#x2F;shiyanlou 然后使用 touch 命令创建 2 个文件，后缀都为 txt： touch asd.txt fgh.txt 可以给文件随意命名，假如过了很长时间，你已经忘了这两个文件的文件名，现在你想在一大堆文件中找到这两个文件，就可以使用通配符： ls *.txt 在创建文件的时候，如果需要一次性创建多个文件，比如：“love_1_linux.txt，love_2_linux.txt，… love_10_linux.txt”。在 Linux 中十分方便： touch love_{1..10}_shiyanlou.txt Shell 常用通配符： 字符****含义 4) 学会在命令行中获取帮助在 Linux 环境中，如果你遇到困难，可以使用man命令，它是Manual pages的缩写。 Manual pages 是 UNIX 或类 UNIX 操作系统中在线软件文档的一种普遍的形式， 内容包括计算机程序（包括库和系统调用）、正式的标准和惯例，甚至是抽象的概念。用户可以通过执行man命令调用手册页。 你可以使用如下方式来获得某个命令的说明和使用方式的详细介绍： man 比如你想查看 man 命令本身的使用方式，你可以输入： man man 通常情况下，man 手册里面的内容都是英文的，这就要求你有一定的英文基础。man 手册的内容很多，涉及了 Linux 使用过程中的方方面面。为了便于查找，man 手册被进行了分册（分区段）处理，在 Research UNIX、BSD、OS X 和 Linux 中，手册通常被分为 8 个区段，安排如下： 区段****说明 要查看相应区段的内容，就在 man 后面加上相应区段的数字即可，如： man 1 ls 会显示第一区段中的ls命令 man 页面。 所有的手册页遵循一个常见的布局，为了通过简单的 ASCII 文本展示而被优化，而这种情况下可能没有任何形式的高亮或字体控制。一般包括以下部分内容： NAME（名称） 该命令或函数的名称，接着是一行简介。 SYNOPSIS（概要） 对于命令，正式的描述它如何运行，以及需要什么样的命令行参数。对于函数，介绍函数所需的参数，以及哪个头文件包含该函数的定义。 DESCRIPTION（说明） 命令或函数功能的文本描述。 EXAMPLES（示例） 常用的一些示例。 SEE ALSO（参见） 相关命令或函数的列表。 也可能存在其它部分内容，但这些部分没有得到跨手册页的标准化。常见的例子包括：OPTIONS（选项），EXIT STATUS（退出状态），ENVIRONMENT（环境），BUGS（程序漏洞），FILES（文件），AUTHOR（作者），REPORTING BUGS（已知漏洞），HISTORY（历史）和 COPYRIGHT（版权）。 通常 man 手册中的内容很多，你可能不太容易找到你想要的结果，不过幸运的是你可以在 man 中使用搜索&#x2F;&lt;你要搜索的关键字&gt;，查找完毕后你可以使用n键切换到下一个关键字所在处，shift+n为上一个关键字所在处。使用Space（空格键）翻页，Enter（回车键）向下滚动一行，或者使用k，j（vim 编辑器的移动键）进行向前向后滚动一行。按下h键为显示使用帮助（因为 man 使用 less 作为阅读器，实为less工具的帮助），按下q退出。 想要获得更详细的帮助，你还可以使用info命令，不过通常使用man就足够了。如果你知道某个命令的作用，只是想快速查看一些它的某个具体参数的作用，那么你可以使用–help参数，大部分命令都会带有这个参数，如： ls –help 命令行一些说明 ‘-‘ 单横杠一般为短命令行选项，一般单横岗后面跟一些简短的字母 如rm -f， mv -r , npm -D , npm -g，npm -h, npm -v ‘–’ 双横杠一般为长命令行选项，一般双横杠后面跟完整的单词。如 npm –help， npm –version, npm –save-dev, npm –global 上面的npm命令，npm -h和npm –help、npm -v和npm –version 、 npm -D和 npm –save-dev 等命令是等价的，只是写法不同，npm –save-dev中‘save-dev’是一个完整的单词，中间的横杠不是选项 命令行选项参数设置，如npm设置，npm install -g cnpm –registry&#x3D;https://registry.npm.taobao.org和npm install -g cnpm –registry https://registry.npm.taobao.org , 这两个命令是等价的，–registry后面可以空格设置参数，也可用‘&#x3D;’设置，两者效果一样，区别在终端是否识别 短命令行的参数可以合并，如ls -al，rm -rf两个命令可以写成 ls -a -l 和 rm -r -f 命令行区分大小写，不同的大小写会有不同的功能，如ps -a和ps -A，前一个显示同一终端下的所有程序，后一个显示所有进程，不在局限在同一终端 !! 表示执行上一条命令，如 执行 ls -al 后发现没有权限，可以 sudo !! linux命令行大小学敏感，既要区分大写 常用终端命令 pwd：获取当前所在路径； ls：列出当前目录下的所有文件；ls -al ：以长格式列出所有目录下文件（包括隐藏的文件）-a代表所有，-l长格式显示 cd： 进行目录之间的相互跳转；cd ~ ：返回家目录；cd - ：打开上一个 cd 过的目录；cd ..&#x2F; : 返回上一层目录，cd - 和cd ..&#x2F; 这两个注意区别 mkdir、rmdir、mv [源地址]：创建、删除、移动文件夹，如果文件夹内不为空，则无法用rmdir删除文件夹； rm -rf [源地址]：递归删除文件夹及文件夹内所有内容，无法挽回 cp -R [源地址] [目的地址]: 带参数-R是目录复制，不带是文件复制 history：查看之前执行过命令的历史记录，关闭终端后输入history将不会显示上个关闭终端的历史记录 ps：查看当前终端运行的程序；ps -A：显示所有进程 top：显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等，终端按q退出显示 kill [PID]：结束指定进程ID的进程，先使用top命令查看想要结束进程的PID，然后使用命令kill [PID结束进程 touch [文件名] ：创建文件 more [文件名] 或 cat [文件名]：查看文件内容 管道符号 ‘&gt;’ 、’&lt;’ 、’&lt; &gt;’：将文件作为输入内容， &gt; 左边的输出作为右边的输入，&lt; 右边的输出作为左边的输入，如：node test.js &gt; hello.txt，test.js的输出的内容会输入到hello.txt，打开hello.txt文件能看到test.js的输出内容；同理，node test.js &lt; hello.txt中，运行test.js的输入内容是hello.txt的内容；node test.js &lt; hello.txt &gt; world.txt中，test.js使用hello.txt内容作为输出，运行完成后将test.js运行输出写入world.txt，(刚开始时理解成了hello.txt既向 test.js输入内容，也向world.txt输入内容，运行后才发现理解错了)，‘&gt;’会将原有内容覆盖，‘&gt;&gt;’是内容追加，不覆盖 管道符号 ‘|’： 如 node test.js | node test_1.js，将test.js 的输出作为test_1.js的输入；’|’ 和 ‘&gt;’比较像，但有区别，个人理解的区别是，&gt; 一般作用于文件的输入输出，直接使用读取文件，而‘|’ 作用于程序运行的输入输出，读取文件内容需要通过命令如 cat hello.txt | node test_1.js 和 node test_1.js &lt; hello.txt 一样的效果。 tail和head：列出文件的结尾和文件的开头；tail -n 5 hello.txt：显示hello.txt的最后五行内容；tail -n +5 hello.txt：显示hello.txt的从第五行开始到结束的内容，区别在显示的行数添加了一个“+”；tail -f [文件名]：若有程序持续向该文件写入内容，则可以实时查看该文件的内容；head -n 5 hello.txt：显示hello.txt的前五行内容；head -n -5 hello.txt：输出文件除了最后5行的全部内容，head和tail效果相反 grep: 强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来；grep -n hello hello_world.txt：从hello_world.txt中找出hello位置的数据，并在前面加上行号；cat hello_world.txt | grep -n hello：和上面的效果一样，grep的输入内容使用了上面管道命令的输入cat hello_world.txt ；cat hello_world.txt | grep -n o* ：寻找hello_world.txt中包含“o”的行 ifconfig：列出本机所有的网络设备以及其上面的配置，主要使用ip地址和mac地址 which：查看可执行文件的位置，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果；which cd：&#x2F;usr&#x2F;bin&#x2F;cd ； which pwd ：bin&#x2F;pwd find pathname -options [-print -exec -ok …]：用于在文件树种查找文件；如 find . -name “*.txt”：查找当前目录下一 .txt结尾的文件 man 命令：查看命令是的使用，如man cd、man rm等 常用网络命令 curl和wget：利用URL规则在命令行下工作的文件传输工具，；如：curl -o mybaidu.html http://www.baidu.com 和wget -O mybaidu.html http://www.baidu.com：会将内容下载到本地baidu.html（mac 需要下载wget命令）；根据命令行不同选项有强大的功能，具体可以参考：https://www.cnblogs.com/Downtime/p/8068097.html；curl和wget基础功能有诸多重叠，curl由于可自定义各种请求参数所以在模拟web请求方面更擅长；wget由于支持ftp和Recursive所以在下载文件方面更擅长 ping：通常用来测试与目标主机的连通性；如ping -c 10 -i 5 www.baidu.com：“-c”要发送数据包数目，“-i”数据包建个发送多少秒，每5秒向百度发送一个数据包，总共发送10次， telnet ：通常用来远程登录；telnet www.baidu.com：尝试登陆百度主机，如果能连上需要输入账号和密码 route：用于显示和操作IP路由表，也提供了路由增加、修改、删除等操作；route -n：显示当前路由信息（mac中route命令不起作用，可以使用netstat -nr查看）； netstat：用于显示与IP、TCP、UDP和ICMP协议相关的统计数据，一般用于检验本机各端口的网络连接情况；netstat -a：列出所有的端口；netstat -nr：显示路由信息 traceroute：可以知道信息从你的计算机到互联网另一端的主机是走的什么路径；如 traceroute www.baidu.com：能在信息界面看到路由是如何跳转的 Linux 目录结构 Linux 的目录与 Windows 的目录是有区别的，或许对于一般操作上的感受来说没有多大不同，但从它们的实现机制来说是完全不同的。 一种不同是体现在目录与存储介质（磁盘，内存，DVD 等）的关系上，以往的 Windows 一直是以存储介质为主的，主要以盘符（C 盘，D 盘…）及分区来实现文件管理，然后之下才是目录，目录就显得不是那么重要，除系统文件之外的用户文件放在任何地方任何目录也是没有多大关系。所以通常 Windows 在使用一段时间后，磁盘上面的文件目录会显得杂乱无章（少数善于整理的用户除外吧）。然而 UNIX&#x2F;Linux 恰好相反，UNIX 是以目录为主的，Linux 也继承了这一优良特性。 Linux 是以树形目录结构的形式来构建整个系统的，可以理解为树形目录是一个用户可操作系统的骨架。虽然本质上无论是目录结构还是操作系统内核都是存储在磁盘上的，但从逻辑上来说 Linux 的磁盘是“挂在”（挂载在）目录上的，每一个目录不仅能使用本地磁盘分区的文件系统，也可以使用网络上的文件系统。举例来说，可以利用网络文件系统（Network File System，NFS）服务器载入某特定目录等。 Linux系统各个目录的一些作用文件系统的类型 LINUX有四种基本文件系统类型：普通文件、目录文件、链接文件和特殊文件，可用file命令来识别. 普通文件： 如文本文件、C语言元代码、SHELL脚本、二进制的可执行文件等，可用cat、less、more、vi、emacs来察看内容，用mv来改名。 目录文件： 包括文件名、子目录名及其指针。它是LINUX储存文件名的唯一地方，可用ls列出目录文件。 链接文件： 是指向同一索引节点的那些目录条目。用ls来查看是，连接文件的标志用l开头，而文件面后以”-&gt;”指向所连接的文件。 特殊文件： LINUX的一些设备如磁盘、终端、打印机等都在文件系统中表示出来，则一类文件就是特殊文件，常放在&#x2F;dev目录内。例如，软驱A称为&#x2F;dev&#x2F;fd0。LINUX无C：的概念，而是用&#x2F;dev&#x2F;had来自第一硬盘。 对于linux新手来说，最感到迷惑的问题之一就是文件都存在哪里呢?特别是对于那些从windows转过来的新手来说，linux的目录结构看起来有些奇怪哦。所以，在这里讲一下linux下的主要目录以及它们都是用来干什么的。 系统目录简介 &#x2F; 这就是根目录。对你的电脑来说，有且只有一个根目录。所有的东西，我是说所有的东西都是从这里开始。举个例子：当你在终端里输入“&#x2F;home”，你其实是在告诉电脑，先从&#x2F;(根目录)开始，再进入到home目录。 &#x2F;root 这是系统管理员(root user)的目录。对于系统来说，系统管理员就好比是上帝，它能对系统做任何事情，甚至包括删除你的文件。因此，请小心使用root帐号。 &#x2F;bin 这里存放了标准的(或者说是缺省的)linux的工具，比如像“ls”、“vi”还有“more”等等。通常来说，这个目录已经包含在你的“path”系 统变量里面了。什么意思呢?就是：当你在终端里输入ls，系统就会去&#x2F;bin目录下面查找是不是有ls这个程序。 &#x2F;etc 这里主要存放了系统配置方面的文件。举个例子：你安装了samba这个套件，当你想要修改samba配置文件的时候，你会发现它们(配置文件)就在&#x2F;etc&#x2F;samba目录下。 &#x2F;dev 这里主要存放与设备(包括外设)有关的文件(unix和linux系统均把设备当成文件)。想连线打印机吗?系统就是从这个目录开始工作的。另外还有一些包括磁盘驱动、USB驱动等都放在这个目录。 &#x2F;home 这里主要存放你的个人数据。具体每个用户的设置文件，用户的桌面文件夹，还有用户的数据都放在这里。每个用户都有自己的用户目录，位置为：&#x2F;home&#x2F;用户名。当然，root用户除外。 &#x2F;tmp 这是临时目录。对于某些程序来说，有些文件被用了一次两次之后，就不会再被用到，像这样的文件就放在这里。有些linux系统会定期自动对这个目录进行清理，因此，千万不要把重要的数据放在这里。 &#x2F;usr 在这个目录下，你可以找到那些不适合放在&#x2F;bin或&#x2F;etc目录下的额外的工具。比如像游戏阿，一些打印工具拉等等。&#x2F;usr目录包含了许多子目录： &#x2F;usr&#x2F;bin目录用于存放程序;&#x2F;usr&#x2F;share用于存放一些共享的数据，比如音乐文件或者图标等等;&#x2F;usr&#x2F;lib目录用于存放那些不能直接 运行的，但却是许多程序运行所必需的一些函数库文件。你的软件包管理器(应该是“新立得”吧)会自动帮你管理好&#x2F;usr目录的。 &#x2F;opt 这里主要存放那些可选的程序。你想尝试最新的firefox测试版吗?那就装到&#x2F;opt目录下吧，这样，当你尝试完，想删掉firefox的时候，你就可 以直接删除它，而不影响系统其他任何设置。安装到&#x2F;opt目录下的程序，它所有的数据、库文件等等都是放在同个目录下面。 举个例子：刚才装的测试版firefox，就可以装到&#x2F;opt&#x2F;firefox_beta目录下，&#x2F;opt&#x2F;firefox_beta目录下面就包含了运 行firefox所需要的所有文件、库、数据等等。要删除firefox的时候，你只需删除&#x2F;opt&#x2F;firefox_beta目录即可，非常简单。 &#x2F;usr&#x2F;local 这里主要存放那些手动安装的软件，即不是通过“新立得”或apt-get安装的软件。它和&#x2F;usr目录具有相类似的目录结构。让软件包管理器来管理&#x2F;usr目录，而把自定义的脚本(scripts)放到&#x2F;usr&#x2F;local目录下面，我想这应该是个不错的主意。 &#x2F;media 有些linux的发行版使用这个目录来挂载那些usb接口的移动硬盘(包括U盘)、CD&#x2F;DVD驱动器等等。 目录路径路径有人可能不明白这路径是指什么，有什么用。顾名思义，路径就是你要去哪儿的路线嘛。如果你想进入某个具体的目录或者想获得某个目录的文件（目录本身也是文件）那就得用路径来找到了。 使用 cd 命令可以切换目录，在 Linux 里面使用 . 表示当前目录，.. 表示上一级目录（注意，我们上一节介绍过的，以 . 开头的文件都是隐藏文件，所以这两个目录必然也是隐藏的，你可以使用 ls -a 命令查看隐藏文件），- 表示上一次所在目录，～ 通常表示当前用户的 home 目录。使用 pwd 命令可以获取当前所在路径（绝对路径）。 进入上一级目录： cd .. 进入你的 home 目录： cd ~ # 或者 cd &#x2F;home&#x2F;&lt;你的用户名&gt; 使用 pwd 获取当前路径： pwd 绝对路径关于绝对路径，简单地说就是以根” &#x2F; “目录为起点的完整路径，以你所要到的目录为终点，表现形式如： &#x2F;usr&#x2F;local&#x2F;bin，表示根目录下的 usr 目录中的 local 目录中的 bin 目录。 相对路径相对路径，也就是相对于你当前的目录的路径，相对路径是以当前目录 . 为起点，以你所要到的目录为终点，表现形式如： usr&#x2F;local&#x2F;bin （这里假设你当前目录为根目录）。你可能注意到，我们表示相对路径实际并没有加上表示当前目录的那个 . ，而是直接以目录名开头，因为这个 usr 目录为 &#x2F; 目录下的子目录，是可以省略这个 . 的（以后会讲到一个类似不能省略的情况）；如果是当前目录的上一级目录，则需要使用 .. ，比如你当前目录为 &#x2F;home&#x2F;shiyanlou 目录下，根目录就应该表示为 ..&#x2F;..&#x2F; ，表示上一级目录（ home 目录）的上一级目录（ &#x2F; 目录）。 下面我们以你的 home 目录为起点，分别以绝对路径和相对路径的方式进入 &#x2F;usr&#x2F;local&#x2F;bin 目录： # 绝对路径 cd &#x2F;usr&#x2F;local&#x2F;bin # 相对路径 cd ..&#x2F;..&#x2F;usr&#x2F;local&#x2F;bin 进入一个目录，可以使用绝对路径也可以使用相对路径，那我们应该在什么时候选择正确的方式进入某个目录呢。就是凭直觉嘛，你觉得怎样方便就使用哪一个，而不用特意只使用某一种。比如假设我当前在 &#x2F;usr&#x2F;local&#x2F;bin 目录，我想进入上一级的 local 目录你说是使用 cd .. 方便还是 cd &#x2F;usr&#x2F;local 方便？而如果要进入的是 usr 目录，那么 cd &#x2F;usr ，就比 cd ..&#x2F;.. 方便一点了。 提示：在进行目录切换的过程中请多使用 Tab 键自动补全，可避免输入错误，连续按两次 Tab 可以显示全部候选结果。 新建空白文件使用 touch 命令创建空白文件，关于 touch 命令，其主要作用是来更改已有文件的时间戳的（比如，最近访问时间，最近修改时间），但其在不加任何参数的情况下，只指定一个文件名，则可以创建一个指定文件名的空白文件（不会覆盖已有同名文件），当然你也可以同时指定该文件的时间戳，更多关于 touch 命令的用法，会在下一讲文件搜索中涉及。 创建名为 test 的空白文件，因为在其它目录没有权限，所以需要先 cd ~ 切换回 shiyanlou 用户的 Home 目录： cd ~ touch test 新建目录使用 mkdir（make directories）命令可以创建一个空目录，也可同时指定创建目录的权限属性。 创建名为“ mydir ”的空目录： mkdir mydir 使用 -p 参数，同时创建父目录（如果不存在该父目录），如下我们同时创建一个多级目录（这在安装软件、配置安装路径时非常有用）： mkdir -p father&#x2F;son&#x2F;grandson 这里使用的路径是相对路径，代表在当前目录下生成，当然我们直接以绝对路径的方式表示也是可以的。 还有一点需要注意的是，若当前目录已经创建了一个 test 文件，再使用 mkdir test 新建同名的文件夹，系统会报错文件已存在。这符合 Linux 一切皆文件的理念。 若当前目录存在一个 test 文件夹，则 touch 命令，则会更改该文件夹的时间戳而不是新建文件。 复制文件使用 cp 命令（copy）复制一个文件到指定目录。 将之前创建的 test 文件复制到 &#x2F;home&#x2F;shiyanlou&#x2F;father&#x2F;son&#x2F;grandson 目录中： cp test father&#x2F;son&#x2F;grandson 是不是很方便啊，如果在图形界面则需要先在源目录复制文件，再进到目的目录粘贴文件，而命令行操作步骤就一步到位了嘛。 复制目录如果直接使用 cp 命令复制一个目录的话，会出现如下错误： 要成功复制目录需要加上 -r 或者 -R 参数，表示递归复制，就是说有点“株连九族”的意思： cd &#x2F;home&#x2F;shiyanlou mkdir family cp -r father family 删除删除文件使用 rm（remove files or directories）命令删除一个文件： rm test 有时候你会遇到想要删除一些为只读权限的文件，直接使用 rm 删除会显示一个提示，如下： 你如果想忽略这提示，直接删除文件，可以使用 -f 参数强制删除： rm -f test 删除目录跟复制目录一样，要删除一个目录，也需要加上 -r 或 -R 参数： rm -r family 遇到权限不足删除不了的目录也可以和删除文件一样加上 -f 参数： rm -rf family 移动文件使用 mv（move or rename files）命令移动文件（剪切）。命令格式是 mv 源目录文件 目的目录。 例如将文件“ file1 ”移动到 Documents 目录： mkdir Documents touch file1 mv file1 Documents 重命名文件mv 命令除了能移动文件外，还能给文件重命名。命令格式为 mv 旧的文件名 新的文件名。 例如将文件“ file1 ”重命名为“ myfile ”： mv file1 myfile 批量重命名要实现批量重命名，mv 命令就有点力不从心了，我们可以使用一个看起来更专业的命令 rename 来实现。不过它要用 perl 正则表达式来作为参数，关于正则表达式我们要在后面才会介绍到，这里只做演示，你只要记得这个 rename 命令可以批量重命名就好了，以后再重新学习也不会有任何问题，毕竟你已经掌握了一个更常用的 mv 命令。 rename 命令并不是内置命令，若提示无该命令可以使用 sudo apt-get install rename 命令自行安装。 cd &#x2F;home&#x2F;shiyanlou&#x2F; # 使用通配符批量创建 5 个文件: touch file{1..5}.txt # 批量将这 5 个后缀为 .txt 的文本文件重命名为以 .c 为后缀的文件: rename ‘s&#x2F;.txt&#x2F;.c&#x2F;‘ *.txt # 批量将这 5 个文件，文件名和后缀改为大写: rename ‘y&#x2F;a-z&#x2F;A-Z&#x2F;‘ *.c 简单解释一下上面的命令，rename 是先使用第二个参数的通配符匹配所有后缀为 .txt 的文件，然后使用第一个参数提供的正则表达式将匹配的这些文件的 .txt 后缀替换为 .c，这一点在我们后面学习了 sed 命令后，相信你会更好地理解。 有的同学可能在输入时出现命令未闭合的状态，命令行会出现 quote&gt; 开头的提示符。这是因为上述命令中的 ‘ 未输入完成，这时按下 ctrl+c 即可退出该模式。还有就是注意 ‘ 必须为英文符号（半角），若输入的是中文符号（全角）也会报错。 查看文件使用 cat，tac 和 nl 命令查看文件前两个命令都是用来打印文件内容到标准输出（终端），其中 cat 为正序显示，tac 为倒序显示。 标准输入输出：当我们执行一个 shell 命令行时通常会自动打开三个标准文件，即标准输入文件（stdin），默认对应终端的键盘、标准输出文件（stdout）和标准错误输出文件（stderr），后两个文件都对应被重定向到终端的屏幕，以便我们能直接看到输出内容。进程将从标准输入文件中得到输入数据，将正常输出数据输出到标准输出文件，而将错误信息送到标准错误文件中。 比如我们要查看之前从 &#x2F;etc 目录下拷贝来的 passwd 文件： cd &#x2F;home&#x2F;shiyanlou cp &#x2F;etc&#x2F;passwd passwd cat passwd 可以加上 -n 参数显示行号： cat -n passwd nl 命令，添加行号并打印，这是个比 cat -n 更专业的行号打印命令。 这里简单列举它的常用的几个参数： -b : 指定添加行号的方式，主要有两种： -b a:表示无论是否为空行，同样列出行号(“cat -n”就是这种方式) -b t:只列出非空行的编号并列出（默认为这种方式） -n : 设置行号的样式，主要有三种： -n ln:在行号字段最左端显示 -n rn:在行号字段最右边显示，且不加 0 -n rz:在行号字段最右边显示，且加 0 -w : 行号字段占用的位数(默认为 6 位) 你会发现使用这几个命令，默认的终端窗口大小，一屏显示不完文本的内容，得用鼠标拖动滚动条或者滑动滚轮才能继续往下翻页，要是可以直接使用键盘操作翻页就好了，那么你就可以使用下面要介绍的命令。 使用 more 和 less 命令分页查看文件如果说上面的 cat 是用来快速查看一个文件的内容的，那么这个 more 和 less 就是天生用来”阅读”一个文件的内容的，比如说 man 手册内部就是使用的 less 来显示内容。其中 more 命令比较简单，只能向一个方向滚动，而 less 为基于 more 和 vi （一个强大的编辑器，我们有单独的课程来让你学习）开发，功能更强大。less 的使用基本和 more 一致，具体使用请查看 man 手册，这里只介绍 more 命令的使用。 使用 more 命令打开 passwd 文件： more passwd 打开后默认只显示一屏内容，终端底部显示当前阅读的进度。可以使用 Enter 键向下滚动一行，使用 Space 键向下滚动一屏，按下 h 显示帮助，q 退出。 使用 head 和 tail 命令查看文件这两个命令，那些性子比较急的人应该会喜欢，因为它们一个是只查看文件的头几行（默认为 10 行，不足 10 行则显示全部）和尾几行。还是拿 passwd 文件举例，比如当我们想要查看最近新增加的用户，那么我们可以查看这个 &#x2F;etc&#x2F;passwd 文件，不过我们前面也看到了，这个文件里面一大堆乱糟糟的东西，看起来实在费神啊。因为系统新增加一个用户，会将用户的信息添加到 passwd 文件的最后，那么这时候我们就可以使用 tail 命令了： tail &#x2F;etc&#x2F;passwd 甚至更直接的只看一行， 加上 -n 参数，后面紧跟行数： tail -n 1 &#x2F;etc&#x2F;passwd 关于 tail 命令，不得不提的还有它一个很牛的参数 -f，这个参数可以实现不停地读取某个文件的内容并显示。这可以让我们动态查看日志，达到实时监视的目的。不过我不会在这门基础课程中介绍它的更多细节，感兴趣的用户可以自己去了解。 查看文件类型我们可以使用 file 命令查看文件的类型： file &#x2F;bin&#x2F;ls 说明这是一个可执行文件，运行在 64 位平台，并使用了动态链接文件（共享库）。 与 Windows 不同的是，如果你新建了一个 shiyanlou.txt 文件，Windows 会自动把它识别为文本文件，而 file 命令会识别为一个空文件。这个前面我提到过，在 Linux 中文件的类型不是根据文件后缀来判断的。当你在文件里输入内容后才会显示文件类型。 编辑文件在 Linux 下面编辑文件通常我们会直接使用专门的命令行编辑器比如（emacs，vim，nano），由于涉及 Linux 上的编辑器的内容比较多，且非常重要，故我们有一门单独的基础课专门介绍这中一个编辑器 vim 。 强烈建议正在学习这门 Linux 基础课的你先在这里暂停一下，去学习 vim 编辑器的使用（至少掌握基本的操作），然后再继续本课程后面的内容，因为后面的内容会假设你已经学会了 vim 编辑器的使用。 如果你想更加快速地入门，可以直接使用 Linux 内部的 vim 学习教程，输入如下命令即可开始： vimtutor #### 挑战：寻找文件介绍有一个非常重要的文件（sources.list）但是你忘了它在哪了，你依稀记得它在 &#x2F;etc&#x2F; 目录下，现在要你把这个文件找出来，然后设置成自己（shiyanlou 用户）可以访问，但是其他用户并不能访问。 目标 找到 sources.list 文件 把文件所有者改为自己 把权限修改为仅仅只有自己可读可写 提示 find chmod chown sudo 参考答案 sudo find &#x2F;etc -name sources.listsudo chown shiyanlou &#x2F;etc&#x2F;apt&#x2F;sources.listsudo chmod 600 &#x2F;etc&#x2F;apt&#x2F;sources.list Linux 用户管理Linux 是一个可以实现多用户登录的操作系统，比如“李雷”和“韩梅梅”都可以同时登录同一台主机，他们共享一些主机的资源，但他们也分别有自己的用户空间，用于存放各自的文件。但实际上他们的文件都是放在同一个物理磁盘上的甚至同一个逻辑分区或者目录里，但是由于 Linux 的 用户管理 和 权限机制，不同用户不可以轻易地查看、修改彼此的文件。 下面我们就来学习一下 Linux 下的账户管理的基础知识。 查看用户请打开终端，输入命令： who am i # 或者 who mom likes 输出的第一列表示打开当前伪终端的用户的用户名（要查看当前登录用户的用户名，去掉空格直接使用 whoami 即可），第二列的 pts&#x2F;0 中 pts 表示伪终端，所谓伪是相对于 &#x2F;dev&#x2F;tty 设备而言的 还有一点需要注意的是，在某些环境中 who am i 和 who mom likes 命令不会输出任何内容，这是因为当前使用的 SHELL 不是登录时的 SHELL，没有用户与 who 的 stdin 相关联，因此不会输出任何内容。例如我在本地的 Ubuntu 系统上输入这个命令就不会有提示。 who 命令其它常用参数 参数****说明 创建用户在 Linux 系统里， root 账户拥有整个系统至高无上的权限，比如新建和添加用户。 root 权限，系统权限的一种，与 SYSTEM 权限可以理解成一个概念，但高于 Administrator 权限，root 是 Linux 和 UNIX 系统中的超级管理员用户帐户，该帐户拥有整个系统至高无上的权力，所有对象他都可以操作，所以很多黑客在入侵系统的时候，都要把权限提升到 root 权限，这个操作等同于在 Windows 下就是将新建的非法帐户添加到 Administrators 用户组。更比如安卓操作系统中（基于 Linux 内核）获得 root 权限之后就意味着已经获得了手机的最高权限，这时候你可以对手机中的任何文件（包括系统文件）执行所有增、删、改、查的操作。 大部分 Linux 系统在安装时都会建议用户新建一个用户而不是直接使用 root 用户进行登录，当然也有直接使用 root 登录的例如 Kali（基于 Debian 的 Linux 发行版，集成大量工具软件，主要用于数字取证的操作系统）。一般我们登录系统时都是以普通账户的身份登录的，要创建用户需要 root 权限，这里就要用到 sudo 这个命令了。不过使用这个命令有两个大前提，一是你要知道当前登录用户的密码，二是当前用户必须在 sudo 用户组。 需要注意 Linux 环境下输入密码是不会显示的。 su 可以切换到用户 user，执行时需要输入目标用户的密码，sudo 可以以特权级别运行 cmd 命令，需要当前用户属于 sudo 组，且需要输入当前用户的密码。su - 命令也是切换用户，但是同时用户的环境变量和工作目录也会跟着改变成目标用户所对应的。 现在我们新建一个叫 lilei 的用户： sudo adduser lilei 环境目前设置为 root 用户执行 sudo 不需要输入密码，通常此处需要按照提示输入 root密码（Linux 下密码输入是不显示任何内容的），root用户密码可以通过 sudo passwd root 命令进行设置。然后是给 lilei 用户设置密码，后面的选项的一些内容你可以选择直接回车使用默认值。 这个命令不但可以添加用户到系统，同时也会默认为新用户在 &#x2F;home 目录下创建一个工作目录： ls &#x2F;home su root &#x2F;&#x2F;切换到root sudo passwd lilei &#x2F;&#x2F; 设置用户密码 现在你已经创建好一个用户，并且你可以使用你创建的用户登录了，使用如下命令切换登录用户： su -l lilei 输入刚刚设置的 lilei 的密码，然后输入如下命令并查看输出： who am i whoami &#x2F;&#x2F; 当前用户 pwd 你发现了区别了吗？这就是上一小节我们讲到的 who am i 和 whoami 命令的区别。 退出当前用户跟退出终端一样，可以使用 exit 命令或者使用快捷键 Ctrl+D。 文件权限文件权限就是文件的访问控制权限，即哪些用户和组群可以访问文件以及可以执行什么样的操作。 Unix&#x2F;Linux 系统是一个典型的多用户系统，不同的用户处于不同的地位，对文件和目录有不同的访问权限。为了保护系统的安全性，Unix&#x2F;Linux 系统除了对用户权限作了严格的界定外，还在用户身份认证、访问控制、传输安全、文件读写权限等方面作了周密的控制。 在 Unix&#x2F;Linux 中的每一个文件或目录都包含有访问权限，这些访问权限决定了谁能访问和如何访问这些文件和目录。 查看文件权限我们之前已经很多次用到 ls 命令了，如你所见，我们用它来列出并显示当前目录下的文件，当然这是在不带任何参数的情况下，它能做的当然不止这么多，现在我们就要用它来查看文件权限。 使用较长格式列出文件： ls -l ) 你可能除了知道最后面那一项是文件名之外，其它项就不太清楚了，那么到底是什么意思呢： 可能你还是不太明白，比如第一项文件类型和权限那一堆东西具体指什么，链接又是什么，何为最后修改时间，下面一一道来： 文件类型 关于文件类型，这里有一点你必需时刻牢记 Linux 里面一切皆文件，正因为这一点才有了设备文件（ &#x2F;dev 目录下有各种设备文件，大都跟具体的硬件设备相关）这一说。 socket：网络套接字，具体是什么，感兴趣的用户可以去学习实验楼的后续相关课程。pipe 管道，这个东西很重要，我们以后将会讨论到，这里你先知道有它的存在即可。软链接文件：链接文件是分为两种的，另一种当然是“硬链接”（硬链接不常用，具体内容不作为本课程讨论重点，而软链接等同于 Windows 上的快捷方式，你记住这一点就够了）。 文件权限 读权限，表示你可以使用 cat 之类的命令来读取某个文件的内容；写权限，表示你可以编辑和修改某个文件的内容； 执行权限，通常指可以运行的二进制程序文件或者脚本文件，如同 Windows 上的 exe 后缀的文件，不过 Linux 上不是通过文件后缀名来区分文件的类型。你需要注意的一点是，一个目录同时具有读权限和执行权限才可以打开并查看内部文件，而一个目录要有写权限才允许在其中创建其它文件，这是因为目录文件实际保存着该目录里面的文件的列表等信息。 所有者权限，这一点相信你应该明白了，至于所属用户组权限，是指你所在的用户组中的所有其它用户对于该文件的权限，比如，你有一个 iPad，那么这个用户组权限就决定了你的兄弟姐妹有没有权限使用它破坏它和占有它。 链接数 链接到该文件所在的 inode 结点的文件名数目（关于这个概念涉及到 Linux 文件系统的相关概念知识，不在本课程的讨论范围，感兴趣的用户可以查看 硬链接和软链接的联系与区别）。 文件大小 以 inode 结点大小为单位来表示的文件大小，你可以给 ls 加上 -lh 参数来更直观的查看文件的大小。 明白了文件权限的一些概念，我们顺带补充一下关于 ls 命令的一些其它常用的用法： 显示除了 .（当前目录）和 ..（上一级目录）之外的所有文件，包括隐藏文件（Linux 下以 . 开头的文件为隐藏文件）。 ls -a ) 当然，你可以同时使用 -a 和 -l 参数： ls -al 查看某一个目录的完整属性，而不是显示目录里面的文件属性： ls -dl &lt;目录名&gt; 显示所有文件大小，并以普通人类能看懂的方式呈现： ls -asSh 其中小 s 为显示文件大小，大 S 为按文件大小排序，若需要知道如何按其它方式排序，可以使用 man ls 命令查询。 修改文件权限如果你有一个自己的文件不想被其他用户读、写、执行，那么就需要对文件的权限做修改。文件的权限有两种表示方式： 方式一：二进制数字表示 每个文件有三组固定的权限，分别对应拥有者，所属用户组，其他用户，记住这个顺序是固定的。文件的读写执行对应字母 rwx，以二进制表示就是 111，用十进制表示就是 7，对进制转换不熟悉的同学可以看看 进制转换。例如我们刚刚新建的文件 iphone11 的权限是 rw-rw-rw-，换成对应的十进制表示就是 666，这就表示这个文件的拥有者，所属用户组和其他用户具有读写权限，不具有执行权限。 如果我要将文件 iphone11 的权限改为只有我自己可以用那么就可以用这个方法更改它的权限。 为了演示，我先在文件里加点内容： echo “echo &quot;hello shiyanlou&quot;“ &gt; iphone11 然后修改权限： chmod 600 iphone11 ls -alh iphone11 切换到 lilei 用户，尝试写入和读取操作，可以看到 lilei 用户已经不能读写这个 iphone11 文件了： 方式二：加减赋值操作 要完成上述实验相同的效果，你可以： chmod go-rw iphone11 g、o 还有 u 分别表示 group（用户组）、others（其他用户） 和 user（用户），+ 和 - 分别表示增加和去掉相应的权限。","categories":[],"tags":[]},{"title":"","slug":"HM_011_微分方程","date":"2023-08-09T03:00:40.533Z","updated":"2023-08-07T08:49:47.000Z","comments":true,"path":"2023/08/09/HM_011_微分方程/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_011_%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/","excerpt":"","text":"微分方程一、常微分方程的基本概念1.微分方程$$y’&#x3D;2x$$ 含有未知函数的导数或微分的方程 2.微分方程的阶$$1阶方程$$ 微分方程中所出现的未知函数最高阶导数的阶数 3.微分方程的解$$y&#x3D;f(x)&#x3D;x^2$$ 满足微分方程的函数 4.微分方程的通解$$y&#x3D;f(x)&#x3D;x^2+c$$ 如果微分方程的解中包含任意常数，且任意常数的个数与微分方程的阶数相同 5.微分方程的特解$$y&#x3D;f(x)&#x3D;x^2+1$$ 微分方程的不含任意常数的解 6. 初始条件确定特解的一组常数 7.积分曲线方程的一个解在平面上对应一条曲线 二、一阶微分方程 y’&#x3D;f(x,y)1.可分离变量的方程$$\\begin{align}&amp;y’&#x3D;f(x)g(y)\\Leftrightarrow \\frac{dy}{dx}&#x3D;f(x)g(y)\\Leftrightarrow \\frac{dy}{g(y)}&#x3D;f(x)dx \\&amp;求解方法对两端积分\\Leftrightarrow \\int\\frac{dy}{g(y)}&#x3D;\\int f(x)dx\\\\end{align}$$ 2.齐次微分方程$$\\begin{align}&amp;\\frac{dy}{dx}&#x3D;\\Phi(\\frac{y}{x})\\&amp;方法：令u&#x3D;\\frac{y}{x},y&#x3D;ux,y’&#x3D;u+u’x\\\\end{align}$$ 例题$$\\begin{align}&amp;y’+\\frac{y}{x}&#x3D;(\\frac{y}{x})^2\\&amp;令u&#x3D;\\frac{y}{x},y&#x3D;ux,y’&#x3D;u+u’x\\&amp;\\Leftrightarrow u+u’x+u&#x3D;u^2\\&amp;\\Leftrightarrow u+\\frac{du}{dx}x+u&#x3D;u^2\\&amp;\\Leftrightarrow \\frac{du}{u^2-2u}&#x3D;\\frac{1}{x}dx\\&amp;\\Leftrightarrow \\int\\frac{du}{u^2-2u}&#x3D;\\int\\frac{1}{x}dx\\&amp;\\Leftrightarrow \\frac{1}{2}(\\ln|u-2|-\\ln|u|)&#x3D;\\ln|x|+C\\&amp;\\Leftrightarrow \\frac{u-2}{u}&#x3D;Cx^2\\\\end{align}$$ 3.一阶线性微分方程$$\\begin{align}&amp;形如y’+p(x)y&#x3D;Q(x)\\&amp;通解：y&#x3D;e^{-\\int p(x)dx}[\\int Q(x)e^{\\int p(x)dx}+C]\\\\end{align}$$ 4.伯努利方程$$\\begin{align}&amp;形如y’+p(x)y&#x3D;Q(x)y^n(n \\neq 0,1)\\&amp;方法：令u&#x3D;y^{1-n}\\&amp;y^{-\\alpha}y’+p(x)y^{1-\\alpha}&#x3D;Q(x)\\&amp;令u&#x3D;y^{1-\\alpha}\\&amp;(1-\\alpha)y^{-\\alpha}y’&#x3D;\\frac{du}{dx}\\\\end{align}$$ $$\\begin{align}&amp;观察到微分方程中包含y’’和y’和y,该方程为y’’&#x3D;f(y,y’)型\\&amp;令y’&#x3D;p,y’’&#x3D;p\\frac{dp}{dy}\\&amp;得yp\\frac{dp}{dy}+p^2&#x3D;0\\&amp;\\Leftrightarrow y\\frac{dp}{dy}+p&#x3D;0\\&amp;当p&#x3D;0时,该方程成立,但跟据初始条件y|{x&#x3D;0}&#x3D;1\\&amp;即p&#x3D;1与其不符\\&amp;\\Leftrightarrow \\int\\frac{dp}{p}&#x3D;-\\int\\frac{dy}{y}\\&amp;\\Leftrightarrow |py|&#x3D;e^c\\&amp;\\Leftrightarrow p&#x3D;\\frac{c}{y}\\&amp;\\Leftrightarrow \\frac{dy}{dx}&#x3D;\\frac{c}{y}\\&amp;根据初始条件y’|{x&#x3D;0}&#x3D;\\frac{1}{2}\\Leftrightarrow c&#x3D;\\frac{1}{2}\\&amp;\\Leftrightarrow y^2&#x3D;x+c\\&amp;跟据初始条件y|{x&#x3D;0}&#x3D;1,y^2&#x3D;x+1\\&amp;又y|{x&#x3D;0}&#x3D;1&gt;0,y&#x3D;\\sqrt{x+1}\\end{align}$$ ##### 5.全微分方程 三、可降阶的高阶方程$$\\begin{align}&amp;y’&#x3D;p&#x3D;\\frac{dy}{dx},y’’&#x3D;\\frac{dp}{dx}\\&amp;y’’&#x3D;f(x,y’)\\Leftrightarrow\\frac{dp}{dx}&#x3D;f(x,p)\\end{align}$$ $$\\begin{align}&amp;x\\frac{dp}{dx}+3p&#x3D;0\\&amp;\\int{\\frac{1}{p}dp}&#x3D;-\\frac{1}{3}\\int{\\frac{1}{x}dx}\\&amp;p&#x3D;\\frac{c}{x^3}\\&amp;\\frac{dy}{dx}&#x3D;\\frac{c}{x^3}\\&amp;y&#x3D;\\frac{c_2}{x^2}+c_1\\\\end{align}$$$$\\begin{align}&amp;y’&#x3D;p&#x3D;\\frac{dy}{dx},y’’&#x3D;\\frac{dp}{dx}\\&amp;可得\\frac{dp}{dy}&#x3D;f(y,p)无解\\&amp;令y’&#x3D;p&#x3D;\\frac{dy}{dx},y’’&#x3D;\\frac{dp}{dx}&#x3D;\\frac{dp}{dy}p\\&amp;得\\frac{dp}{dx}p&#x3D;f(y,y’)\\\\end{align}$$ $$\\begin{align}&amp;y\\frac{dp}{dy}p+p^2&#x3D;0\\&amp;\\int\\frac{dp}{p}&#x3D;-\\int\\frac{dy}{y}\\&amp;py&#x3D;c\\&amp;p&#x3D;\\frac{c}{y}\\&amp;\\frac{dy}{dx}&#x3D;\\frac{c}{y}\\&amp;因为y’|{x&#x3D;0}&#x3D;\\frac{1}{2}\\&amp;\\frac{dy}{dx}&#x3D;\\frac{\\frac{1}{2}}{y}\\&amp;y^2&#x3D;x+c\\&amp;因为y|{x&#x3D;0}&#x3D;1\\&amp;y&#x3D;\\sqrt{x+1}\\end{align}$$ 四、高阶线性微分方程 $$\\begin{align}&amp;y’’+py’+qy\\Leftrightarrow r^2+pr+q&#x3D;0\\&amp;共轭复根求法：x_{1},x_2&#x3D;\\frac{-b\\pm i\\sqrt{4ac-b^2}}{2a}(i^2&#x3D;-1)\\&amp;例：y&#x3D;xe^x是y’’+py’+qy&#x3D;0的解，\\&amp;(r-1)^2&#x3D;0\\&amp;r^2-2r+1&#x3D;0\\&amp;p&#x3D;-2,q&#x3D;1\\\\end{align}$$$$\\begin{align}&amp;r^2-r+\\frac{1}{4}&#x3D;0\\&amp;(r-\\frac{1}{2})^2&#x3D;0\\&amp;r_1&#x3D;r_2&#x3D;\\frac{1}{2}\\&amp;y&#x3D;e^{\\frac{1}{2}x}(C_1+C_2x)\\\\end{align}$$$$\\begin{align}&amp;r^2+2r+5&#x3D;0\\&amp;r_{1,2}&#x3D;\\frac{-2\\pm i\\sqrt{4-20}}{2}&#x3D;-1\\pm 2i\\&amp;y&#x3D;e^{-x}(C_1\\cos{2x}+C_2\\sin{2x})\\end{align}$$ $$\\begin{align}&amp;r^3-2r^2+r-2&#x3D;0\\&amp;r^2(r-2)+r-2&#x3D;0\\&amp;(r-2)(r^2+1)&#x3D;0\\&amp;r_1&#x3D;2,r_{2,3}&#x3D;\\pm i\\&amp;y&#x3D;C_1e^{2x}+C_2\\cos{x}+C_3\\sin{x}\\\\end{align}$$$$\\begin{align}&amp;解法核心：找到齐次特解\\ \\end{align}$$$$\\begin{align}&amp;D&#x3D;\\frac{d}{dt}\\&amp;令x&#x3D;e^t或t&#x3D;\\ln x,t’&#x3D;\\frac{dt}{dx}&#x3D;\\frac{1}{x}\\&amp;y’&#x3D;\\frac{dy}{dx}&#x3D;\\frac{dy}{dt}\\frac{dt}{dx}&#x3D;\\frac{1}{x}\\frac{dy}{dt}\\&amp;xy’&#x3D;\\frac{dy}{dt}&#x3D;Dy\\&amp;y’’&#x3D;(\\frac{1}{x}\\frac{dy}{dt})’&#x3D;(\\frac{dy}{dt})’\\frac{1}{x}-\\frac{1}{x^2}\\frac{dy}{dt}&#x3D;\\frac{d^2y}{dt^2}\\frac{dt}{dx}\\frac{1}{x}-\\frac{1}{x^2}\\frac{dy}{dt}&#x3D;\\frac{d^2y}{dt^2}\\frac{1}{x^2}-\\frac{1}{x^2}\\frac{dy}{dt}&#x3D;D(D-1)y\\\\end{align}$$ $$\\begin{align}&amp;D(D-1)y+4D+2y&#x3D;0\\&amp;r^2-r+4r+2&#x3D;0\\&amp;(r+1)(r+2)&#x3D;0\\&amp;y&#x3D;C_1e^{-t}+C_2e^{-2t}\\&amp;y&#x3D;\\frac{C_1}{x}+\\frac{C_2}{x^2}\\\\end{align}$$ 例题1.$$\\begin{align}&amp;若二阶常系数线性齐次微分方程y’’+ay’+by&#x3D;0的通解为y&#x3D;(C_1+C_2x)e^x,\\&amp;则非齐次方程y’’+ay’+by&#x3D;x满足条件y(0)&#x3D;2,y’(0)&#x3D;0的通解为？\\&amp;解：\\&amp;由线性齐次微分方程的通解可知r&#x3D;1是齐次方程的特征方程的二重根，\\&amp;则齐次方程的特征方程为(r-1)^2&#x3D;0,r^2-2r+1&#x3D;0\\&amp;可得a&#x3D;-2,b&#x3D;1\\&amp;非齐次方程为y’’-2y’+y&#x3D;x,即x&#x3D;e^{0x}x,解为线性解y*&#x3D;a’x+b’\\&amp;带入非齐次方程得0-2a’+a’x+b’&#x3D;0\\&amp;a’&#x3D;1,b’&#x3D;2\\&amp;则非齐次方程的通解为y&#x3D;(C_1+C_2x)e^x+x+2\\&amp;根据y(0)&#x3D;2,y’(0)&#x3D;0,得到y&#x3D;x(1-e^x)+2\\end{align}$$ 2.$$\\begin{align}&amp;设y&#x3D;\\frac{1}{2}e^{2x}+(x-\\frac{1}{3})e^x是二阶常系数非齐次线性微分方程\\&amp;y’’+ay’+by&#x3D;ce^x的一个特解,则a,b,c各为多少？\\&amp;解：\\&amp;y&#x3D;\\frac{1}{2}e^{2x}+xe^x-\\frac{1}{3}e^x对应y&#x3D;ay_1+by_2+y^*\\&amp;可知r&#x3D;2,r&#x3D;1是齐次方程的两个线性无关解\\&amp;特征方程为(r-1)(r-2)&#x3D;0,r^2-3r+2&#x3D;0\\&amp;a&#x3D;-3,b&#x3D;2,y’’-3y’+2y&#x3D;ce^x,将非线性解带入\\&amp;\\begin{cases}&amp;y’&#x3D;e^x+xe^x\\&amp;y’’&#x3D;2e^x+xe^x\\\\end{cases}\\&amp;得c&#x3D;-1\\&amp;a,b,c&#x3D;-3,2,-1\\\\end{align}$$","categories":[],"tags":[]},{"title":"","slug":"HM_010_泰勒公式","date":"2023-08-09T03:00:40.441Z","updated":"2023-08-07T08:49:46.000Z","comments":true,"path":"2023/08/09/HM_010_泰勒公式/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_010_%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F/","excerpt":"","text":"泰勒公式在数学中，泰勒公式是一个用函数在某点的信息描述其附近取值的公式。如果函数足够光滑的话，在已知函数在某一点的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。泰勒公式还给出了这个多项式和实际的函数值之间的偏差。 整体思想：用多项式函数逼近目标函数近似替代 以下推导为皮亚诺型余项的泰勒公式 1.泰勒公式的推导 $$(1)Sinx$$ 首先对f(x)&#x3D;Sinx进行n阶求导可以发先规律$$Sinx\\rightarrow Cosx\\rightarrow -Sinx\\rightarrow -Cosx$$用多项式函数近似代替$$g(x)&#x3D;\\sum_{i&#x3D;0}^{n}a_0x^i$$得到如下推导$$\\begin{align}g^{(0)}(x)&amp;&#x3D;Sinx &#x3D;a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+…+a_nx^n\\g^{(1)}(x)&amp;&#x3D;Cosx &#x3D;a_1x^0+2a_2x^1+3a_3x^2+4a_4x^3+5a_5x^4+…+a_nx^n\\g^{(2)}(x)&amp;&#x3D;-Sinx &#x3D;21a_2x^0+32a_3x^1+43a_4x^2+54a_5x^3+…+a_nx^n\\g^{(3)}(x)&amp;&#x3D;-Cosx&#x3D;321a_3x^0+432a_4x^1+543a_5x^2+…+a_nx^n\\g^{(4)}(x)&amp;&#x3D;Sinx&#x3D;4321a_4x^0+5432a_5x^1+…+a_nx^n\\g^{(5)}(x)&amp;&#x3D;Cosx&#x3D;54321a_5x^0+…+a_nx^n\\end{align}$$当x&#x3D;0时：$$\\begin{align}0&amp;&#x3D;a_0\\+1&amp;&#x3D;1a_1\\0&amp;&#x3D;21a_2\\-1&amp;&#x3D;321a_3\\0&amp;&#x3D;4321a_4\\+1&amp;&#x3D;54321a_5\\\\end{align}$$归纳得：$$a_k&#x3D; \\begin{cases}0 &amp; 除以四余数为0 \\\\frac{1}{k!} &amp; 除以四余数为1 \\0 &amp; 除以四余数为2 \\\\frac{-1}{k!} &amp; 除以四余数为3 \\\\end{cases}$$可以得出$$Sinx&#x3D;x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\frac{x^7}{7!}+…+(-1)^{n-1}\\frac{x^{2n-1}}{2n-1!}+o(x^{2x-1})$$根据上述思想和推到方法可以对其他基本初等函数进行泰勒展开 $$(2)e^x$$ 发现求导规律：$$e^x\\rightarrow e^x\\rightarrow e^x\\rightarrow e^x$$ $$\\begin{align}g^{(0)}(x)&amp;&#x3D;e^x &#x3D;a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+…+a_nx^n\\g^{(1)}(x)&amp;&#x3D;e^x &#x3D;a_1x^0+2a_2x^1+3a_3x^2+4a_4x^3+5a_5x^4+…+a_nx^n\\g^{(2)}(x)&amp;&#x3D;e^x &#x3D;21a_2x^0+32a_3x^1+43a_4x^2+54a_5x^3+…+a_nx^n\\ \\end{align}$$当x&#x3D;0时：$$\\begin{align}1&amp;&#x3D;a_0\\1&amp;&#x3D;1a_1\\1&amp;&#x3D;21*a_2\\\\end{align}$$归纳得$$\\begin{align}a_k&#x3D;\\frac{1}{k!}\\end{align}$$可以得出$$e^x&#x3D;1+x+\\frac{x^2}{2!}+\\frac{x^3}{3!}+…+\\frac{x^n}{n!}+o(x^n)$$ $$(3)ln(1+x)$$ 发现求导规律：$$ln(1+x)\\rightarrow (1+x)^{-1}\\rightarrow (-1)(1+x)^{-2}\\rightarrow (-2)(1+x)^{-3}$$ $$\\begin{align}g^{(0)}(x)&amp;&#x3D;ln(1+x) &#x3D;a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+…+a_nx^n\\g^{(1)}(x)&amp;&#x3D;(1+x)^{-1} &#x3D;a_1x^0+2a_2x^1+3a_3x^2+4a_4x^3+5a_5x^4+…+a_nx^n\\g^{(2)}(x)&amp;&#x3D;(-1)(1+x)^{-2} &#x3D;21a_2x^0+32a_3x^1+43a_4x^2+54a_5x^3+…+a_nx^n\\g^{(3)}(x)&amp;&#x3D;(-1)^2(1+x)^{-3}&#x3D;321a_3x^0+432a_4x^1+543a_5x^2+…+a_nx^n\\g^{(4)}(x)&amp;&#x3D;(-1)^3(1+x)^{-4}&#x3D;4321a_4x^0+5432a_5x^1+…+a_nx^n\\g^{(5)}(x)&amp;&#x3D;(-1)^4(1+x)^{-5}&#x3D;54321a_5x^0+…+a_nx^n\\end{align}$$当x&#x3D;0时：$$\\begin{align}0&amp;&#x3D;a_0\\1&amp;&#x3D;1a_1\\-1&amp;&#x3D;21a_2\\1&amp;&#x3D;321a_3\\-1&amp;&#x3D;4321a_4\\1&amp;&#x3D;54321*a_5\\\\end{align}$$归纳得$$\\begin{align}a_k&#x3D;\\frac{(-1)^{k-1}}{k!}\\end{align}$$可以得出$$ln(1+x)&#x3D;x-\\frac{x^2}{2!}+\\frac{x^3}{3!}+…+ \\frac{(-1)^{n-1}x^n}{n!}+o(x^n)$$ $$(4)Cosx$$ 发现求导规律：$$Cosx\\rightarrow -Sinx\\rightarrow -Cosx\\rightarrow Sinx\\rightarrow Cosx$$ $$\\begin{align}g^{(0)}(x)&amp;&#x3D;Cosx &#x3D;a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+…+a_nx^n\\g^{(1)}(x)&amp;&#x3D;-Sinx &#x3D;a_1x^0+2a_2x^1+3a_3x^2+4a_4x^3+5a_5x^4+…+a_nx^n\\g^{(2)}(x)&amp;&#x3D;-Cosx &#x3D;21a_2x^0+32a_3x^1+43a_4x^2+54a_5x^3+…+a_nx^n\\g^{(3)}(x)&amp;&#x3D;Sinx&#x3D;321a_3x^0+432a_4x^1+543a_5x^2+…+a_nx^n\\g^{(4)}(x)&amp;&#x3D;Cosx&#x3D;4321a_4x^0+5432a_5x^1+…+a_nx^n\\g^{(5)}(x)&amp;&#x3D;Sinx&#x3D;54321a_5x^0+…+a_nx^n\\end{align}$$当x&#x3D;0时：$$\\begin{align}1&amp;&#x3D;a_0\\0&amp;&#x3D;1a_1\\-1&amp;&#x3D;21a_2\\0&amp;&#x3D;321a_3\\1&amp;&#x3D;4321a_4\\0&amp;&#x3D;54321*a_5\\\\end{align}$$归纳得$$a_k&#x3D; \\begin{cases}\\frac{1}{k!}&amp; 除以四余数为0 \\0 &amp; 除以四余数为1 \\\\frac{-1}{k!} &amp; 除以四余数为2 \\0&amp; 除以四余数为3 \\\\end{cases}$$可以得出$$Cosx&#x3D;1-\\frac{x^{2}}{2!}+\\frac{x^4}{4!}-\\frac{x^6}{6!}+…+(-1)^{n}\\frac{x^{2n}}{2n!}+o(x^{2n})$$ $$(5)(1+x)^a$$ 发现求导规律：$$(1+x)^a\\rightarrow a(1+x)^{a-1}\\rightarrow a(a-1)(1+x)^{a-2}\\rightarrow a(a-1)(a-2)(1+x)^{a-3}$$ $$\\begin{align}g^{(0)}(x)&amp;&#x3D;(1+x)^a &#x3D;a_0x^0+a_1x^1+a_2x^2+a_3x^3+a_4x^4+a_5x^5+…+a_nx^n\\g^{(1)}(x)&amp;&#x3D;a(1+x)^{a-1} &#x3D;a_1x^0+2a_2x^1+3a_3x^2+4a_4x^3+5a_5x^4+…+a_nx^n\\g^{(2)}(x)&amp;&#x3D;a(a-1)(1+x)^{a-2} &#x3D;21a_2x^0+32a_3x^1+43a_4x^2+54a_5x^3+…+a_nx^n\\g^{(3)}(x)&amp;&#x3D;a(a-1)(a-2)(1+x)^{a-3}&#x3D;321a_3x^0+432a_4x^1+543a_5x^2+…+a_nx^n\\ \\end{align}$$当x&#x3D;0时：$$\\begin{align}&amp;1&#x3D;a_0\\&amp;a&#x3D;1a_1\\&amp;a(a-1)&#x3D;21a_2\\&amp;a(a-1)(a-2)&#x3D;321a_3\\\\end{align}$$归纳得$$a_k&#x3D;\\frac{a(a-1)(a-2)…(a-k+1)}{k!}$$可以得出$$(1+x)^a&#x3D;1+ax+\\frac{a(a-1)x^2}{2!}+\\frac{a(a-1)(a-2)x^3}{3!}+…+\\frac{a(a-1)(a-2)…(a-n+1)x^n}{n!}+o(x^n)$$ 2.皮亚诺与拉格朗日型余项（1)皮亚诺型余项泰勒公式$$\\begin{align}&amp;如果f(x)在点x_0有直至n阶的导数，则有\\&amp;f(x)&#x3D;f(x_0)+f’(x_0)(x-x_0)+\\frac{1}{2!}f’’(x_0)(x-x_0)^2+…+\\frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n+o[(x-x_0)^{n}]\\&amp;x_0&#x3D;0时，得到麦克劳林公式\\&amp;f(x)&#x3D;f(0)+f’(0)x+\\frac{1}{2!}f’’(0)x^2+…+\\frac{1}{n!}f^{(n)}(0)x^n+o(x^n)\\end{align}$$(2)拉格朗日余项泰勒公式$$\\begin{align}&amp;设函数f(x)在含有x_0的开区间(a,b)内有n+1阶的导数，则当x\\in(a,b)时有\\&amp;f(x)&#x3D;f(x_0)+f’(x_0)(x-x_0)+\\frac{1}{2!}f’’(x_0)(x-x_0)^2+…+\\frac{1}{n!}f^{(n)}(x_0)(x-x_0)^n+R_n(x)\\&amp;其中R_n(x)&#x3D;\\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{(n+1)},这里\\xi介于x_0与x之间，称为拉格朗日余项\\end{align}$$(3)区别 1、描述对象区别： 拉格朗日余项的泰勒公式是描述整体$$拉格朗日余项(整体)\\rightarrow \\begin{cases}最值\\不等式\\end{cases}$$ 皮亚诺余项的泰勒公式描述局部$$皮亚诺余项(整体)\\rightarrow \\begin{cases}极限\\极值\\end{cases}$$ 2、表达式区别： 其中拉格朗日余项使用的是具体表达式，为某个n+1阶导数乘以（x-x0)的(n+1)次方 皮亚诺型余项没有具体表达式只是一个高阶无穷小 Rn(x)&#x3D;0((x-x0)的n次方) 3、公式计算方式的区别 麦克劳林公式是泰勒公式中（在a&#x3D;0 ,记ξ&#x3D;θX）的一种特殊形式; 皮亚诺型余项为Rn(x) &#x3D; o(x^n)； 因此再展开时候只需根据要求","categories":[],"tags":[]},{"title":"","slug":"HM_009_二重积分","date":"2023-08-09T03:00:39.774Z","updated":"2023-08-07T08:49:46.000Z","comments":true,"path":"2023/08/09/HM_009_二重积分/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_009_%E4%BA%8C%E9%87%8D%E7%A7%AF%E5%88%86/","excerpt":"","text":"二重积分","categories":[],"tags":[]},{"title":"","slug":"HM_008_二元函数可微与偏导的联系​","date":"2023-08-09T03:00:39.739Z","updated":"2023-08-07T08:49:46.000Z","comments":true,"path":"2023/08/09/HM_008_二元函数可微与偏导的联系​/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_008_%E4%BA%8C%E5%85%83%E5%87%BD%E6%95%B0%E5%8F%AF%E5%BE%AE%E4%B8%8E%E5%81%8F%E5%AF%BC%E7%9A%84%E8%81%94%E7%B3%BB%E2%80%8B/","excerpt":"","text":"1.二元函数的可偏导** 在二元函数中，一元函数的可导的概念变为可偏导，导函数的概念变为偏导函数，具体看下例： 二元函数f(x,y)对x、y的偏导函数分别为： 在求二元函数的偏导函数时，都是假设另外一个变量为常量，然后对余下那个变量求导数。例如，f(x,y)对x的偏导函数，就是假设y为常量，然后f(x,y)对变量x求导数即得。 对于某一点，函数f(x, y)在该点的两个偏导数可能都存在、可能只存在一个、也可能都不存在。 在点(0, 0)的两个偏导数只存在一个的函数例子： 在点(0, 0)的两个偏导数都不存在的函数例子： 在点(0, 0)的两个偏导数都存在的函数例子： 对于上面三个例子，小编建议大家亲手去算算偏导数，这样能加深对二元函数偏导数的理解。 2.二元函数的可微 某一点可微描述的是函数增量与自变量增量之间的线性关系。在一元函数中，若线性主部的系数只与该点有关，则可微。以此类推，在二元函数中，若多个自变量的线性主部的系数都只与该点有关，则可微。下面分别列出一元函数、二元函数函数增量与自变量增量之间的关系式： 对于一特定点，当A、B为常数时，即A、B与自变量增量无关，则函数在该点可微，且A、B分别为函数在该点对x、y求偏导后的偏导数。 3.可微、可偏导、连续、导函数连续之间的关系 为了方便比较一元函数，小编先给出一元函数在某点C上关于可微、可导、连续、导函数连续的关系图。在图1中，函数f(x)可微与可导等价，因此可微与可导之间是双向箭头；在点C可微、可导必能得出函数f(x)在点C连续，但连续不能推出f(x)在点C可导、可微。因此可微、可导与连续之间是单向箭头。而导函数在点C连续，很明显就能推出函数在点C可导、可微、连续，但反过来，无法推出导函数在点C连续。 图1.一元函数可微、可导关系示意图 小编提醒大家，一定要经常记忆上图，而且是要理解性地记忆，比如说一元函数可微，要能明白可微是什么，关系式如何写！ 相比于一元函数，二元函数就复杂多了，下面先给出二元函数可微、可偏导、连续、导函数连续的关系图。 图2.多元函数可微、可偏导关系示意图 当然在记忆这些关系时，我们通常要花时间记忆的是那些不容易理解的关系，而这些不容易理解的关系是与一元函数相比较后的那些不同之处。 3.1可微与可偏导不等价 在阐述二元函数可微与可偏导不等价前，不妨先回顾下，为什么一元函数中可微与可导是等价的？ 在一元函数中，如果函数f(x)在x&#x3D;x0处可导，则有如下关系式： 假设在一元函数中，函数增量与自变量存在如下关系： 上式两边同除以△x，然后两边对△x取极限，可知A&#x3D;m，则根据一元函数可微的定义，A只与x&#x3D;x0有关，与△x无关，所以f(x)在x&#x3D;x0可微。同理，不难得出在一元函数中，可微亦可推出可导。 那么在二元函数中，如何论证可微必可推导呢？ 假设二元函数在点C(x0, y0)可微，则由可微的定义，必存在(x0, y0)的某邻域，使得下式成立： 不妨分别令△x&#x3D;0、△y&#x3D;0，根据①式可得： 之所以可以令△x&#x3D;0、△y&#x3D;0，是因为点(x0, y0+△y)和(x0+△x, y0)都在点(x0, y0)的可微邻域内。 对②中两式求极限，可得: 结合偏导数的定义和③中的两个极限，可知可微情况下，函数在点C的两个偏导数都存在，因此可微必可偏导。 尽管可微必可偏导，但反过来不成立，请看下面这个例子： 函数F在(0, 0)的两个偏导数都存在且为0，现在用反证法证明函数F在点(0, 0)不可微。假设函数F在原点可微，则根据可微定义，下列极限必存在，但是下列极限可以通过列举两条路径很容易验证不存在，原假设错误，所以可偏导不一定可微。 3.2 可偏导不一定连续 在二元函数关系图中，另外一个很让人费解的地方，是二元函数在某点的两个偏导数都存在，但是函数在这一点却不一定连续。为了说明这一点，请看下面这个函数： 相信大家都能很熟练地计算出函数F在原点对x、y的偏导数均为0，但是当曲线沿着y&#x3D;x的路径趋于原点时，函数值会趋于1，不等于0，因此函数F在原点不连续。 从抽象的角度看，二元函数在某一点的两个偏导数都存在，只能说明二元函数沿x方向、沿y方向趋于该点的值等于函数在该点的定义值，但无法保证沿其它方向趋于该点的值也等于函数值。","categories":[],"tags":[]},{"title":"","slug":"HM_007_向量代数与空间解析几何","date":"2023-08-09T03:00:39.690Z","updated":"2023-08-07T08:49:45.000Z","comments":true,"path":"2023/08/09/HM_007_向量代数与空间解析几何/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_007_%E5%90%91%E9%87%8F%E4%BB%A3%E6%95%B0%E4%B8%8E%E7%A9%BA%E9%97%B4%E8%A7%A3%E6%9E%90%E5%87%A0%E4%BD%95/","excerpt":"","text":"第四章 向量代数与空间解析几何第一节 向量及向量代数定义1：$$\\begin{align}&amp;(1)向量:\\vec{a}&#x3D;x\\vec{i}+y\\vec{j}+z\\vec{k}&#x3D;{x,y,z}\\&amp;(2)向量的模: |\\vec{a}|&#x3D;\\sqrt{x^2+y^2+z^2}\\&amp;(3)单位向量: |\\vec a|&#x3D;1,\\vec a&#x3D;(\\frac{x}{\\sqrt{x^2+y^2+z^2}},\\frac{y}{\\sqrt{x^2+y^2+z^2}},\\frac{z}{\\sqrt{x^2+y^2+z^2}})&#x3D;(\\cos \\alpha,\\cos\\beta,\\cos \\gamma)\\&amp;(4)向量\\vec{a}的方向余弦(方向数):方向角\\alpha,\\beta,\\gamma\\in[0,\\pi]\\&amp;\\vec{a}&#x3D;(\\cos \\alpha,\\cos\\beta,\\cos \\gamma)\\\\end{align}$$ 定理1：$$\\begin{align}&amp;设A(a_1,a_2,a_3),B(b_1,b_2,b_3)\\in R^3,则\\vec{AB}&#x3D;{b_1,b_2,b_3}-{a_1,a_2,a_3}&#x3D;{b_1-a_1,b_2-a_2,b_3-a_3}\\\\end{align}$$ 定义2$$\\begin{align}&amp;(1)线性运算:\\vec{a}+\\vec{b},\\vec{a}-\\vec{b}\\&amp;\\lambda\\vec{a}&#x3D;\\begin{cases}&amp;|\\lambda\\vec a|\\vec a,\\lambda&gt;0,即与\\vec a同向\\&amp;\\vec{0},\\lambda&#x3D;0,即为零向量\\&amp;-|\\lambda\\vec a|\\vec a,\\lambda&lt;0,即与\\vec a反向\\\\end{cases}\\end{align}$$$$\\begin{align}&amp;(2)数积(内积,点积):数积\\vec{a}\\cdot\\vec{b}是一个数\\&amp;\\vec{a}\\cdot\\vec{b}&#x3D;|\\vec a||\\vec b|\\cos \\theta\\&amp;注解：判定垂直\\&amp;(3)矢积(外积,叉积):矢积\\vec{a}\\vec{b}是一个向量,满足:\\&amp;[1]|\\vec a\\vec b|&#x3D;|\\vec{a}||\\vec{b}|\\sin \\theta\\&amp;[2]\\vec a\\vec b\\perp\\vec{a}和\\vec{b},\\vec{a},\\vec{b}和\\vec a\\vec b的方向满足右手法则\\&amp;注解：\\&amp;1)判断平行\\&amp;2)|\\vec{a}*\\vec{b}|的几何意义：\\\\end{align}$$$$\\begin{align}&amp;(4)混合积:[\\vec a\\vec b \\vec c]\\overset{\\Delta}{&#x3D;}(\\vec a *\\vec b)\\cdot \\vec c&#x3D;\\vec c\\cdot(\\vec a *\\vec b)&#x3D;(\\vec b *\\vec c)\\cdot \\vec a\\&amp;注解:[\\vec a\\vec b \\vec c]的几何意义是体积\\\\end{align}$$ 定理 2$$\\begin{align}&amp;设\\vec{a}&#x3D;{a_1,a_2,a_3},\\vec{b}&#x3D;{b_1,b_2,b_3}\\&amp;则\\lambda\\vec{a}+\\mu\\vec{b}&#x3D;{\\lambda a_1+\\mu b_1,\\lambda a_2+\\mu b_2,\\lambda a_3+\\mu b_3}\\\\end{align}$$ 定理 3 定理 4 定理 5 定理 6 例题 $$\\begin{align}&amp;解：\\&amp;(a,b,c)表示a,b,c的混合积,即(a,b,c)&#x3D;(aXb)..c .且(aXb)..c&#x3D;(bXc)..a&#x3D;(cXa)..b\\&amp;混合积几何意义为三向量空间围成的体积\\&amp;AXB方向尊崇右手定则 值为ABsin A..B&#x3D;ABcos\\&amp;因为 (a+b)X(b+c)&#x3D;aXb+aXc+bXb+bXc&#x3D;aXb+aXc+bXc\\&amp;所以 (a+b)X(b+c)..(c+a)&#x3D;(aXb+aXc+bXc)..(c+a)&#x3D;(a,b,c)+(a,b,a)+(a,c,c)+(a,c,a)+(b,c,c)+(b,c,a).\\&amp;因为 (aXb) 垂直于a\\&amp;所以 (a,b,a)&#x3D;(aXb)*a&#x3D;0,\\&amp;同理,(a,c,c)&#x3D;(a,c,a)&#x3D;(b,c,c)&#x3D;0.\\&amp;又因为 (a,b,c)&#x3D;2,\\&amp;所以 (b,c,a)&#x3D;(a,b,c)&#x3D;2.\\&amp;所以 (a+b)x(b+c)..(c+a) &#x3D;4.\\end{align}$$ 第二节 平面与直线平面点法式方程$$\\begin{align}&amp;\\vec{p_0p}\\cdot\\vec{n}&#x3D;0\\&amp;\\Leftrightarrow a(x-x_0)+b(y-y_0)+c(z-z_0)&#x3D;0\\&amp;\\vec{n}&#x3D;{a,b,c}\\neq0为法向量,P_0(x_0,y_0,z_0)为平面上的一点\\\\end{align}$$ 一般式方程$$\\begin{align}&amp;ax+by+cz+d&#x3D;0,\\vec{n}&#x3D;{a,b,c}\\neq0为法向量\\\\end{align}$$ 注解： 截距式方程$$\\begin{align}&amp;\\frac{x}{a}+\\frac{y}{b}+\\frac{z}{c}&#x3D;1\\&amp;a,b,c为平面在三个坐标轴上的截距\\&amp;注解:截距式方程只能表示和三个坐标轴都相交的平面\\end{align}$$ 点到面的距离$$\\begin{align}&amp;点P_0(x_0,y_0,z_0)到平面ax+by+cz+d&#x3D;0的距离\\&amp;\\rho&#x3D;\\frac{|ax_0+by_0+cz_0+d|}{\\sqrt{a^2+b^2+c^2}}\\end{align}$$ 两平面夹角（法向量夹角）$$\\begin{align}&amp;cos\\theta&#x3D;\\frac{\\vec{n_1}\\cdot\\vec{n_2}}{|\\vec{n_1}||\\vec{n_2}|}\\\\end{align}$$ 平面与平面的位置关系 直线标准式方程（点向式）$$\\begin{align}&amp;\\vec{p}&#x2F;&#x2F;\\vec{p_0p}\\&amp;\\frac{x-x_0}{l}&#x3D;\\frac{y-y_0}{m}&#x3D;\\frac{z-z_0}{n}\\&amp;其中P_0(x_0,y_0,z_0)为直线上一点,\\vec{L}&#x3D;{l,m,n}\\neq0为直线的方向向量\\end{align}$$ 一般式方程 参数式方程 点到直线的距离 两直线夹角 直线与直线的位置 直线与平面的位置关系 基本方法","categories":[],"tags":[]},{"title":"","slug":"HM_006_多元函数微分学","date":"2023-08-09T03:00:39.637Z","updated":"2023-08-07T08:49:45.000Z","comments":true,"path":"2023/08/09/HM_006_多元函数微分学/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_006_%E5%A4%9A%E5%85%83%E5%87%BD%E6%95%B0%E5%BE%AE%E5%88%86%E5%AD%A6/","excerpt":"","text":"第四章 多元函数微分学第一节 基本概念机结论定义1：（二元函数）$$\\begin{align}&amp;z&#x3D;f(x,y),(x,y)\\in D\\subset R^2\\\\end{align}$$ 例题$$\\begin{align}&amp;f(x,y)&#x3D;\\arcsin(2x)+\\ln y+\\frac{\\sqrt{4x-y^2}}{\\ln{(1-x^2-y^2)}}\\&amp;解：-1\\leq 2x\\leq 1,y&gt;0,1-x^2-y^2&gt;0,1-x^2-y^2\\neq1,4x-y^2\\geq0\\&amp;D&#x3D;{(x,y)|-\\frac{1}{2}\\leq x\\leq\\frac{1}{2},y&gt;0,x^2+y^2&lt;1,x\\geq\\frac{1}{4}y^2}\\\\end{align}$$ 定义2：（二元函数的极限）$$\\begin{align}&amp;\\lim_{(x,y)\\rightarrow(x_0,y_0)}f(x,y)&#x3D;A或\\lim_{x\\rightarrow x_0,y\\rightarrow y_0}f(x,y)&#x3D;A\\\\end{align}$$ 例题$$\\begin{align}&amp;求极限\\&amp;解法1；\\&amp;\\lim_{x\\rightarrow 0,y\\rightarrow 0}xy\\frac{x^2-y^2}{x^2+y^2}\\&amp;\\xlongequal[x&#x3D;r\\cos\\theta]{y&#x3D;r\\sin\\theta}\\lim_{r\\rightarrow0}{r\\cos\\theta r\\sin\\theta\\frac{r^2\\cos2\\theta}{r^2}}&#x3D;0\\&amp;解法2：\\&amp;0\\leq|xy\\frac{x^2-y^2}{x^2+y^2}|\\leq{|xy|}\\&amp;\\lim_{x\\rightarrow 0,y\\rightarrow 0}{0}&#x3D;0\\leq\\lim_{x\\rightarrow 0,y\\rightarrow 0}|xy\\frac{x^2-y^2}{x^2+y^2}|\\leq\\lim_{x\\rightarrow 0,y\\rightarrow 0}|xy|&#x3D;0\\&amp;\\Leftrightarrow\\lim_{x\\rightarrow 0,y\\rightarrow 0}xy\\frac{x^2-y^2}{x^2+y^2}&#x3D;0\\&amp;\\&amp;验证极限不存在\\&amp;(1)\\lim_{x\\rightarrow 0,y\\rightarrow 0}\\frac{xy}{x^2+y^2}\\&amp;(2)\\lim_{x\\rightarrow 0,y\\rightarrow 0}\\frac{x^3+y^3}{x^2+y}\\&amp;解：\\&amp;(1)令y&#x3D;kx\\&amp;\\lim_{x\\rightarrow0,y&#x3D;kx}\\frac{xkx}{x^2+k^2x^2}&#x3D;\\frac{k}{1+k^2}\\&amp;(2)令y&#x3D;-x^2+x^4\\&amp;\\lim_{x\\rightarrow0,y&#x3D;-x^2+x^4}\\frac{x^3+(x^4-x^2)^3}{x^2+(-x^2+x^4)}&#x3D;\\lim_{x\\rightarrow0}{[\\frac{1}{x}}+\\frac{(x^4-x^2)^2}{x^4}]&#x3D;\\infty\\\\end{align}$$注解：$$\\begin{align}&amp;一元函数{(x,f(x))|x\\in D}\\&amp;多元函数{(x,y,f(x,y))|(x,y)\\in D}\\end{align}$$ 定义3（二院函数的连续性）$$\\begin{align}&amp;f(x,y)在点P_0处连续:\\lim_{x\\rightarrow x_0,y\\rightarrow y_0}f(x,y)&#x3D;f(x_0,y_0)\\&amp;注解1：z&#x3D;f(x,y)于P_0点连续\\Leftrightarrow\\Delta z&#x3D;f(x,y)-f(x_0,y_0)\\rightarrow0(x\\rightarrow x_0,y\\rightarrow y_0)\\&amp;注解2：”二元初等函数”在其定义域内处处连续，\\lim_{x\\rightarrow 1,y\\rightarrow2}\\frac{x+y}{x-y}&#x3D;-3\\\\end{align}$$ 定理1$$\\begin{align}&amp;有界闭区域D\\subset{R}上的连续函数,必有界,且有最大值最小值 \\\\end{align}$$ 定义4（偏导数）$$\\begin{align}&amp;\\frac{\\partial{z}}{\\partial x}|{(x_0,y_0)}&#x3D;\\lim{\\Delta x\\rightarrow 0}\\frac{\\Delta Z_x}{\\Delta x}&#x3D;\\lim_{\\Delta x\\rightarrow0}\\frac{f(x_0+\\Delta x,y_0)-f(x_0,y_0)}{\\Delta x}\\&amp;f_y’(x_0,y_0)&#x3D;\\lim_{\\Delta y\\rightarrow0}\\frac{\\Delta Z_y}{\\Delta y}&#x3D;\\lim_{y\\rightarrow y_0}\\frac{f(x_0,y_)-f(x_0,y_0)}{y-y_0}\\end{align}$$ 例题$$\\begin{align}&amp;(1)设f(x,y)&#x3D;\\begin{cases}&amp;\\frac{xy}{x^2+y^2},(x,y)\\neq(0,0)\\&amp;0,(x,y)&#x3D;(0,0)\\\\end{cases},求f_x’(0,0)和f’y(0,0),但\\lim{x\\rightarrow 0,y\\rightarrow 0}f(x,y)不存在\\&amp;(2)求f(x,y)&#x3D;\\sqrt{x^2+y^2}在(0,0)处的偏导数，并说明函数在此点的连续性\\\\end{align}$$$$\\begin{align}&amp;（1）f_x’(0,0)&#x3D;\\lim_{x\\rightarrow0}\\frac{f(x,0)-f(0,0)}{x-0}&#x3D;\\lim_{x\\rightarrow 0}{\\frac{0-0}{x}}&#x3D;0\\&amp;同理得f_y’(0,0)&#x3D;0&amp;\\\\end{align}$$ $$\\begin{align}&amp;\\lim_{x\\rightarrow 0^{\\pm}}\\frac{f(x,0)-f(0,0)}{x-0}&#x3D;\\lim_{x\\rightarrow x^{\\pm}}\\frac{\\sqrt{x^2}-0}{x}&#x3D;\\pm 1\\&amp;\\Rightarrow f’_x(0,0),f’_y(0,0)不存在\\\\end{align}$$ 定义5（全微分）$$\\begin{align}&amp;若z&#x3D;f(x,y),\\Delta z&#x3D;A\\Delta x+B\\Delta y+o(\\rho)(\\rho\\rightarrow 0),\\rho&#x3D;\\sqrt{\\Delta x^2+\\Delta y^2},则称z&#x3D;f(x,y)在点P_0可微，\\&amp;dz|{(x_0,y_0)}&#x3D;df|{(x_0,y_0)}&#x3D;A\\Delta x+B\\Delta y\\end{align}$$ 注解：$$\\begin{align}&amp;(1)若\\exist A,B使得\\lim_{\\Delta x\\rightarrow0,\\Delta y\\rightarrow 0}\\frac{\\Delta f-A\\Delta x-B\\Delta y}{\\sqrt{\\Delta x^2+\\Delta y^2}}&#x3D;0,则f于(0,0)可微\\&amp;(2)若f于(x_0,y_0)可微,则\\lim\\frac{\\Delta f-df}{\\sqrt{\\Delta x^2+\\Delta y^2}}&#x3D;0\\end{align}$$ 定理2$$\\begin{align}&amp;若z&#x3D;f(x,y)在点P_0(x_0,y_0)处可微,则偏导数存在，且dz|_{(x_0,y_0)}&#x3D;f’_x(x_0,y_0)dx+f_y’(x_0,y_0)dy\\\\end{align}$$ 定理3 几个命题之间的关系$$\\begin{align}&amp;可微\\Rightarrow\\begin{cases}&amp;连续\\Rightarrow极限存在\\&amp;偏导数存在\\&amp;方向导数存在\\\\end{cases}\\\\end{align}$$ 二元函数可微与偏导的联系 - ebxeax - 博客园 (cnblogs.com) 例题 注解：$$\\begin{align}&amp;可微充分条件\\Leftrightarrow偏导函数在(x_0,y_0)处连续\\Rightarrow 可微\\\\end{align}$$ 第二节 多元函数微分法初等函数的微分法 注解：$$\\begin{align}&amp;(1)对x把y看做常数，对y把x看成常数\\&amp;(2)u\\begin{cases}&amp;u_x’\\begin{cases}&amp;u’’{xx}\\&amp;u’’{xy}\\\\end{cases}\\&amp;u_y’\\begin{cases}&amp;u’’{yx}\\&amp;u’’{yy}\\\\end{cases}\\\\end{cases}当偏导函数连续时,u’’{xy}&#x3D;u’’{yx}\\end{align}$$ 例题$$\\begin{align}&amp;1.设z&#x3D;\\arcsin\\frac{x}{\\sqrt{x^2+y^2}}，求\\frac{\\partial^2z}{\\partial x^2},\\frac{\\partial^2z}{\\partial x\\partial y}\\&amp;解\\&amp;z’x&#x3D;\\frac{|y|}{x^2+y^2}\\&amp;z’’{xx}&#x3D;\\frac{\\partial}{\\partial x}(\\frac{|y|}{x^2+y^2})&#x3D;\\begin{cases}&amp;\\frac{-2xy}{(x^2+y^2)^2},y&gt;0\\&amp;0,x\\neq0,y&#x3D;0\\&amp;\\frac{2xy}{(x^2+y^2)^2},y&gt;0\\end{cases}\\&amp;z’’_{xy}&#x3D;\\frac{\\partial}{\\partial y}(\\frac{|y|}{x^2+y^2})&#x3D;\\begin{cases}&amp;\\frac{x^2-y^2}{(x^2+y^2)^2},y&gt;0\\&amp;不存在,x\\neq 0,y&#x3D;0\\&amp;\\frac{y^2-x^2}{(x^2+y^2)^2},y&lt;0\\\\end{cases}\\\\end{align}$$ 复合函数微分法 $$\\begin{align}&amp;\\frac{\\partial z}{\\partial x}&#x3D;-\\frac{1}{x^2}f(xy)+\\frac{1}{x}f’(xy)y+y\\phi’(x+y)\\&amp;\\frac{\\partial^2 z}{\\partial x\\partial y}&#x3D;-\\frac{1}{x^2}f’(xy)x+\\frac{1}{x}[f’’(xy)xy+f’(xy)]+\\phi’(x+y)+y\\phi’’(x+y)\\\\end{align}$$3. $$\\begin{align}&amp;法1：\\&amp;\\frac{\\partial f}{\\partial x}&#x3D;e^{-(xy)^2}y-e^{-(x+y)^2}\\&amp;\\frac{\\partial^2 f}{\\partial x\\partial y}&#x3D;\\&amp;法2：\\&amp;令u&#x3D;x+y,v&#x3D;xy,f(x,y)由\\int_v^ue^{-t^2}dt与\\begin{cases}&amp;u&#x3D;x+y\\&amp;v&#x3D;xy\\\\end{cases}复合\\&amp;\\frac{\\partial f}{\\partial x}&#x3D;e^{-(v)^2}\\frac{\\partial u}{\\partial x}-e^{-(u)^2}\\frac{\\partial v}{\\partial x}\\&amp;\\frac{\\partial^2 f}{\\partial x\\partial y}&#x3D;\\\\end{align}$$ 多元隐函数的微分法 例题： $$\\begin{align}&amp;法1:公式法\\&amp;F(x,y,u)&#x3D;u+e^u-xy,u’_x&#x3D;\\frac{\\partial u}{\\partial x}&#x3D;-\\frac{F’_x}{F’_x}&#x3D;-\\frac{-y}{1+e^u}\\&amp;法2:\\&amp;u’_x+e^uu’_x&#x3D;y,u_x’&#x3D;\\frac{y}{1+e^u},u’_y&#x3D;{\\frac{x}{1+e^u}}\\&amp;\\frac{\\partial ^2u}{\\partial x\\partial y}&#x3D;{\\frac{1*[1+e^u]-ye^uu_y’}{[1+e^u]^2}}\\\\end{align}$$ 多元函数的极值与最值求法 无条件极值（二元）$$\\begin{align}&amp;(1)定义\\&amp;(2)必要条件\\begin{cases}&amp;z_x’(x_0,y_0)&#x3D;0\\&amp;z_y’(x_0,y_0)&#x3D;0\\\\end{cases}\\&amp;(3)充分条件\\Delta&#x3D;AC-B^2\\begin{cases}&amp;&gt;0\\&amp;&lt;0\\\\end{cases}\\&amp;A&#x3D;f_{xx}’’(x_0,y_0),B&#x3D;f_{xy}’’(x_0,y_0),C&#x3D;f’’_{yy}(x_0,y_0)\\\\end{align}$$ 有界闭区域$$\\begin{align}&amp;有界闭区域D上的连续函数f(x,y)的最值：\\&amp;如果函数f(x,y)在有界闭区域D\\subset R^2上连续，则f(x,y)必在D上取得最大值和最小值\\\\end{align}$$ 解题步骤 例题：","categories":[],"tags":[]},{"title":"","slug":"HM_005_不定积分、定积分与反常积分","date":"2023-08-09T03:00:39.588Z","updated":"2023-08-07T08:49:45.000Z","comments":true,"path":"2023/08/09/HM_005_不定积分、定积分与反常积分/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_005_%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86%E3%80%81%E5%AE%9A%E7%A7%AF%E5%88%86%E4%B8%8E%E5%8F%8D%E5%B8%B8%E7%A7%AF%E5%88%86/","excerpt":"","text":"不定积分、定积分与反常积分不定积分一、不定积分概念1.定义$$\\begin{align}&amp;原函数：设对于区间I上的任意一点x均有F’(x)&#x3D;f(x),则称F(x)为f(x)在区间I上的一个原函数\\&amp;不定积分：设函数f(x)于区间I上有原函数,则其余原函数的全体称为f(x)于区间I上的不定积分,记为\\int{f(x)dx}\\&amp;线性：\\int[\\alpha f(x)+\\beta g(x)]dx&#x3D;\\alpha\\int f(x)dx+\\beta\\int g(x)dx\\\\end{align}$$ 2.计算$$\\begin{align}&amp;计算方法\\begin{cases}&amp;1.基本公式\\&amp;2.线性\\&amp;3.积分法\\begin{cases}&amp;1.换元法\\&amp;2.分部积分法\\\\end{cases}\\\\end{cases}\\\\end{align}$$ (1)第一换元法(凑微分)$$\\begin{align}&amp;设F’(u)&#x3D;f(u),则\\int{f(\\Phi(x))\\Phi’(x)}dx&#x3D;\\int{f(\\Phi(x))d(\\Phi(x))}&#x3D;F(\\Phi(x))+C\\&amp;注解：找到合适的凑微分\\Phi’(x)dx&#x3D;d(\\Phi(x))\\end{align}$$ 常见凑微分：$$\\begin{align}&amp;1.\\int{f(ax+b)dx&#x3D;\\frac{1}{a}\\int{f(ax+b)d(ax+b)}}(a\\neq0)\\&amp;eg1.\\int{\\sin (2x+3)}dx&#x3D;\\frac{1}{2}\\int\\sin (2x+3)d(2x+3)&#x3D;\\frac{1}{2}\\cos{(2x+3)}+C\\&amp;2.\\int{f(ax^n+b)x^{n-1}dx}&#x3D;\\frac{1}{na}\\int{f(ax^n+b)d(ax^n+b)}\\&amp;eg2.\\int{\\cos(2x^4+3)x^3dx}&#x3D;\\frac{1}{4*2}\\int{\\cos(2x^4+3)d(2x^4+3)}&#x3D;\\frac{1}{8}\\cos{(2x^4+3)}+C\\&amp;3.\\int{f(a^x+c)a^xdx}&#x3D;\\frac{1}{\\ln{a}}\\int{f(a^x+c)}d(a^x+c)\\&amp;eg3.\\int{\\sin(2^x+3)2^xdx}&#x3D;\\frac{1}{\\ln2}\\int{\\sin{(2^x+3)}d(2^x+3)}&#x3D;\\frac{1}{\\ln 2}\\cos{(2^x+3)}\\&amp;4.\\int{f(\\frac{1}{x})\\frac{1}{x^2}}dx&#x3D;-\\int{f(\\frac{1}{x})}d(\\frac{1}{x})\\&amp;eg4.\\int{\\ln(\\frac{1}{x})}\\frac{1}{x^2}dx&#x3D;-\\int\\ln (\\frac{1}{x})d({\\frac{1}{x}})+C\\&amp;5.\\int{f(\\ln |x|})\\frac{1}{x}d(x)&#x3D;\\int{f(\\ln{|x|)}}{d(\\ln|x|)}\\&amp;eg5.\\int{\\sin ({\\ln{|x|}}})\\frac{1}{x}dx&#x3D;\\int{\\sin(\\ln(|x|)d(\\ln{|x|})}&#x3D;\\cos(\\ln x)+C\\&amp;6.\\int{f(\\sqrt x)\\frac{1}{\\sqrt x}}dx&#x3D;2\\int{f(\\sqrt x)}d(\\sqrt x)\\&amp;7.\\int f(\\sin x)\\cos xdx&#x3D;-\\int{(\\sin x)}d(\\sin x)\\&amp;8.\\int{f(\\cos x)\\sin dx}&#x3D;\\int{f(\\cos x)d(\\cos x)}\\&amp;9.\\int{f(\\tan x)\\sec^2 xdx}&#x3D;\\int{f(\\tan x)d(\\tan x)}\\&amp;10.\\int{f(\\cot x)\\csc^2xdx}&#x3D;-\\int{f(\\cot x)d{(\\cot x)}}\\&amp;11.\\int{f{(\\arcsin x)\\frac{1}{\\sqrt{1-x^2}}}}dx&#x3D;\\int{f(\\arcsin x)d({\\arcsin x})}\\&amp;12.\\int{f(\\arccos x)(-\\frac{1}{\\sqrt{1-x^2}}})dx&#x3D;\\int{f(\\arccos x)d(\\arccos x)}\\&amp;13.\\int{f(\\arctan x)\\frac{1}{1+x^2}dx}&#x3D;\\int{f(\\arctan x)d(\\arctan x)}\\&amp;14.\\int{f(\\sqrt{x^2+a})}\\frac{x}{\\sqrt{x^2+a}}dx&#x3D;\\int{f(\\sqrt{x^2+a})}d(\\sqrt{x^2+a})\\&amp;注解：(\\sqrt{x^2\\pm a})’&#x3D;\\frac{x}{\\sqrt{x^2+a}},(\\sqrt{a^2-x^2})’&#x3D;\\frac{-x}{\\sqrt{a^2-x^2}}\\\\end{align}$$ (2)第二换元法$$\\begin{align}&amp;设F’(u)&#x3D;f(\\Phi(u))\\Phi’(u),则\\&amp;\\int{f(x)dx}\\overset{x&#x3D;\\Phi(u)}{&#x3D;}\\int{f(\\Phi(u))\\Phi’(u)du}&#x3D;F(u)+C&#x3D;F(\\Phi^{-1}(x))+C\\&amp;注解：找到合适的x&#x3D;\\Phi(u)\\\\end{align}$$ 1)三角换元$$\\begin{align}&amp;x&#x3D;a\\sin u,x&#x3D;a\\tan u,x&#x3D;a \\sec u\\&amp;\\sqrt{a^2-x^2}\\overset{x&#x3D;a\\sin u}{&#x3D;}a\\cos u,u\\in[-\\frac{\\pi}{2},\\frac{\\pi}{2}],x\\in[-a,a]\\&amp;\\sqrt{a^2+x^2}\\overset{x&#x3D;a\\tan u}{&#x3D;}a\\sec u,u\\in{(-\\frac{\\pi}{2},\\frac{\\pi}{2})},x\\in{(-\\infty,\\infty)}\\&amp;\\sqrt{x^2-a^2}\\overset{x&#x3D;a\\sec u}{&#x3D;}a\\tan u,u\\in(\\frac{\\pi}{2},\\pi]\\cup(0,\\frac{\\pi}{2}]\\\\end{align}$$2)倒变换$$\\begin{align}&amp;x&#x3D;\\frac{1}{u}常用于含\\frac{1}{x}的函数\\\\end{align}$$3）指数(或对数)变换$$\\begin{align}&amp;a^x&#x3D;u或x&#x3D;\\frac{\\ln u}{\\ln a}常用于含a^x的函数\\\\end{align}$$4）用于有理化的变换$$\\begin{align}&amp;\\frac{1}{\\sqrt{x}+\\sqrt[3]{x}}用x&#x3D;u^6\\&amp;\\sqrt[n]{\\frac{ax+b}{cx+d}}用u&#x3D;\\sqrt[n]{\\frac{ax+b}{cx+d}}或x&#x3D;-\\frac{du^n-b}{cu^n-a}\\\\end{align}$$ (3)分部积分法$$\\begin{align}&amp;\\int{u(x)v’(x)dx}&#x3D;\\int{u(x)d(v(x))}&#x3D;u(x)v(x)-\\int{v(x)u’(x)dx}\\&amp;注解：找到合适的u(x),v(x)\\\\end{align}$$ 1)降幂法$$\\begin{align}&amp;\\int{x^ne^{ax}dx},\\int{x^n\\sin axdx},\\int{x^n\\cos ax dx}\\&amp;取u(x)&#x3D;x^n\\\\end{align}$$2)升幂法$$\\begin{align}&amp;\\int{x^a\\ln xdx},\\int{x^a\\arcsin xdx},\\int{x^a\\arccos x dx},\\int{x^a\\arctan x dx}\\&amp;取u(x)&#x3D;\\ln x\\\\end{align}$$3)循环法$$\\begin{align}&amp;\\int{e^{ax}\\sin ax dx},\\int{e^{ax}\\cos {ax} dx}\\&amp;取u(x)&#x3D;e^{ax}或\\sin{ax}\\end{align}$$4)递推公式法$$\\begin{align}&amp;与n有关的结果I_n，建立递推关系I_n&#x3D;f(I_{n-1})或f(I_{n-2})\\\\end{align}$$ 定积分一、定积分概念1.定义$$\\begin{align}&amp;定义:设函数f(x)在区间[a,b]上有定义且有界\\&amp;(1)分割：将[a,b]分成n个[x_{i-1},x_{i}]小区间\\&amp;(2)求和：[x_{i-1},x_{i}]上取一点\\xi_{i},\\sum_{i&#x3D;1}^{n}{f(\\xi_{i})\\Delta x_i},\\lambda&#x3D;\\max{\\Delta x_{1},\\Delta x_{2},…,\\Delta x_{n}}\\&amp;(3)取极限：若\\lim_{\\lambda \\rightarrow 0}{\\sum_{i&#x3D;1}^{n}f(\\xi_{i})\\Delta x}\\exist,且极值不依赖区间[a,b]分发以及点\\xi_{i}的取法,则称f(x)在区间[a,b]上可积,\\&amp;\\int^{b}{a}{f(x)dx}&#x3D;\\lim{\\lambda \\rightarrow 0}{f(\\xi)\\Delta x_{i}}&amp;\\&amp;注解：\\&amp;(1)\\lambda \\rightarrow0 \\rightarrow \\nleftarrow n\\rightarrow \\infty\\&amp;(2)定积分表示一个值,与积分区间[a,b]有关,与积分变化量x无关\\&amp;\\int_{a}^{b}{f(x)dx}&#x3D;\\int_{a}^{b}{f(t)dt}\\&amp;(3)如果积分\\int_{0}^{1}{f(x)dx}\\exist,将[0,1]n等分，此时\\Delta{x_{i}}&#x3D;\\frac{1}{n},取\\xi_{i}&#x3D;\\frac{i}{n},\\&amp;\\int_{0}^{1}f(x)dx&#x3D;\\lim_{\\lambda \\rightarrow 0}{\\sum_{i&#x3D;1}{n}{f(\\xi_{i})\\Delta x_{i}}}&#x3D;\\lim_{n\\rightarrow \\infty}\\sum_{i&#x3D;1}^{n}f(\\frac{i}{n})\\\\end{align}$$ $$\\begin{align}&amp;\\int^{b}{a}{f(x)dx}&#x3D;\\lim{\\lambda \\rightarrow 0}\\sum^{n}{i&#x3D;1}f(\\xi_i)\\Delta_i&#x3D;\\begin{cases}&amp;\\lim{n\\rightarrow \\infty}{\\sum_{i&#x3D;1}^{n}{f(a+(i-1)\\frac{b-a}{n})\\frac{b-a}{n}}},左侧\\&amp;\\lim_{n\\rightarrow \\infty}{\\sum_{i&#x3D;1}^{n}{f(a+i\\frac{b-a}{n})\\frac{b-a}{n}}},右侧\\\\end{cases}\\&amp;中点：\\Phi_i&#x3D;a+(i-1)\\frac{b-a}{n}+\\frac{b-a}{2n}\\\\end{align}$$ 定理：(线性)$$\\begin{align}&amp;\\int[\\alpha f(x)+\\beta g(x)]dx&#x3D;\\alpha\\int f(x)dx+\\beta\\int g(x)dx\\\\end{align}$$注解：积分无小事$$\\begin{align}&amp;\\int{e^{\\pm x^2}dx,\\int{\\frac{\\sin x}{x}}}积不出来\\&amp;F’(x)&#x3D;f(x),x\\in I,连续函数一定存在原函数，无穷多个\\&amp;[F(x)+C]’&#x3D;f(x)\\end{align}$$ 2.定积分存在的充分条件$$\\begin{align}&amp;若f(x)在[a,b]上连续,则\\int^{b}{a}{f(x)dx}必定存在\\&amp;若f(x)在[a,b]上有上界,且只有有限个间断点,则\\int^{b}{a}{f(x)dx}必定存在\\&amp;若f(x)在[a,b]上只有有限个第一类间断点,则\\int^{b}_{a}{f(x)dx}必定存在\\\\end{align}$$ 3.定积分的几何意义$$\\begin{align}&amp;(1)f(x)\\geqslant{0},\\int_{a}^{b}{f(x)dx}&#x3D;S\\\\end{align}$$$$\\begin{align}&amp;(2)f(x)\\leqslant{0},\\int_{a}^{b}{f(x)dx}&#x3D;-S\\\\end{align}$$ $$\\begin{align}&amp;(3)f(x)\\geqslant{0}\\cup f(x)\\leqslant{0},\\int_{a}^{b}{f(x)dx}&#x3D;S_1+S_3-S_2\\ \\end{align}$$ 注解：$$\\begin{align}&amp;（1）当f(x)\\geq0时,定积分的几何意义是,以区间[a,b]为底,y&#x3D;f(x)为曲边的曲边梯形面积\\&amp;（2）定积分是一个常数，只与f和区间[a,b]有关,与积分变量用什么字母无关\\&amp;\\int_a^b{f(x)}dx&#x3D;\\int_a^b{f(t)dt}\\&amp;（3）\\int_a^bdx&#x3D;b-a\\&amp;（4）\\int_{a}^{a}f(x)&#x3D;0,\\int_a^bf(x)dx&#x3D;-\\int_b^a{f(t)}dt\\end{align}$$ 二、定积分的性质1.不等式性质$$\\begin{align}&amp;(1)保序性：若在区间[a,b]上f(x)\\leqslant{g(x)},则\\int_a^{b}{f(x)dx}\\leqslant{\\int_a^{b}{g(x)dx}}\\&amp;推论：\\&amp;(1)f(x)\\geq0,\\forall x\\in[a,b],则\\int_a^b{f(x)dx}\\geq0\\&amp;(2)f(x)\\geq0,\\forall x\\in[a,b],且[c,d]\\subset[a,b],则\\int_a^b{f(x)dx}\\geq\\int_c^d{f(x)dx}\\&amp;(3)|\\int_a^bf(x)dx|\\leq\\int_a^b{|f(x)|dx}\\&amp;-|f|\\leq f\\leq |f|\\Rightarrow \\int_a^b-|f|\\leq \\int_a^bf\\leq \\int_a^b|f|\\Rightarrow |\\int_a^bf|\\leq\\int_a^b|f|\\&amp;如：x^2\\leq x^3,x\\in[0,1],则\\int_0^1{x^3dx}\\leq\\int_0^1{x^2dx}\\\\end{align}$$ $$\\begin{align}&amp;(4)(估值不等式)若M及m分别是f(x)在[a,b]上的最大值和最小值,\\&amp;则m(b-a)\\leqslant{\\int_a^{b}{f(x)dx}\\leqslant{M(b-a)}}\\\\end{align}$$ $$\\begin{align}&amp;证明：M(b-a)&#x3D;S_{AFDC}&#x3D;S_1+S_2+S_3\\&amp;m(b-a)&#x3D;S_{EBDC}&#x3D;S_3\\&amp;\\int_a^{b}{f(x)dx}&#x3D;S_{ADBC}&#x3D;S_2+S_3\\&amp;S_3\\leqslant{S_2+S_3\\leqslant{S_1+S_2+S_3}}\\&amp;\\Leftrightarrow{m(b-a)\\leqslant{\\int_a^{b}{f(x)dx}\\leqslant{M(b-a)}}}\\\\end{align}$$ $$\\begin{align}&amp;(3)|\\int_a^{b}{f(x)dx}|\\leqslant{\\int_a^{b}{|f(x)|dx}}\\\\end{align}$$ 2.中值定理$$\\begin{align}&amp;(1)若f(x)在[a,b]上连续,则\\int_a^{b}{f(x)dx}&#x3D;f(\\xi)(b-a),(a&lt;\\xi&lt;b)\\&amp;称\\frac{1}{b-a}{\\int_{a}^{b}{f(x)dx}为函数y&#x3D;f(x)在区间[a,b]上的平均值}\\&amp;注解：F’(x)&#x3D;f(x),F(b)-F(a)&#x3D;\\int_a^b{f(x)dx},f(\\xi)(b-a)&#x3D;F’(\\xi)(b-a)\\&amp;(2)若f(x),g(x)在[a,b]上连续，g(x)不变号,则\\int_{a}^{b}{f(x)g(x)dx}&#x3D;f(\\xi)\\int_a^b{g(x)dx}\\\\end{align}$$ 注解：$$\\begin{align}&amp;\\int_0^1{\\frac{x}{\\sin x}}dx\\&amp;f(x)&#x3D;\\begin{cases}&amp;\\frac{x}{\\sin x},x\\in[0,1]\\&amp;1,x&#x3D;0\\\\end{cases}\\&amp;结论：有限处点的函数不影响定积分\\&amp;f(x)&#x3D;{\\begin{cases}&amp;x+1,[1,2]\\&amp;x,[0,1]\\\\end{cases}}\\&amp;\\int_0^2{f(x)dx}&#x3D;\\int_0^1{xdx}+\\int_1^2{(x+1)dx}\\\\end{align}$$ $$\\begin{align}&amp;证明：\\frac{1}{2}\\leq\\int_0^{\\frac{1}{2}}\\frac{1}{\\sqrt{1-x^n}}dx\\leq\\frac{\\pi}{6}\\&amp;估值积分：x\\in[0,\\frac{1}{2}]\\&amp;\\\\end{align}$$ 例题：$$\\begin{align}&amp;1.求极限\\lim_{n\\rightarrow \\infty}\\int_0^1{\\frac{x^ne^x}{1+e^x}dx}\\&amp;根据积分容易知道0\\leq\\frac{x^ne^x}{1+e^x}\\leq x^n,x\\in[0,1],n\\in N^*\\&amp;用积分的保号性\\&amp;0\\leq\\int_0^1{\\frac{x^ne^x}{1+e^x}dx}\\leq \\int_0^1{x^n}dx&#x3D;\\frac{1}{n+1}\\&amp;用夹逼定理\\&amp;\\lim_{n\\rightarrow\\infty}\\frac{1}{n+1}&#x3D;0\\&amp;\\lim_{n\\rightarrow \\infty}\\int_0^1{\\frac{x^ne^x}{1+e^x}dx}&#x3D;0\\\\end{align}$$ $$\\begin{align}&amp;2.设I_1&#x3D;\\int_0^{\\frac{4}{\\pi}}\\frac{\\tan x}{x}dx,I_2&#x3D;\\int_0^{\\frac{4}{\\pi}}\\frac{x}{\\tan x}dx则\\&amp;(A)I_1&gt;I_2&gt;1(B)1&gt;I_1&gt;I_2(C)I_2&gt;I_1&gt;1(D)1&gt;I_2&gt;I_1\\&amp;解：用保序性a&lt;b,f(x)\\leq g(x),\\int_a^b f(x)\\leq \\int_a^b g(x)\\&amp;\\tan x&gt;x,x\\in[0,\\frac{\\pi}{2}]\\&amp;\\frac{\\tan x}{x}&gt;1&gt;\\frac{x}{\\tan x},x\\in[0,\\frac{\\pi}{4}]\\&amp;根据保序性\\&amp;\\int_0^{\\frac{\\pi}{4}}\\frac{\\tan x}{x}dx&gt;\\int_0^{\\frac{\\pi}{4}}1dx&#x3D;\\frac{\\pi}{4}&gt;\\int_0^{\\frac{\\pi}{4}}\\frac{x}{\\tan x},x\\in[0,\\frac{\\pi}{4}]\\&amp;证：\\int_0^{\\frac{\\pi}{4}}\\frac{\\tan x}{x}与1的关系\\&amp;积分中值定理\\&amp;\\int_0^{\\frac{\\pi}{4}}\\frac{\\tan x}{x}&#x3D;f(\\xi)(\\frac{\\pi}{4}-0)&#x3D;\\frac{\\tan \\xi}{\\xi}*\\frac{\\pi}{4},\\xi\\in{[0,\\frac{\\pi}{4}]}\\&amp;根据\\frac{\\tan x}{x}在x\\in[0,\\frac{\\pi}{4}]上单调递增\\&amp;0&lt;f(\\xi)&lt;\\frac{4}{\\pi},0&lt;\\int_0^{\\frac{\\pi}{4}}\\frac{\\tan x}{x}&lt;1\\&amp;选(B)\\\\end{align}$$ 三、积分上限函数$$\\begin{align}&amp;如果f(x)在区间[a,b]上连续,则\\Phi(x)&#x3D;\\int_a^b{f(t)dt}在[a,b]上可导,且\\int_a^b{f(t)dt})\\&amp;(\\int_a^xf(t)dt)’&#x3D;f(x),(\\int_a^{x^2}f(t)dt)’&#x3D;f(x^2)*2x\\&amp;如果f(x)在区间[a,b]上连续,\\phi_1(x),\\phi_2(x)为可导函数,则\\Phi(x)&#x3D;\\int_a^b{f(t)dt}在[a,b]上可导,且(\\int_{\\phi_1(x)}^{\\phi_2(x)}{f(t)dt})’\\&amp;&#x3D;f[\\phi_2(x)]*\\phi_2’(x)-f[\\phi_1(x)]*\\phi_1’(x)&#x3D;(\\int_{\\phi_1(x)}^0{f(t)dt}+\\int_{\\phi_2(x)}^0{f(t)dt})’\\&amp;设函数f(x)在[-l,l]上连续,则\\&amp;如果f(x)为奇函数,那么\\int_0^xf(t)dt必为偶函数\\&amp;如果f(x)为偶函数,那么\\int_0^xf(t)dt必为奇函数\\\\end{align}$$ $$\\begin{align}&amp;任取x\\in[a,b),取\\Delta x&gt;0,使x+\\Delta x\\in[a,b)\\&amp;\\frac{\\Delta F}{\\Delta x}&#x3D;\\frac{F(x+\\Delta x)-F(x)}{\\Delta x}&#x3D;\\frac{1}{\\Delta x}[\\int_a^{x+\\Delta x}f(t)dt-\\int_a^xf(t)dt]&#x3D;\\frac{1}{\\Delta x}\\int_x^{x+\\Delta x}f(t)dt&#x3D;f(x+\\sigma\\Delta x)\\rightarrow f(x)(\\Delta x\\rightarrow 0^+)\\\\end{align}$$推论：$$\\begin{align}&amp;若f(x)、\\phi’(x)、\\psi(x)于[a,b]上连续,则\\&amp;(1)(\\int_a^{\\phi(x)}f(t)dt)’&#x3D;f(\\phi(x))\\phi’(x)\\&amp;(2)(\\int_b^{\\psi(x)}f(t)dt)’&#x3D;-f(\\psi(x))\\psi’(x)\\&amp;(3)(\\int_{\\psi(x)}^{\\phi(x)}f(t)dt)’&#x3D;f(\\phi(x))\\phi’(x)-f(\\psi(x))\\psi’(x)\\\\end{align}$$例题$$\\begin{align}&amp;1.设函数f(x)在R上连续,且是奇函数,则其原函数均是偶函数.当f(x)是偶函数时？是周期函数？\\&amp;证：\\&amp;令F_0(x)\\int_0^xf(t)dt,x\\in R\\&amp;F_0(-x)&#x3D;\\int_0^{-x}f(t)dt\\overset{t&#x3D;-u}{&#x3D;}\\int_0^xf(-u)d(u)&#x3D;\\int_0^xf(u)du&#x3D;F_0(x)\\Rightarrow F_0(x)为偶函数\\\\end{align}$$ $$\\begin{align}&amp;求变现积分导数\\&amp;(1)F(x)&#x3D;\\int_x^{e^{-x}}f(t)dt\\&amp;(2)F(x)&#x3D;\\int_0^{x^2}(x^2-t)f(t)dt\\&amp;(3)F(x)&#x3D;\\int_0^{x}f(x^2-t)dt\\&amp;(4)设函数y&#x3D;y(x)由参数方程\\begin{cases}&amp;x&#x3D;1+2t^2\\&amp;y&#x3D;\\int_1^{1+2\\ln t}\\frac{e^u}{u}du\\\\end{cases}(t&gt;1),求\\frac{d^2y}{dx^2}|_{x&#x3D;9}\\&amp;解:\\&amp;(1)F(x)’&#x3D;(\\int_x^{e^{-x}}f(t)dt)’&#x3D;f(e^{-x})(-e^{-x})-f(x)\\&amp;(2)F(x)’&#x3D;(\\int_0^{x^2}(x^2-t)f(t)dt)’&#x3D;(\\int_0^{x^2}x^2f(t)dt-\\int_0^{x^2}tf(t)dt)’\\&amp;&#x3D;2x\\int_0^{x^2}f(t)dt+x^2f(x^2)2x-x^2f(x^2)2x&#x3D;2x\\int_0^{x^2}f(t)dt\\&amp;(3)F(x)&#x3D;\\int_0^{x}f(x^2-t)dt&#x3D;-\\frac{1}{2}\\int_0^xf(x^2-t^2)d(x^2-t^2)\\overset{u&#x3D;x^2-t^2}{&#x3D;}-\\frac{1}{2}\\int_0^xf(u)du\\&amp;F(x)’&#x3D;\\frac{1}{2}f(x^2)2x&#x3D;xf(x^2)\\&amp;(4)\\frac{dy}{dx}&#x3D;\\frac{\\frac{e^{1+2\\ln t}}{1+2\\ln t}\\frac{2}{t}}{4t^2}&#x3D;\\frac{e}{2(1+2\\ln t)}\\&amp;\\frac{d^2y}{dx^2}&#x3D;\\frac{d(\\frac{dy}{dx})}{dx}&#x3D;\\frac{e}{2}(-\\frac{\\frac{2}{t}}{(1+2\\ln t)^2})\\frac{1}{4t}\\\\end{align}$$ $$\\begin{align}&amp;2.求变现积分的积分:\\&amp;(1)设f(x)&#x3D;\\int_0^x{\\frac{\\sin t}{\\pi -t}dt},求\\int_0^\\pi{f(x)}dx\\&amp;解:\\&amp;\\int_0^\\pi{f(x)}dx&#x3D;\\int_0^{\\pi}\\int_0^x\\frac{\\sin t}{\\pi -t}dt\\space dx\\&amp;&#x3D;x\\int_0^x\\frac{\\sin t}{\\pi t}|0^{\\pi}-\\int_0^{\\pi}x\\frac{\\sin x}{\\pi -x}dx\\&amp;&#x3D;\\pi\\int_0^{\\pi}\\frac{\\sin x}{\\pi t}+\\int_0^{\\pi}\\frac{[(\\pi-x)-\\pi]\\sin x}{\\pi-x}dx&#x3D;\\int_0^{\\pi}\\sin xdx&#x3D;2\\&amp;(2)\\lim{x\\rightarrow\\infty}{\\frac{(\\int_0^x{e^{t^2}}dt)^2}{\\int_0^xe^{2t^2}dt}}&#x3D;\\lim_{x\\rightarrow\\infty}{\\frac{(2\\int_0^{x}e^{t^2}dt)e^{x^2}}{e^{2x^2}}}&#x3D;\\lim_{x\\rightarrow\\infty}\\frac{2\\int_0^{x}e^{t^2}}{e^{x^2}}&#x3D;\\lim_{x\\rightarrow\\infty}\\frac{1}{2x}&#x3D;0\\\\end{align}$$ { } $$\\begin{align}&amp;(3)设f(x)连续,\\phi(x)&#x3D;\\int_0^1{f(tx)dt},且\\lim_{x\\rightarrow0}\\frac{f(x)}{x}&#x3D;A(常数),求\\phi’(x)并讨论\\phi’(x)在x&#x3D;0处的连续性\\&amp;当x\\neq0时\\&amp;令u&#x3D;tx,t\\in[0,1],u&#x3D;tx\\in[0,x],\\phi(x)&#x3D;\\int_0^1f(tx)dt\\overset{tx&#x3D;u}{&#x3D;}\\int_0^x{f(u)d(\\frac{u}{x})}&#x3D;\\frac{\\int_0^xf(u)du}{x}\\&amp;\\phi’(x)&#x3D;\\frac{xf(x)-\\int_0^xf(u)du}{x^2}\\&amp;当x&#x3D;0时,f(0)&#x3D;0,\\phi(0)&#x3D;f(0)&#x3D;0,\\phi’(0)&#x3D;\\lim_{x\\rightarrow0}\\frac{\\phi(x)\\phi(0)}{x-0}&#x3D;\\lim_{x\\rightarrow0}\\frac{\\int_0^xf(u)du}{x^2}&#x3D;\\lim_{x\\rightarrow 0}\\frac{f(x)}{2x}&#x3D;\\frac{1}{2}A\\&amp;\\lim_{x\\rightarrow0}\\phi’(x)&#x3D;\\lim_{x\\rightarrow 0}{\\frac{xf(x)-\\int_0^xf(u)du}{x^2}}&#x3D;A-\\frac{1}{2}A&#x3D;\\frac{1}{2}A&#x3D;\\phi’(0)\\Leftrightarrow\\phi’(x)在x&#x3D;0处连续\\\\end{align}$$ 注解：$$\\begin{align}&amp;注意变限积分进行正逆运算时上下限的映射\\&amp;例如F(x)&#x3D;\\int_0^x{f(t)dt}\\overset{t&#x3D;-u}{&#x3D;}\\int_{-a}^{x}f(-u)d(-u)\\\\end{align}$$ 四、定积分的计算1.牛顿莱布尼茨公式$$\\int_a^bf(x)dx&#x3D;F(x)|_a^b&#x3D;F(b)-F(a)$$ 2.换元积分法$$\\int_a^bf(x)dx&#x3D;\\int_\\alpha^\\beta{f(\\Phi(t))\\Phi’(t)dt}$$ 3.分部积分法$$\\int_a^budv&#x3D;uv|_a^b-\\int_a^bvdu$$ 4.奇偶性和周期性$$\\begin{align}&amp;直接使用奇偶性周期性定义证明\\&amp;(1)设f(x)为[-a,a]上的连续函数(a&gt;0),则\\&amp;\\int_{-a}{a}f(x)dx&#x3D;\\begin{cases}0,&amp;f(x)奇函数\\2\\int_0^af(x)dx,&amp;f(x)偶函数\\end{cases}\\&amp;证：\\int_{-a}^0{f(x)dx}\\overset{x&#x3D;-t}{&#x3D;}\\int_0^a{f(-t)d(-t)}&#x3D;-\\int_{0}^{a}f(t)d(t)&#x3D;-\\int_0^a{f(x)dx}\\\\end{align}$$ $$\\begin{align}&amp;(2)设f(x)是以T为周期的连续函数,则对\\forall A，有\\int_a^{a+T}f(x)&#x3D;\\int_0^T{f(x)dx}\\&amp;\\int_a^{a+T}f(x)dx\\overset{x&#x3D;a+t}{&#x3D;}\\int_0^T{f(a+t)d(a+t)}&#x3D;\\int_0^{a+t}f(a+t)dt\\\\end{align}$$ $$\\begin{align}&amp;\\Phi:x\\in[a,b]\\rightarrow y\\in[c,d],令\\frac{x-a}{b-a}&#x3D;\\frac{y-c}{d-c},y&#x3D;c+\\frac{d-c}{b-a}(x-a)\\\\end{align}\\$$ 5.奇偶函数积分后的奇偶性(奇偶函数求导后的奇偶性)1.奇偶函数求导后的奇偶性$$\\begin{align}&amp;(1)f(x)为奇函数:\\&amp;f(-x)&#x3D;-f(x)\\&amp;\\Leftrightarrow f’(-x)(-1)&#x3D;-f’(x)\\&amp;\\Leftrightarrow f’(-x)&#x3D;f’(x)\\&amp;\\Leftrightarrow f’(x)为偶函数\\&amp;(2)f(x)为偶函数:\\&amp;f(-x)&#x3D;f(x)\\&amp;\\Leftrightarrow f’(-x)&#x3D;f’(x)\\&amp;\\Leftrightarrow f’(-x)(-1)&#x3D;f’(x)\\&amp;\\Leftrightarrow f’(-x)&#x3D;-f’(x)\\&amp;\\Leftrightarrow f’(x)为奇函数\\\\end{align}$$ 2.奇偶函数求积分后的奇偶性$$\\begin{align}&amp;设F(x)为f(x)的原函数\\&amp;(1)f(x)为奇函数:\\&amp;f(-x)&#x3D;-f(x)\\&amp;\\Leftrightarrow \\int f(-x)dx&#x3D;-\\int f(x)dx\\&amp;\\Leftrightarrow -\\int f(-x)d(-x)&#x3D;-\\int f(x)dx\\&amp;\\Leftrightarrow F(-x)&#x3D;F(x)\\&amp;\\Leftrightarrow F(x)为偶函数\\&amp;(2)f(x)为偶函数:\\&amp;f(-x)&#x3D;f(x)\\&amp;\\Leftrightarrow \\int f(-x)dx&#x3D;\\int f(x)dx\\&amp;\\Leftrightarrow -\\int f(-x)d(-x)&#x3D;\\int f(x)dx\\&amp;\\Leftrightarrow F(-x)&#x3D;-F(x)\\&amp;\\Leftrightarrow F(x)为奇函数\\\\end{align}$$ 3.奇偶函数复合后的奇偶性$$\\begin{align}&amp;\\exist f(x),g(x),F(x)&#x3D;f(g(x))\\&amp;设f(x)为奇函数\\&amp;(1)g(x)为偶函数\\&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(g(x))&#x3D;F(x),F(x)为偶函数\\&amp;(2)g(x)为奇函数\\&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(-g(x))&#x3D;-f(g(x))&#x3D;-F(x),F(x)为奇函数\\&amp;设f(x)为偶函数\\&amp;(1)g(x)为奇函数\\&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(g(x))&#x3D;F(x),F(x)为偶函数\\&amp;(2)g(x)为偶函数\\&amp;F(-x)&#x3D;f(g(-x))&#x3D;f(g(x))&#x3D;F(x),F(x)为偶函数\\&amp;注解:外偶全偶,外奇奇偶\\\\end{align}$$ 例题：$$\\begin{align}&amp;1.设M&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}{\\frac{\\sin x}{1+x^2}\\cos^4xdx},N&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}{(\\sin x^3+\\cos^4x)dx},P&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}(x^2\\sin^3x-\\cos^4x)dx,则\\&amp;(A)N&lt;P&lt;M(B)M&lt;P&lt;N(C)N&lt;M&lt;P(D)P&lt;M&lt;N\\&amp;根据对称性判断\\&amp;M:f_M(x)为奇函数，F_M(x)为偶函数\\&amp;N:N&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}{(\\sin x^3+\\cos^4x)dx}&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}\\sin ^3xdx+\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}\\cos ^4xdx\\&amp;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}\\sin ^3xdx&#x3D;0,\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}\\cos ^4xdx\\geq 0,\\Rightarrow N\\geq 0\\&amp;P:P&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}(x^2\\sin^3x-\\cos^4x)dx&#x3D;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}x^2\\sin^3xdx-\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}\\cos^4xdx\\&amp;\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}x^2\\sin^3xdx&#x3D;0,\\int_{-\\frac{\\pi}{2}}^{\\frac{\\pi}{2}}\\cos^4xdx\\geq0,\\Rightarrow P\\leq0\\&amp;\\Leftrightarrow P&lt;M&lt;N,\\space\\space选(D)\\\\end{align}$$ $$\\begin{align}&amp;2.设f(x)&#x3D;\\begin{cases}&amp;kx,0\\leq x\\leq \\frac{1}{2}a\\&amp;c,\\frac{1}{2}a&lt;x\\leq a\\\\end{cases},求F(x)&#x3D;\\int_0^xf(t)dt,x\\in[0,a]\\&amp;F(x)&#x3D;\\begin{cases}&amp;\\int_0^xktdt&#x3D;\\frac{1}{2}kt^2|0^x&#x3D;\\frac{1}{2}kx^2,0\\leq x\\leq \\frac{1}{2}a\\&amp;\\int_0^{\\frac{1}{2}a}ktdt+\\int{\\frac{1}{2}a}^c cdt&#x3D;\\frac{1}{8}ka^2+c^2-\\frac{1}{2}ac,\\frac{1}{2}a&lt;x\\leq a\\\\end{cases}\\\\end{align}$$ $$\\begin{align}&amp;3.证明：\\int_0^{2\\pi}f(|\\cos x|)dx&#x3D;4\\int_0^{\\frac{\\pi}{2}}f(|\\cos x|)dx\\\\end{align}$$ 6.已有公式$$\\begin{align}&amp;(1)\\int_0^{\\frac{\\pi}{2}}{\\sin^nxdx&#x3D;\\int_0^{\\frac{\\pi}{2}}\\cos^n xdx&#x3D;\\begin{cases}\\frac{n-1}{n}\\frac{n-3}{n-2}…*\\frac{1}{2}*\\frac{\\pi}{2},&amp;n为偶数\\\\frac{n-1}{n}\\frac{n-3}{n-2}…*\\frac{2}{3},&amp;n为大于1的奇数\\\\end{cases}}\\&amp;(2)\\int_0^{\\pi}xf(\\sin x)dx&#x3D;\\frac{\\pi}{2}\\int_0^{\\pi}f(\\sin x)dx(f(x)为连续函数)\\\\end{align}$$ 7.与定积分有关的证明8.经典例题：例题1:$$\\begin{align}&amp;\\lim_{n\\rightarrow \\infty}{(\\frac{1}{n+1}+\\frac{1}{n+2}+…+\\frac{1}{n+n})}\\&amp;法1：夹逼定理+基本不等式\\&amp;\\frac{1}{1+x}&lt;\\ln(x+1)&lt;x\\&amp;令x&#x3D;\\frac{1}{n}\\&amp;得\\frac{1}{n+1}&#x3D;\\frac{\\frac{1}{n}}{\\frac{1}{n}+1}&lt;\\ln(\\frac{1}{n}+1)&#x3D;\\ln(n+1)-\\ln(n)&lt;\\frac{1}{n}\\&amp;得\\frac{1}{n+2}&lt;ln(n+2)-ln(n+1)&lt;\\frac{1}{n+1}\\&amp;得\\frac{1}{n+n}&lt;\\ln(n+n)-\\ln(n+n-1)&lt;\\frac{1}{n+n-1}\\&amp;得\\frac{1}{n+1}+\\frac{1}{n+2}+…+\\frac{1}{n+n}&lt;ln(2n)-ln(n)&#x3D;ln2\\&amp;法2：\\lim_{n\\rightarrow \\infty}{(\\frac{1}{n+1}+\\frac{1}{n+2}+…+\\frac{1}{n+n})}中\\&amp;\\frac{1}{n+1}中n为主体，1为变体\\&amp;\\frac{变体}{主体}\\rightarrow^{n \\rightarrow{\\infty}}\\begin{cases}0,次(夹逼定理)\\A\\neq 0,同(定积分)\\end{cases}\\&amp;\\lim_{\\lambda \\rightarrow 0}{\\sum_{i&#x3D;1}^{n}{f(\\xi_i)\\Delta x_i}&#x3D;\\lim_{n\\rightarrow \\infty}\\frac{1}{n}\\sum_{i&#x3D;1}^{n}f(\\xi_i)(b-a)}&#x3D;\\int_0^1\\frac{1}{1+x}&#x3D;\\ln(1+x)|_{0}^{1}&#x3D;\\ln2\\\\end{align}$$ 例题2$$\\begin{align}&amp;设f(x)&#x3D;\\int_0^{\\pi}{\\frac{\\sin x}{\\pi-t}dt},计算\\int_0^{\\pi}f(x)dx.\\&amp;法1：分部积分+换元法\\&amp;原式&#x3D;xf(x)|_0^{\\pi}-\\int_0^{\\pi}{\\frac{x\\sin x}{\\pi-x}dx}\\&amp;&#x3D;\\pi{\\int_0^{\\pi}{\\frac{\\sin{t}}{\\pi-t}dt}-\\int_0^{\\pi}{\\frac{x\\sin x}{\\pi-x}}dx}\\&amp;&#x3D;\\int_0^{\\pi}{\\frac{(\\pi-x)\\sin x}{\\pi-x}dx}&#x3D;2\\&amp;法2：\\&amp;原式&#x3D;\\int_0^\\pi{f(x)d(x-{\\pi})}&#x3D;(x-\\pi)f(x)|_0^{\\pi}-\\int_0^{\\pi}{\\frac{(x-\\pi)\\sin x}{\\pi-x}dx}&#x3D;2\\&amp;法3：二重积分转化为累次积分\\&amp;原式&#x3D;\\int_0^{\\pi}{\\int_0^{\\pi}\\frac{x\\sin t}{\\pi-t}dt}dx\\\\end{align}$$ 例题3 $$\\begin{align}&amp;法1：构造辅助函数\\&amp;根据题意f(1)&#x3D;f(-1)&#x3D;1,f(0)&#x3D;-1\\Rightarrow f(x)为偶函数,f最低点函数值为-1\\&amp;可以构造符合题意的辅助函数f(x)&#x3D;2x^2-1\\&amp;法2：根据函数的性质直接判断\\end{align}$$ 例题4 $$\\begin{align}&amp;因为\\lim_{x\\rightarrow 0}{\\frac{ax-\\sin x}{\\int_b^x{\\frac{\\ln{1+t^3}}{t}dt}}}&#x3D;c(c\\neq 0)\\&amp;所以\\lim_{x\\rightarrow 0}{ax-\\sin x}&#x3D;0并且\\lim_{x \\rightarrow 0}{\\int_b^x{\\frac{\\ln{1+t^3}}{t}dt}}&#x3D;0\\&amp;化简,使用洛必达法则上下求导\\&amp;\\lim_{x\\rightarrow 0}{\\frac{ax-\\sin x}{\\int_b^x{\\frac{\\ln{1+t^3}}{t}dt}}}&#x3D;\\lim_{x\\rightarrow 0}{\\frac{a-\\cos x}{\\frac{\\ln{1+x^3}}{x}}}&#x3D;\\lim_{x\\rightarrow 0}{\\frac{a-\\cos x}{x^2}}\\&amp;\\Rightarrow a&#x3D;1,c&#x3D;\\frac{1}{2},b&#x3D;0\\\\end{align}$$ 反常积分一、无穷区间上的反常积分$$\\begin{align}&amp;(1)\\int_a^{+\\infty}{f(x)}dx&#x3D;\\lim_{t\\rightarrow +\\infty}{\\int_{a}^{t}f(x)dx}\\&amp;(2)\\int_{-\\infty}^{b}{f(x)}dx&#x3D;\\lim_{t\\rightarrow -\\infty}{\\int_{t}^{b}f(x)dx}\\&amp;(3)\\int_{-\\infty}^{0}{f(x)}dx和{\\int_{0}^{+\\infty}f(x)dx}都收敛,则{\\int_{-\\infty}^{+\\infty}f(x)dx}收敛\\&amp;且{\\int_{-\\infty}^{+\\infty}f(x)dx}&#x3D;\\int_{-\\infty}^{0}{f(x)}dx+{\\int_{0}^{+\\infty}f(x)dx}\\&amp;如果其中一个发散,结果也发散\\&amp;常用结论：\\int_a^{+\\infty}{\\frac{1}{x^p}dx}\\begin{cases}&amp;p&gt;1,收敛\\&amp;p\\leq1 ,发散\\\\end{cases},(a&gt;0)\\\\end{align}$$ 二、无界函数的反常积分$$\\begin{align}&amp;如果函数f(x)在点a的任一领域内都无界,那么点a为函数f(x)的瑕点(也称为无界点).无界函数的反常积分也成为瑕积分\\&amp;(1)设函数f(x)在(a,b]上连续,点a为f(x)的瑕点.如果极限\\lim_{t\\rightarrow a^+}{\\int_{t}^{b}{f(x)dx}}\\exist,\\&amp;则称此极限为函数f(x)在区间[a,b]上的反常区间,记作\\int_{a}^{b}f(x)dx,即\\int_{a}^{b}f(x)dx&#x3D;\\lim_{t\\rightarrow a^+}{\\int_{t}^{b}{f(x)dx}}\\&amp;这时也称反常积分\\int_a^b{f(x)dx}收敛,如果上述极限不存在，则反常积分\\int_a^b{f(x)dx}发散\\&amp;(2)设函数f(x)在[a,b)上连续,点b为函数f(x)的瑕点,则可以类似定义函数f(x)在区间[a,b]上的反常积分\\int_a^bf(x)dx&#x3D;\\lim_{t\\rightarrow b^-}{\\int_a^tf(x)dx}\\&amp;设函数f(x)在[a,b]上除点c(a&lt;c&lt;b)外连续,点c为函数f(x)的瑕点,如果反常积分\\int_a^c{f(x)dx}和\\int_c^b{f(x)dx}都收敛\\&amp;则称反常积分\\int_a^b{f(x)dx}收敛,且\\int_a^b{f(x)dx}&#x3D;\\int_a^c{f(x)dx}+\\int_c^b{f(x)dx}\\&amp;如果至少一个发散,则称\\int_a^b{f(x)dx}发散\\&amp;常用结论：\\&amp;\\int_a^b{\\frac{1}{(x-a)^p}}\\begin{cases}&amp;p&lt;1,收敛\\&amp;p\\geq 1,发散\\\\end{cases}\\&amp;\\int_a^b{\\frac{1}{(x-a)^p}}\\begin{cases}&amp;p&lt;1,收敛\\&amp;p\\geq 1,发散\\\\end{cases}\\\\end{align}$$ 三、例题例题1$$\\begin{align}&amp;\\int\\frac{1}{\\ln^{\\alpha}x}d(\\ln x)\\rightarrow^{\\ln x&#x3D;u}\\int{\\frac{du}{u^{\\alpha+1}}}\\begin{cases}&amp;{\\alpha-1&lt; 1}\\&amp;{\\alpha+1&gt;1}\\\\end{cases}\\Rightarrow 0&lt;\\alpha&lt;2\\\\end{align}$$ 定积分的应用微元法$$\\begin{align}&amp;\\\\end{align}$$ 一、几何应用1.平面图形的面积$$\\begin{align}&amp;(1)若平面域D由曲线y&#x3D;f(x),y&#x3D;g(x)(f(x)\\geq g(x)),x&#x3D;a,x&#x3D;b(a&lt;b)所围成,则平面域D的面积为\\&amp;S&#x3D;\\int_a^b{[f(x)-g(x)]dx}\\&amp;(2)若平面域D由曲线由\\rho&#x3D;\\rho(\\theta),\\theta&#x3D;\\alpha,\\theta&#x3D;\\beta(\\alpha&lt;\\beta)所围成,则其面积为S&#x3D;\\frac{1}{2}\\int_{\\alpha}^{\\beta}{\\rho^2(\\theta)d\\theta}\\end{align}$$ 2.旋转体的体积$$\\begin{align}&amp;若区域D由曲线y&#x3D;f(x)(f(x)\\geq 0)和直线x&#x3D;a,x&#x3D;b(0\\leq a&lt;b)及x轴所围成,则\\&amp;(1)区域D绕x轴旋转一周所得到的旋转体体积为V_x&#x3D;\\pi\\int_a^b{f^2(x)dx}\\&amp;(2)区域D绕y轴旋转一周所得到的旋转体体积为V_y&#x3D;2\\pi\\int_a^b{xf(x)dx}\\&amp;(3)区域D绕y&#x3D;kx+b轴旋转一周所得到的旋转体体积为V&#x3D;2\\pi\\int_D\\int{r(x,y)d\\sigma}\\&amp;例如：求y&#x3D;x,y&#x3D;x^2在第一象限的封闭图形绕转轴的体积\\\\end{align}$$ $$\\begin{align}&amp;V_x&#x3D;2\\pi\\int_D\\int yd\\sigma&#x3D;2\\pi\\int_0^1{dx}\\int_{x^2}^{x}ydy\\&amp;V_y&#x3D;2\\pi\\int_D\\int xd\\sigma&#x3D;2\\pi\\int_0^1{dx}\\int_{x^2}^{x}xdy\\&amp;V_{x&#x3D;1}&#x3D;2\\pi\\int_D\\int (1-x)d\\sigma\\&amp;V_{y&#x3D;2}&#x3D;2\\pi\\int_D\\int (2-y)d\\sigma\\\\end{align}$$ 3.曲线弧长$$\\begin{align}&amp;(1)C:y&#x3D;y(x),a\\leq x\\leq b,s&#x3D;\\int_a^b{\\sqrt{1+y’^2}dx}\\&amp;(2)C:\\begin{cases}&amp;x&#x3D;x(t)\\&amp;y&#x3D;y(t)\\\\end{cases},\\alpha \\leq t\\leq \\beta,s&#x3D;\\int_{\\alpha}^{\\beta}{\\sqrt{x’^2+y’^2}dx}\\&amp;(3)C:\\rho&#x3D;\\rho(\\theta),\\alpha \\leq \\theta\\leq \\beta,s&#x3D;\\int_{\\alpha}^{\\beta}{\\sqrt{\\rho^2+\\rho’^2}dx}\\\\end{align}$$ 4.旋转体侧面积$$\\begin{align}&amp;曲线y&#x3D;f(x)(f(x)\\geq 0)和直线x&#x3D;a,x&#x3D;b(0\\leq a&lt;b)及x轴所围成的区域绕x轴旋转所得到的旋转体的侧面积为\\&amp;S&#x3D;2\\pi\\int_a^b{f(x)\\sqrt{1+f’^2(x)}dx}\\\\end{align}$$ 二、物理应用1.压力2.变力做功3.引力（较少考）例题1$$\\begin{align}&amp;分析题意可知,该容器由x^2+y^2&#x3D;1的圆和x^2+(y-1)^2&#x3D;1的偏心圆组成\\&amp;根据图像的对称性可以避免不同表达式带来的困难\\&amp;对圆的小带子进行积分，带子长度为x，积分区间为-1到\\frac{1}{2}，\\int_{-1}^{\\frac{1}{2}}{\\pi x^2dy}\\&amp;由于图像的对称性，将积分结果乘二\\&amp;(1)V&#x3D;2\\pi\\int_{-1}^{\\frac{1}{2}}{x^2}dy&#x3D;2\\pi\\int_{-1}^{\\frac{1}{2}}{(1-y^2)dy}&#x3D;\\frac{9\\pi}{4}\\\\end{align}$$ $$\\begin{align}&amp;(2)W&#x3D;FS&#x3D;GS&#x3D;mgS&#x3D;\\rho VSg\\&amp;上部为W_1&#x3D;\\int_{\\frac{1}{2}}^{2}(2y-y^2)(2-y)dy\\rho g\\&amp;下部为W_2&#x3D;\\int^{\\frac{1}{2}}_{-1}(1-y^2)(2-y)dy*\\rho g\\&amp;W&#x3D;W_1+W_2\\\\end{align}$$ 例题2$$\\begin{align}&amp;F_p&#x3D;PA&#x3D;\\rho ghA\\&amp;将图像分为上部和下部，上部为矩形区域和下部的抛物线围成的面积区域，对其进行依次求解\\&amp;P_1&#x3D;2\\rho gh\\int_1^{h+1}{h+1-y}dy&#x3D;\\rho gh^2\\&amp;P_2&#x3D;2\\rho gh\\int_0^1{(h+1-y)\\sqrt{y}dy&#x3D;4\\rho g(\\frac{1}{3}h+\\frac{2}{15})}\\&amp;\\frac{P_1}{P_2}&#x3D;\\frac{4}{5}\\Rightarrow h&#x3D;2,h&#x3D;-\\frac{1}{3}(舍去)\\end{align}$$","categories":[],"tags":[]},{"title":"","slug":"HM_004_定积分与反常积分","date":"2023-08-09T03:00:39.542Z","updated":"2023-08-07T08:49:45.000Z","comments":true,"path":"2023/08/09/HM_004_定积分与反常积分/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_004_%E5%AE%9A%E7%A7%AF%E5%88%86%E4%B8%8E%E5%8F%8D%E5%B8%B8%E7%A7%AF%E5%88%86/","excerpt":"","text":"定积分与反常积分定积分一、定积分概念1.定义$$\\begin{align}&amp;定义:设函数f(x)在区间[a,b]上有定义且有界\\&amp;(1)分割：将[a,b]分成n个[x_{i-1},x_{i}]小区间\\&amp;(2)求和：[x_{i-1},x_{i}]上取一点\\xi_{i},\\sum_{i&#x3D;1}^{n}{f(\\xi_{i})\\Delta x_i},\\lambda&#x3D;\\max{\\Delta x_{1},\\Delta x_{2},…,\\Delta x_{n}}\\&amp;(3)取极限：若\\lim_{\\lambda \\rightarrow 0}{\\sum_{i&#x3D;1}^{n}f(\\xi_{i})\\Delta x}\\exist,且极值不依赖区间[a,b]分发以及点\\xi_{i}的取法,则称f(x)在区间[a,b]上可积,\\&amp;\\int^{b}{a}{f(x)dx}&#x3D;\\lim{\\lambda \\rightarrow 0}{f(\\xi)\\Delta x_{i}}&amp;\\&amp;注解：\\&amp;(1)\\lambda \\rightarrow0 \\rightarrow \\nleftarrow n\\rightarrow \\infty\\&amp;(2)定积分表示一个值,与积分区间[a,b]有关,与积分变化量x无关\\&amp;\\int_{a}^{b}{f(x)dx}&#x3D;\\int_{a}^{b}{f(t)dt}\\&amp;(3)如果积分\\int_{0}^{1}{f(x)dx}\\exist,将[0,1]n等分，此时\\Delta{x_{i}}&#x3D;\\frac{1}{n},取\\xi_{i}&#x3D;\\frac{i}{n},\\&amp;\\int_{0}^{1}f(x)dx&#x3D;\\lim_{\\lambda \\rightarrow 0}{\\sum_{i&#x3D;1}{n}{f(\\xi_{i})\\Delta x_{i}}}&#x3D;\\lim_{n\\rightarrow \\infty}\\sum_{i&#x3D;1}^{n}f(\\frac{i}{n})\\\\end{align}$$ 定理：(线性)$$\\begin{align}&amp;\\int[\\alpha f(x)+\\beta g(x)]dx&#x3D;\\alpha\\int f(x)dx+\\beta\\int g(x)dx\\\\end{align}$$注解：积分无小事$$\\begin{align}&amp;\\int{e^{\\pm x^2}dx,\\int{\\frac{\\sin x}{x}}}积不出来\\\\end{align}$$ 2.定积分存在的充分条件$$\\begin{align}&amp;若f(x)在[a,b]上连续,则\\int^{b}{a}{f(x)dx}必定存在\\&amp;若f(x)在[a,b]上有上界,且只有有限个间断点,则\\int^{b}{a}{f(x)dx}必定存在\\&amp;若f(x)在[a,b]上只有有限个第一类间断点,则\\int^{b}_{a}{f(x)dx}必定存在\\\\end{align}$$ 3.定积分的几何意义$$\\begin{align}&amp;(1)f(x)\\geqslant{0},\\int_{a}^{b}{f(x)dx}&#x3D;S\\\\end{align}$$$$\\begin{align}&amp;(2)f(x)\\leqslant{0},\\int_{a}^{b}{f(x)dx}&#x3D;-S\\\\end{align}$$ $$\\begin{align}&amp;(3)f(x)\\geqslant{0}\\cup f(x)\\leqslant{0},\\int_{a}^{b}{f(x)dx}&#x3D;S_1+S_3-S_2\\end{align}$$ 二、定积分的性质1.不等式性质$$\\begin{align}&amp;(1)保序性：若在区间[a,b]上f(x)\\leqslant{g(x)},则\\int_a^{b}{f(x)dx}\\leqslant{\\int_a^{b}{g(x)dx}}\\\\end{align}$$ $$\\begin{align}&amp;(2)若M及m分别是f(x)在[a,b]上的最大值和最小值,\\&amp;则m(b-a)\\leqslant{\\int_a^{b}{f(x)dx}\\leqslant{M(b-a)}}\\\\end{align}$$ $$\\begin{align}&amp;证明：M(b-a)&#x3D;S_{AFDC}&#x3D;S_1+S_2+S_3\\&amp;m(b-a)&#x3D;S_{EBDC}&#x3D;S_3\\&amp;\\int_a^{b}{f(x)dx}&#x3D;S_{ADBC}&#x3D;S_2+S_3\\&amp;S_3\\leqslant{S_2+S_3\\leqslant{S_1+S_2+S_3}}\\&amp;\\Leftrightarrow{m(b-a)\\leqslant{\\int_a^{b}{f(x)dx}\\leqslant{M(b-a)}}}\\\\end{align}$$ $$\\begin{align}&amp;(3)|\\int_a^{b}{f(x)dx}|\\leqslant{\\int_a^{b}{|f(x)|dx}}\\\\end{align}$$ 2.中值定理$$\\begin{align}&amp;(1)若f(x)在[a,b]上连续,则\\int_a^{b}{f(x)dx}&#x3D;f(\\xi)(b-a),(a&lt;\\xi&lt;b)\\&amp;称\\frac{1}{b-a}{\\int_{a}^{b}{f(x)dx}为函数y&#x3D;f(x)在区间[a,b]上的平均值}\\&amp;注解：F’(x)&#x3D;f(x),F(b)-F(a)&#x3D;\\int_a^b{f(x)dx},f(\\xi)(b-a)&#x3D;F’(\\xi)(b-a)\\&amp;(2)若f(x),g(x)在[a,b]上连续，g(x)不变号,则\\int_{a}^{b}{f(x)g(x)dx}&#x3D;f(\\xi)\\int_a^b{g(x)dx}\\\\end{align}$$ 三、积分上限函数$$\\begin{align}&amp;如果f(x)在区间[a,b]上连续,则\\Phi(x)&#x3D;\\int_a^b{f(t)dt}在[a,b]上可导,且\\int_a^b{f(t)dt})\\&amp;(\\int_a^xf(t)dt)’&#x3D;f(x),(\\int_a^{x^2}f(t)dt)’&#x3D;f(x^2)*2x\\&amp;如果f(x)在区间[a,b]上连续,\\phi_1(x),\\phi_2(x)为可导函数,则\\Phi(x)&#x3D;\\int_a^b{f(t)dt}在[a,b]上可导,且(\\int_{\\phi_1(x)}^{\\phi_2(x)}{f(t)dt})’\\&amp;&#x3D;f[\\phi_2(x)]*\\phi_2’(x)-f[\\phi_1(x)]*\\phi_1’(x)&#x3D;(\\int_{\\phi_1(x)}^0{f(t)dt}+\\int_{\\phi_2(x)}^0{f(t)dt})’\\&amp;设函数f(x)在[-l,l]上连续,则\\&amp;如果f(x)为奇函数,那么\\int_0^xf(t)dt必为偶函数\\&amp;如果f(x)为偶函数,那么\\int_0^xf(t)dt必为奇函数\\\\end{align}$$ 四、定积分的计算1.牛顿莱布尼茨公式$$\\int_a^bf(x)dx&#x3D;F(x)|_a^b&#x3D;F(b)-F(a)$$ 2.换元积分法$$\\int_a^bf(x)dx&#x3D;\\int_\\alpha^\\beta{f(\\Phi(t))\\Phi’(t)dt}$$ 3.分部积分法$$\\int_a^budv&#x3D;uv|_a^b-\\int_a^bvdu$$ 4.奇偶性和周期性$$\\begin{align}&amp;(1)设f(x)为[-a,a]上的连续函数(a&gt;0),则\\&amp;\\int_{-a}{a}f(x)dx&#x3D;\\begin{cases}0,&amp;f(x)奇函数\\2\\int_0^af(x)dx,&amp;f(x)偶函数\\end{cases}\\&amp;(2)设f(x)是以T为周期的连续函数,则对\\forall A，有\\int_a^{a+T}f(x)&#x3D;\\int_0^T{f(x)dx}\\\\end{align}$$ 5.已有公式$$\\begin{align}&amp;(1)\\int_0^{\\frac{\\pi}{2}}{\\sin^nxdx&#x3D;\\int_0^{\\frac{\\pi}{2}}\\cos^n xdx&#x3D;\\begin{cases}\\frac{n-1}{n}\\frac{n-3}{n-2}…*\\frac{1}{2}*\\frac{\\pi}{2},&amp;n为偶数\\\\frac{n-1}{n}\\frac{n-3}{n-2}…*\\frac{2}{3},&amp;n为大于1的奇数\\\\end{cases}}\\&amp;(2)\\int_0^{\\pi}xf(\\sin x)dx&#x3D;\\frac{\\pi}{2}\\int_0^{\\pi}f(\\sin x)dx(f(x)为连续函数)\\\\end{align}$$ 6.经典例题：例题1:$$\\begin{align}&amp;\\lim_{n\\rightarrow \\infty}{(\\frac{1}{n+1}+\\frac{1}{n+2}+…+\\frac{1}{n+n})}\\&amp;法1：夹逼定理+基本不等式\\&amp;\\frac{1}{1+x}&lt;\\ln(x+1)&lt;x\\&amp;令x&#x3D;\\frac{1}{n}\\&amp;得\\frac{1}{n+1}&#x3D;\\frac{\\frac{1}{n}}{\\frac{1}{n}+1}&lt;\\ln(\\frac{1}{n}+1)&#x3D;\\ln(n+1)-\\ln(n)&lt;\\frac{1}{n}\\&amp;得\\frac{1}{n+2}&lt;ln(n+2)-ln(n+1)&lt;\\frac{1}{n+1}\\&amp;得\\frac{1}{n+n}&lt;\\ln(n+n)-\\ln(n+n-1)&lt;\\frac{1}{n+n-1}\\&amp;得\\frac{1}{n+1}+\\frac{1}{n+2}+…+\\frac{1}{n+n}&lt;ln(2n)-ln(n)&#x3D;ln2\\&amp;法2：\\lim_{n\\rightarrow \\infty}{(\\frac{1}{n+1}+\\frac{1}{n+2}+…+\\frac{1}{n+n})}中\\&amp;\\frac{1}{n+1}中n为主体，1为变体\\&amp;\\frac{变体}{主体}\\rightarrow^{n \\rightarrow{\\infty}}\\begin{cases}0,次(夹逼定理)\\A\\neq 0,同(定积分)\\end{cases}\\&amp;\\lim_{\\lambda \\rightarrow 0}{\\sum_{i&#x3D;1}^{n}{f(\\xi_i)\\Delta x_i}&#x3D;\\lim_{n\\rightarrow \\infty}\\frac{1}{n}\\sum_{i&#x3D;1}^{n}f(\\xi_i)(b-a)}&#x3D;\\int_0^1\\frac{1}{1+x}&#x3D;\\ln(1+x)|_{0}^{1}&#x3D;\\ln2\\\\end{align}$$ 例题2$$\\begin{align}&amp;设f(x)&#x3D;\\int_0^{\\pi}{\\frac{\\sin x}{\\pi-t}dt},计算\\int_0^{\\pi}f(x)dx.\\&amp;法1：分部积分+换元法\\&amp;原式&#x3D;xf(x)|_0^{\\pi}-\\int_0^{\\pi}{\\frac{x\\sin x}{\\pi-x}dx}\\&amp;&#x3D;\\pi{\\int_0^{\\pi}{\\frac{\\sin{t}}{\\pi-t}dt}-\\int_0^{\\pi}{\\frac{x\\sin x}{\\pi-x}}dx}\\&amp;&#x3D;\\int_0^{\\pi}{\\frac{(\\pi-x)\\sin x}{\\pi-x}dx}&#x3D;2\\&amp;法2：\\&amp;原式&#x3D;\\int_0^\\pi{f(x)d(x-{\\pi})}&#x3D;(x-\\pi)f(x)|_0^{\\pi}-\\int_0^{\\pi}{\\frac{(x-\\pi)\\sin x}{\\pi-x}dx}&#x3D;2\\&amp;法3：二重积分转化为累次积分\\&amp;原式&#x3D;\\int_0^{\\pi}{\\int_0^{\\pi}\\frac{x\\sin t}{\\pi-t}dt}dx\\\\end{align}$$ 例题3 $$\\begin{align}&amp;法1：构造辅助函数\\&amp;根据题意f(1)&#x3D;f(-1)&#x3D;1,f(0)&#x3D;-1\\Rightarrow f(x)为偶函数,f最低点函数值为-1\\&amp;可以构造符合题意的辅助函数f(x)&#x3D;2x^2-1\\&amp;法2：根据函数的性质直接判断\\end{align}$$ 例题4 $$\\begin{align}&amp;因为\\lim_{x\\rightarrow 0}{\\frac{ax-\\sin x}{\\int_b^x{\\frac{\\ln{1+t^3}}{t}dt}}}&#x3D;c(c\\neq 0)\\&amp;所以\\lim_{x\\rightarrow 0}{ax-\\sin x}&#x3D;0并且\\lim_{x \\rightarrow 0}{\\int_b^x{\\frac{\\ln{1+t^3}}{t}dt}}&#x3D;0\\&amp;化简,使用洛必达法则上下求导\\&amp;\\lim_{x\\rightarrow 0}{\\frac{ax-\\sin x}{\\int_b^x{\\frac{\\ln{1+t^3}}{t}dt}}}&#x3D;\\lim_{x\\rightarrow 0}{\\frac{a-\\cos x}{\\frac{\\ln{1+x^3}}{x}}}&#x3D;\\lim_{x\\rightarrow 0}{\\frac{a-\\cos x}{x^2}}\\&amp;\\Rightarrow a&#x3D;1,c&#x3D;\\frac{1}{2},b&#x3D;0\\\\end{align}$$ 反常积分一、无穷区间上的反常积分$$\\begin{align}&amp;(1)\\int_a^{+\\infty}{f(x)}dx&#x3D;\\lim_{t\\rightarrow +\\infty}{\\int_{a}^{t}f(x)dx}\\&amp;(2)\\int_{-\\infty}^{b}{f(x)}dx&#x3D;\\lim_{t\\rightarrow -\\infty}{\\int_{t}^{b}f(x)dx}\\&amp;(3)\\int_{-\\infty}^{0}{f(x)}dx和{\\int_{0}^{+\\infty}f(x)dx}都收敛,则{\\int_{-\\infty}^{+\\infty}f(x)dx}收敛\\&amp;且{\\int_{-\\infty}^{+\\infty}f(x)dx}&#x3D;\\int_{-\\infty}^{0}{f(x)}dx+{\\int_{0}^{+\\infty}f(x)dx}\\&amp;如果其中一个发散,结果也发散\\&amp;常用结论：\\int_a^{+\\infty}{\\frac{1}{x^p}dx}\\begin{cases}&amp;p&gt;1,收敛\\&amp;p\\leq1 ,发散\\\\end{cases},(a&gt;0)\\\\end{align}$$ 二、无界函数的反常积分$$\\begin{align}&amp;如果函数f(x)在点a的任一领域内都无界,那么点a为函数f(x)的瑕点(也称为无界点).无界函数的反常积分也成为瑕积分\\&amp;(1)设函数f(x)在(a,b]上连续,点a为f(x)的瑕点.如果极限\\lim_{t\\rightarrow a^+}{\\int_{t}^{b}{f(x)dx}}\\exist,\\&amp;则称此极限为函数f(x)在区间[a,b]上的反常区间,记作\\int_{a}^{b}f(x)dx,即\\int_{a}^{b}f(x)dx&#x3D;\\lim_{t\\rightarrow a^+}{\\int_{t}^{b}{f(x)dx}}\\&amp;这时也称反常积分\\int_a^b{f(x)dx}收敛,如果上述极限不存在，则反常积分\\int_a^b{f(x)dx}发散\\&amp;(2)设函数f(x)在[a,b)上连续,点b为函数f(x)的瑕点,则可以类似定义函数f(x)在区间[a,b]上的反常积分\\int_a^bf(x)dx&#x3D;\\lim_{t\\rightarrow b^-}{\\int_a^tf(x)dx}\\&amp;设函数f(x)在[a,b]上除点c(a&lt;c&lt;b)外连续,点c为函数f(x)的瑕点,如果反常积分\\int_a^c{f(x)dx}和\\int_c^b{f(x)dx}都收敛\\&amp;则称反常积分\\int_a^b{f(x)dx}收敛,且\\int_a^b{f(x)dx}&#x3D;\\int_a^c{f(x)dx}+\\int_c^b{f(x)dx}\\&amp;如果至少一个发散,则称\\int_a^b{f(x)dx}发散\\&amp;常用结论：\\&amp;\\int_a^b{\\frac{1}{(x-a)^p}}\\begin{cases}&amp;p&lt;1,收敛\\&amp;p\\geq 1,发散\\\\end{cases}\\&amp;\\int_a^b{\\frac{1}{(x-a)^p}}\\begin{cases}&amp;p&lt;1,收敛\\&amp;p\\geq 1,发散\\\\end{cases}\\\\end{align}$$ 三、例题例题1$$\\begin{align}&amp;\\int\\frac{1}{\\ln^{\\alpha}x}d(\\ln x)\\rightarrow^{\\ln x&#x3D;u}\\int{\\frac{du}{u^{\\alpha+1}}}\\begin{cases}&amp;{\\alpha-1&lt; 1}\\&amp;{\\alpha+1&gt;1}\\\\end{cases}\\Rightarrow 0&lt;\\alpha&lt;2\\\\end{align}$$ 定积分的应用一、几何应用1.平面图形的面积$$\\begin{align}&amp;(1)若平面域D由曲线y&#x3D;f(x),y&#x3D;g(x)(f(x)\\geq g(x)),x&#x3D;a,x&#x3D;b(a&lt;b)所围成,则平面域D的面积为\\&amp;S&#x3D;\\int_a^b{[f(x)-g(x)]dx}\\&amp;(2)若平面域D由曲线由\\rho&#x3D;\\rho(\\theta),\\theta&#x3D;\\alpha,\\theta&#x3D;\\beta(\\alpha&lt;\\beta)所围成,则其面积为S&#x3D;\\frac{1}{2}\\int_{\\alpha}^{\\beta}{\\rho^2(\\theta)d\\theta}\\end{align}$$ 2.旋转体的体积$$\\begin{align}&amp;若区域D由曲线y&#x3D;f(x)(f(x)\\geq 0)和直线x&#x3D;a,x&#x3D;b(0\\leq a&lt;b)及x轴所围成,则\\&amp;(1)区域D绕x轴旋转一周所得到的旋转体体积为V_x&#x3D;\\pi\\int_a^b{f^2(x)dx}\\&amp;(2)区域D绕y轴旋转一周所得到的旋转体体积为V_y&#x3D;2\\pi\\int_a^b{xf(x)dx}\\&amp;(3)区域D绕y&#x3D;kx+b轴旋转一周所得到的旋转体体积为V&#x3D;2\\pi\\int_D\\int{r(x,y)d\\sigma}\\&amp;例如：求y&#x3D;x,y&#x3D;x^2在第一象限的封闭图形绕转轴的体积\\\\end{align}$$ $$\\begin{align}&amp;V_x&#x3D;2\\pi\\int_D\\int yd\\sigma&#x3D;2\\pi\\int_0^1{dx}\\int_{x^2}^{x}ydy\\&amp;V_y&#x3D;2\\pi\\int_D\\int xd\\sigma&#x3D;2\\pi\\int_0^1{dx}\\int_{x^2}^{x}xdy\\&amp;V_{x&#x3D;1}&#x3D;2\\pi\\int_D\\int (1-x)d\\sigma\\&amp;V_{y&#x3D;2}&#x3D;2\\pi\\int_D\\int (2-y)d\\sigma\\\\end{align}$$ 3.曲线弧长$$\\begin{align}&amp;(1)C:y&#x3D;y(x),a\\leq x\\leq b,s&#x3D;\\int_a^b{\\sqrt{1+y’^2}dx}\\&amp;(2)C:\\begin{cases}&amp;x&#x3D;x(t)\\&amp;y&#x3D;y(t)\\\\end{cases},\\alpha \\leq t\\leq \\beta,s&#x3D;\\int_{\\alpha}^{\\beta}{\\sqrt{x’^2+y’^2}dx}\\&amp;(3)C:\\rho&#x3D;\\rho(\\theta),\\alpha \\leq \\theta\\leq \\beta,s&#x3D;\\int_{\\alpha}^{\\beta}{\\sqrt{\\rho^2+\\rho’^2}dx}\\\\end{align}$$ 4.旋转体侧面积$$\\begin{align}&amp;曲线y&#x3D;f(x)(f(x)\\geq 0)和直线x&#x3D;a,x&#x3D;b(0\\leq a&lt;b)及x轴所围成的区域绕x轴旋转所得到的旋转体的侧面积为\\&amp;S&#x3D;2\\pi\\int_a^b{f(x)\\sqrt{1+f’^2(x)}dx}\\\\end{align}$$ 二、物理应用1.压力2.变力做功3.引力（较少考）例题1$$\\begin{align}&amp;分析题意可知,该容器由x^2+y^2&#x3D;1的圆和x^2+(y-1)^2&#x3D;1的偏心圆组成\\&amp;根据图像的对称性可以避免不同表达式带来的困难\\&amp;对圆的小带子进行积分，带子长度为x，积分区间为-1到\\frac{1}{2}，\\int_{-1}^{\\frac{1}{2}}{\\pi x^2dy}\\&amp;由于图像的对称性，将积分结果乘二\\&amp;(1)V&#x3D;2\\pi\\int_{-1}^{\\frac{1}{2}}{x^2}dy&#x3D;2\\pi\\int_{-1}^{\\frac{1}{2}}{(1-y^2)dy}&#x3D;\\frac{9\\pi}{4}\\\\end{align}$$ $$\\begin{align}&amp;(2)W&#x3D;FS&#x3D;GS&#x3D;mgS&#x3D;\\rho VSg\\&amp;上部为W_1&#x3D;\\int_{\\frac{1}{2}}^{2}(2y-y^2)(2-y)dy\\rho g\\&amp;下部为W_2&#x3D;\\int^{\\frac{1}{2}}_{-1}(1-y^2)(2-y)dy*\\rho g\\&amp;W&#x3D;W_1+W_2\\\\end{align}$$ 例题2$$\\begin{align}&amp;F_p&#x3D;PA&#x3D;\\rho ghA\\&amp;将图像分为上部和下部，上部为矩形区域和下部的抛物线围成的面积区域，对其进行依次求解\\&amp;P_1&#x3D;2\\rho gh\\int_1^{h+1}{h+1-y}dy&#x3D;\\rho gh^2\\&amp;P_2&#x3D;2\\rho gh\\int_0^1{(h+1-y)\\sqrt{y}dy&#x3D;4\\rho g(\\frac{1}{3}h+\\frac{2}{15})}\\&amp;\\frac{P_1}{P_2}&#x3D;\\frac{4}{5}\\Rightarrow h&#x3D;2,h&#x3D;-\\frac{1}{3}(舍去)\\end{align}$$","categories":[],"tags":[]},{"title":"","slug":"HM_003_函数","date":"2023-08-09T03:00:39.499Z","updated":"2023-08-07T08:49:44.000Z","comments":true,"path":"2023/08/09/HM_003_函数/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_003_%E5%87%BD%E6%95%B0/","excerpt":"","text":"函数定义: 二要素：定义域&amp;对应关系$$\\begin{align}&amp;y&#x3D;f(x),x\\in R \\Leftrightarrow y&#x3D;(t),t\\in R\\&amp;\\int_{a}^{b}f(t)dt&#x3D;\\int_{a}^{b}f(x)dx\\&amp;\\sqrt{x^2}&#x3D;|x|&#x3D;(x^{2})^{\\frac{1}{2}}\\&amp;-\\sqrt{x^2}&#x3D;-|x|&#x3D;-(x^2)^{\\frac{1}{2}}\\end{align}$$ 例题：$$\\begin{align}&amp;证明：|\\frac{x}{1+x^2}|\\leq\\frac{1}{2}\\&amp;用a^2+b^2\\geq 2ab\\&amp;解：|\\frac{x}{1+x^2}|\\geq|\\frac{x}{2x}|&#x3D;\\frac{1}{2}得证\\end{align}$$ 基本初等函数：$$\\begin{align}&amp;常函数:y&#x3D;c\\&amp;幂函数:y&#x3D;x^a\\&amp;指数函数:y&#x3D;a^x\\&amp;对数函数:y&#x3D;\\log_{a}x\\&amp;三角函数:y&#x3D;\\sin x ,y&#x3D;\\cos x,y&#x3D;\\tan x,y&#x3D;\\csc x&#x3D;\\frac{1}{\\sin x},y&#x3D;\\sec x&#x3D;\\frac{1}{\\cos x},y&#x3D;\\cot x&#x3D;\\frac{1}{\\tan x}\\&amp;反三角函数:y&#x3D;\\arcsin x,y&#x3D;\\arccos x,y&#x3D;\\arctan x,y&#x3D;arccot x\\end{align}$$ arcsinx&amp;sinx$$\\begin{align}&amp;y&#x3D;\\arcsin x,x \\in [-1,1],y \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]\\&amp;\\arcsin (x+\\frac{\\sqrt{2}}{2})&#x3D;+\\frac{\\pi}{4}\\&amp;\\arcsin (x-\\frac{\\sqrt{2}}{2})&#x3D;-\\frac{\\pi}{4}\\&amp;\\arcsin (x+1)&#x3D;+\\frac{\\pi}{2}\\&amp;\\arcsin (x-1)&#x3D;-\\frac{\\pi}{2}\\&amp;\\arcsin x并不是\\sin x的反函数，只是\\sin x在x \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]内的逆映射\\end{align}$$ arccosx&amp;cosx$$\\begin{align}&amp;y&#x3D;\\arccos x,x \\in [-1,1],y \\in [0,\\pi]\\&amp;\\arccos (+1)&#x3D;0\\&amp;\\arccos (-1)&#x3D;\\pi\\&amp;\\arccos (\\frac{1}{2})&#x3D;\\frac{\\pi}{3}\\&amp;\\arccos x并不是\\cos x的反函数，只是\\cos x在x \\in [0,\\pi]内的逆映射\\end{align}$$ arctanx&amp;tanx$$\\begin{align}&amp;y&#x3D;\\arctan x,x \\in [-\\infty,+\\infty],y \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]\\&amp;\\arctan (+\\infty)&#x3D;\\frac{\\pi}{2}\\&amp;\\arctan (-\\infty)&#x3D;-\\frac{\\pi}{2}\\end{align}$$ arccotx&amp;cotx$$\\begin{align}&amp;y&#x3D;arccot x,x \\in [-\\infty,+\\infty],y \\in [0,\\pi]\\&amp;\\arctan (+\\infty)&#x3D;0\\&amp;\\arctan (-\\infty)&#x3D;\\pi\\end{align}$$ $$\\begin{align}&amp;连续函数存在反函数，其反函数也是连续函数\\&amp;一个可导函数存在反函数，其反函数不一定可导(可导函数x^3的反函数x^{\\frac{1}{3}}在0处并不可导)\\\\end{align}$$ 常用的三角恒等：$$\\begin{align}&amp;\\sin^{2}x+\\cos^{2}x&#x3D;1 \\&amp;\\sin 2x&#x3D;2\\sin x \\cos x&#x3D;\\frac{2\\tan x}{1+\\tan^{2}x}\\&amp;\\cos 2x&#x3D;\\cos^{2}x-\\sin^{2}x&#x3D;1-2\\sin^{2}x&#x3D;2\\cos^{2}x-1&#x3D;\\frac{1-\\tan^{2}x}{1+\\tan^{2}x}\\&amp;1+\\tan^{2}x&#x3D;\\sec^{2}x\\&amp;1+\\cot^{2}x&#x3D;\\csc^{2}x\\&amp;\\arcsin x+arccosx&#x3D;\\frac{\\pi}{2}(\\forall x\\in[-1,1])\\&amp;\\arctan x+arccot x&#x3D;\\frac{\\pi}{2}(\\forall x\\in(-\\infty,+\\infty))\\&amp;\\arcsin x+\\arcsin \\sqrt{1-x^2}&#x3D;\\frac{\\pi}{2}(\\forall x\\in[0,1])\\&amp;\\arctan x+\\arctan \\frac{1}{x}&#x3D;\\frac{\\pi}{2}(\\forall x \\in(-\\infty,0)\\cup(0,+\\infty))\\end{align}$$ 函数性质1）奇偶性$$\\begin{align}&amp;奇函数:f(-x)&#x3D;-f(x),偶函数f(-x)&#x3D;f(x)\\&amp;f,g为奇函数，f(g(x))为奇函数还是偶函数？\\&amp;用定义:\\&amp;f(g(-x))&#x3D;f(-g(x))&#x3D;-f(g(x))\\Leftrightarrow f(g(x))是奇函数\\end{align}$$ 判断奇偶性的方法：（1）定义 （2）奇偶函数的四则运算：奇函数代数和为奇函数，偶函数的代数和为偶函数，奇函数和偶函数的乘积为奇函数 （3）奇函数的复合运算：内外函数至少一个为偶函数，则复合函数为偶函数，奇函数与奇函数复合为奇函数 （4）奇函数的导数为偶函数，偶函数的导函数为奇函数，奇函数的原函数为偶函数，但偶函数的原函数未必为奇函数$$f(x)&#x3D;1,F(x)&#x3D;x+1,\\int_{0}^{x}f(t)dt为奇函数$$ 2）周期性3）单调性初等函数由基本初等函数经四则运算以及复合运算后得到的函数 一个初等函数的绝对值还是初等函数 函数有界性$$\\begin{align}&amp;有界\\Leftrightarrow 上下有界\\&amp;无界\\Leftrightarrow 上下界1个或没有\\end{align}$$ 判定有界性方法：$$\\begin{align}&amp;1)定义，对函数的绝对值放大不等式，直到某一正常值，按最值得到函数有界\\&amp;2)若f(x)在D1,D2上均有界，则在D1\\cup D2上也有界\\&amp;3)闭区间上的连续函数一定是有界的(开区间上的连续函数不一定是有界\\tan x)\\&amp;4)收敛数列必有界\\&amp;5)存在极限的函数局部有界\\&amp;\\lim_{x \\rightarrow x_0}f(x)\\Rightarrow f(x)必在x_0的某空心邻域内有界\\&amp;\\lim_{x \\rightarrow +\\infty}f(x)\\Rightarrow 必存在M,使f(x)在(M,+\\infty)内有界\\end{align}$$ 判定函数无界方法：$$\\begin{align}&amp;无穷大量必无界(lim_{x\\rightarrow \\frac{\\pi}{2}}\\tan x&#x3D;\\infty)\\&amp;局部无界必整体无界(f(x)&#x3D;x\\sin x)\\end{align}$$例题：$$\\begin{align}&amp;证明：f(x)&#x3D;\\frac{\\ln x}{x-1}在(0,1)内无界，在(1,+\\infty)内有界\\&amp;证：\\lim_{x\\rightarrow0^{+}}f(x)&#x3D;+\\infty \\Rightarrow f(x)在(0,\\epsilon)内无界 \\Rightarrow f(x)在(0,1)内无界\\&amp;\\lim_{x\\rightarrow1^{+}}f(x)&#x3D;1,从而\\exist \\epsilon&gt;0,使f(x)在(1,1+\\epsilon)内有界\\&amp;\\lim_{x\\rightarrow+\\infty}f(x)&#x3D;0,从而\\exist M&gt;1+\\epsilon,使f(x)在(1,+\\infty)内有界\\&amp;又由f(x)在[M,+\\infty]上连续，从而有界，使得f(x)在(1,+\\infty)内有界&amp;\\end{align}$$$$\\begin{align}&amp;注解：f(x)在(1,1)处无定义，导致f(x)在全局不连续，需要分类讨论\\&amp;在x\\rightarrow0^+时，在该空心邻域(0,\\epsilon)内函数极限等于+\\infty,故领域内无上界，可推出邻域内无界，进一步可推出f(x)在(0,1)无界\\&amp;在x\\rightarrow1^+时，在该空心领域(1,1+\\epsilon)内函数极限等于1，故领域内有上界，在x\\rightarrow +\\infty时,函数在邻域内极限等于0又由于f(x)在[1,+\\infty)内为连续函数，可以推出在x\\in [1,+\\infty)f(x)\\end{align}$$ 复合函数$$y&#x3D;f(u),u&#x3D;g(x) ,y\\leftarrow g\\leftarrow x(逐一传递)$$ 例题：$$\\begin{align}&amp;f(\\sqrt[3]{x}-1)&#x3D;x-1,求f(x)\\&amp;解：设u&#x3D;\\sqrt[3]{x}-1\\&amp;x&#x3D;(u+1)^3,f(u)&#x3D;(u+1)^3-1\\&amp;f(x)&#x3D;(x+1)^3-1\\&amp; \\&amp;注解：凑右侧表达式(反解很难时)\\\\end{align}$$ 反函数注解：$$\\begin{align}&amp;1.一一对应的函数有反函数，从而区间上的严格单调必有反函数\\&amp;y&#x3D;\\sin x无反函数，但y&#x3D;\\sin x,x\\in[-\\frac{\\pi}{2},\\frac{\\pi}{2}]\\&amp;2.函数x&#x3D;f^{-1}(y)与y&#x3D;f^{-1}(x)为同一函数，前者图像与y&#x3D;f(x)相同，后者图像与y&#x3D;f(x)关于y&#x3D;x对称\\&amp;3.对于\\forall x\\in D,f^{-1}(f(x))&#x3D;x;当y\\in f(D)时,有f(f^{-1}(y))&#x3D;y\\end{align}$$例题：$$\\begin{align}&amp;\\arcsin (\\sin \\theta)&#x3D;\\frac{\\pi}{4},且\\theta \\in(\\frac{\\pi}{2},\\pi),求\\theta&#x3D;?\\&amp;解：\\theta \\in (\\frac{\\pi}{2},\\pi) \\Rightarrow \\pi-\\theta \\in (0,\\frac{\\pi}{2}) \\&amp;\\arcsin (\\sin(\\pi - \\theta))&#x3D;\\frac{\\pi}{4} \\Rightarrow \\pi - \\theta&#x3D;\\frac{\\pi}{4}\\&amp;\\theta&#x3D;\\frac{3\\pi}{4}\\end{align}$$","categories":[],"tags":[]},{"title":"","slug":"HM_002_导数与微分","date":"2023-08-09T03:00:39.464Z","updated":"2023-08-07T08:49:44.000Z","comments":true,"path":"2023/08/09/HM_002_导数与微分/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_002_%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/","excerpt":"","text":"导数与微分(-)导数与微分概念1.导数$$\\begin{aligned}&amp;定义1：(导数)(相当于x_0处的变化率)f’(x_0)&#x3D;\\lim_{\\Delta x\\to 0} \\tfrac{\\Delta y}{\\Delta x}&#x3D;\\lim_{\\Delta x\\to 0}\\frac{f(x_0+x)-f(x_0)}{\\Delta x}&#x3D;\\lim_{h\\to 0}\\frac{f(x_0+h)-f(x_0)}{h}\\&amp;定义2：(左导数)(左领域内可导)\\&amp;f_-‘(x_0)&#x3D;\\lim_{\\Delta x\\to 0^-} \\tfrac{\\Delta y}{\\Delta x}&#x3D;\\lim_{\\Delta x\\to 0^-}\\frac{f(x_0+x)-f(x_0)}{\\Delta x}&#x3D;\\lim_{h\\to 0^-}\\frac{f(x_0+h)-f(x_0)}{h}\\&amp;定义2：(右导数)(右领域内可导)\\&amp;f_+’(x_0)&#x3D;\\lim_{\\Delta x\\to 0^+} \\tfrac{\\Delta y}{\\Delta x}&#x3D;\\lim_{\\Delta x\\to 0^+}\\frac{f(x_0+x)-f(x_0)}{\\Delta x}&#x3D;\\lim_{h\\to 0^+}\\frac{f(x_0+h)-f(x_0)}{h}\\\\end{aligned}$$ $$定理1：f’(x)可导\\Leftrightarrow f’-(x)\\exist f’+(x)\\exist,f’-(x)&#x3D;f’+(x)$$ 2.微分$$\\begin{align}&amp;定义4：(微分) 如果\\Delta y&#x3D;f(x_0+\\Delta x)-f(x_0)可以表示为\\&amp;\\Delta y&#x3D;A\\Delta x+o(\\Delta x)\\&amp;则称函数f(x)在点x_0处可微,称A\\Delta x为微分，记为dy&#x3D;A\\Delta x\\&amp;dy\\approx \\Delta y在一个微小的区域用均与的变量代替非均匀的变量\\&amp;微分是函数改变量的一个线性主部\\end{align}$$ $$定理2：函数y&#x3D;f(x)在点x_0处可微\\Leftrightarrow f(x)在点x_0处可导，且有dy&#x3D;f’(x_0)\\Delta x&#x3D;f’(x_0)dx$$ $$\\begin{align}&amp;S(x)&#x3D;x^2,S(x+\\Delta x)&#x3D;(x+\\Delta x)^2\\&amp;\\Delta S&#x3D;(x+\\Delta x)^2-x^2&#x3D;2x+(\\Delta x)^2&#x3D;2x+O(\\Delta x)\\&amp;线性主部(ds&#x3D;2x\\Delta x&#x3D;S’(x)\\Delta x)+高阶无穷小\\&amp;\\Delta S\\approx2x-\\Delta x(\\Delta x\\rightarrow 0)\\&amp;\\&amp;\\Delta f(x)&#x3D;A\\Delta x+O(\\Delta x)(x\\rightarrow 0)\\Leftrightarrow \\lim_{\\Delta x \\rightarrow 0}\\frac{\\Delta x-A\\Delta x}{\\Delta x}&#x3D;0\\&amp;(1)若\\exist A,使得\\lim_{\\Delta x \\rightarrow 0}\\frac{\\Delta f-A\\Delta x}{\\Delta x}\\&amp;(2)若f可微,则\\lim_{\\Delta x \\rightarrow 0}\\frac{\\Delta f-f’(x)\\Delta x}{\\Delta x}&#x3D;0\\\\end{align}$$ 3.导数与微分的几何意义 4.连续，可导，可微之间的关系 $$\\begin{align}&amp;注解：f(x)在x_0处连续\\Leftrightarrow \\lim_{x\\rightarrow x_0}f(x)&#x3D;f(x_0)\\&amp;\\Leftrightarrow \\lim_{x\\rightarrow x_0}[f(x)-f(x_0)]&#x3D;0 即\\lim_{\\Delta x\\rightarrow 0}\\Delta f&#x3D;0\\\\end{align}$$ (二)导数公式及求导法则 “乘除变加减”$$u^v&#x3D;e^{vln{u}}$$ $$y&#x3D;u^v \\Leftrightarrow ln{y}&#x3D;vln{u}$$ 高阶导数$$\\begin{align}&amp;设y&#x3D;Sin3x,求y^{(n)}\\&amp;y’&#x3D;Cos3x*3&#x3D;Sin(3x+\\frac{\\pi}{2})3\\&amp;y’’&#x3D;Cos(3x+\\frac{\\pi}{2})3^2&#x3D;Sin(3x+2\\frac{\\pi}{2})\\&amp;\\Rightarrow y^{(n)}&#x3D;Sin(3x+n\\frac{\\pi}{2})3^n\\&amp;y&#x3D;Sin(ax+b) \\Rightarrow y^{(n)}&#x3D;Sin(3x+n\\frac{\\pi}{2})*3^n\\end{align}$$ $$\\begin{align}&amp;设y&#x3D;x^2Cosx,求y^{(n)}\\&amp;令u&#x3D;x^2,v&#x3D;cosx\\&amp;u’&#x3D;2x,u’’&#x3D;2,u’’’&#x3D;0,…u^{(n)}&#x3D;0\\&amp;(uv)^{(n)}&#x3D;\\sum_{k&#x3D;0}^{n}u^{k}v^{(n-k)}\\&amp;y^{(n)}&#x3D;C_{n}^{0}x^2Cos(x+n*\\frac{\\pi}{2})+C_{n}^1(2x)Cos(x+(n-1)\\frac{\\pi}{2})+C^2_{n}(2)Cos(x+(n-2)*\\frac{\\pi}{2})\\end{align}$$ (三)微分法1.复合函数与初等函数的微分法(1).基本微分 (2).复合函数微分法（链式法则）$$\\begin{align}&amp;设函数u&#x3D;\\Phi(x)于点x处可导,函数y&#x3D;f(u)于点u&#x3D;\\Phi(u)可导\\&amp;则函数y&#x3D;f(\\Phi(x))在点x处可导,且\\frac{dy}{dx}&#x3D;\\frac{dy}{du}.\\frac{du}{dx}\\&amp;或写成[f(\\Phi(x))]’&#x3D;f(\\Phi(x))\\Phi’(x)\\\\end{align}$$ 例题$$\\begin{align}&amp;设u&#x3D;\\tan y,x&#x3D;e^t,试将下面y关于x的函数方程F(\\frac{d^2y}{dx^2},\\frac{dy}{dx},y,x)&#x3D;0\\&amp;解：\\&amp;y&#x3D;y(u(t(x))),t&#x3D;\\ln x,y&#x3D;\\arctan u,y&#x3D;\\arctan u(\\ln(x))\\&amp;\\frac{dy}{dx}&#x3D;\\frac{dy}{du}*\\frac{du}{dt}*\\frac{dt}{dx}&#x3D;\\frac{1}{1+u^2}*\\frac{du}{dt}*\\frac{1}{x}\\&amp;\\frac{d^2y}{dx^2}&#x3D;d[\\frac{dy}{du}]*\\frac{du}{dt}*\\frac{dt}{dx}+\\frac{dy}{du}*d[\\frac{du}{dt}]*\\frac{dt}{dx}+\\frac{dy}{du}*\\frac{du}{dt}*d[\\frac{dt}{dx}]\\&amp;[\\frac{2u}{(1+u^2)^2}]*\\frac{du}{dt}\\frac{dt}{dx}+\\frac{1}{1+u^2}[\\frac{d^2u}{dt^2}*\\frac{1}{x}]*\\frac{1}{x}+\\frac{1}{1+u^2}\\frac{du}{dt}[-\\frac{1}{x^2}]\\&amp;\\\\end{align}$$ 2.隐函数微分法例题$$\\begin{align}&amp;设y&#x3D;y(x)由方程e^y+6xy+x^2-1&#x3D;0确定,y’’(0);设y&#x3D;y(x)由方程xe^{f(y)}&#x3D;e^y确定,f二阶可导,且f’\\neq1,求y’’(x)\\&amp;e^yy’+6y+6xy’+2x&#x3D;0\\&amp;e^yy’’+e^y*(y’)^2+6xy’’+12y’+2&#x3D;0\\\\end{align}$$ 例题1.$$\\begin{align}&amp;设f’(x_0)&#x3D;-1,则\\lim_{x\\rightarrow 0}\\frac{x}{f(x_0-2x)-f(x_0-x)}&#x3D;?\\&amp;\\&amp;解1：\\&amp;I&#x3D;\\lim_{x\\rightarrow 0}\\frac{1}{(-2)\\frac{f(x_0-2x)-f(x_0)}{(-2x)}+\\frac{f(x_0-x)-f(x_0)}{-x}}&#x3D;-1\\&amp;解2：\\&amp;因为f’(x_0)&#x3D;-1,I&#x3D;\\lim_{x\\rightarrow 0}\\frac{1}{(-1)*\\frac{f(x_0-2x)-f(x_0-x)}{-x}}&#x3D;-1\\\\end{align}$$ 2.$$\\begin{align}&amp;设函数f(x)在x&#x3D;0处连续,且\\lim_{h\\rightarrow0}\\frac{f(h^2)}{h^2}&#x3D;1,则\\&amp;(A)f(0)&#x3D;0且f_{-}’(0)\\exist\\&amp;(B)f(0)&#x3D;1且f_{-}’(0)\\exist\\&amp;(C)f(0)&#x3D;0且f_{+}’(0)\\exist\\&amp;(D)f(0)&#x3D;1且f_{+}’(0)\\exist\\&amp;\\&amp;解：\\&amp;因为f(x)在x&#x3D;0处连续,\\lim_{x\\rightarrow 0}f(x)&#x3D;f(0)\\&amp;因为\\lim_{h\\rightarrow0}\\frac{f(h^2)}{h^2}&#x3D;1,h\\rightarrow0,h^2&#x3D;0,h\\rightarrow0,f(h^2)&#x3D;0\\&amp;\\lim_{h\\rightarrow0}\\frac{f(h^2)}{h^2}\\overset{令t&#x3D;h^2}{&#x3D;}\\lim_{t\\rightarrow0^+}\\frac{f(t)}{t}&#x3D;\\lim_{t\\rightarrow0^+}\\frac{f(t)-f(0)}{t-0}&#x3D;0\\Leftrightarrow f_{+}’(0)\\exist\\\\end{align}$$ 3.$$\\begin{align}&amp;设f(x)&#x3D;|x^3-1|\\Phi(x),其中\\Phi(x)在x&#x3D;1处连续,则f(x)在x&#x3D;1处可导的充要条件为\\Phi(1)&#x3D;?\\&amp;分析问题：f(x)在x&#x3D;1处什么条件可导\\&amp;解：\\&amp;\\lim_{x\\rightarrow 1^+}\\frac{f(x)-f(1)}{x-1}&#x3D;\\lim_{x\\rightarrow 1^+}\\frac{(x^3-1)\\Phi(x)}{x-1}&#x3D;\\lim_{x\\rightarrow 1^+}\\frac{(x-1)(x^2+x+1)\\Phi(x)}{x-1}&#x3D;3\\Phi(1)\\&amp;\\lim_{x\\rightarrow 1^-}\\frac{f(x)-f(1)}{x-1}&#x3D;\\lim_{x\\rightarrow 1^-}\\frac{(x^3-1)\\Phi(x)}{x-1}&#x3D;\\lim_{x\\rightarrow 1^-}\\frac{(x-1)(x^2+x+1)\\Phi(x)}{x-1}&#x3D;-3\\Phi(1)\\&amp;\\&amp;又因f(x)在x&#x3D;1处连续,则3\\Phi(1)&#x3D;-3\\Phi(1),所以\\Phi(1)&#x3D;0\\&amp;注解：a^n-b^n&#x3D;(a-b)(a^{n-1}b^0+a^{n-2}b^1+…+a^0b^{n-1})\\end{align}$$ 4.$$\\begin{align}&amp;设函数f(x)在x&#x3D;1处连续,且\\lim_{x\\rightarrow 1}\\frac{f(x)}{x-1}&#x3D;2,求f’(1).&amp;解：\\&amp;\\lim_{x \\rightarrow 1}f(x)&#x3D;f(1)\\&amp;\\lim_{x \\rightarrow 1}\\frac{f(x)}{x-1}(x-1)&#x3D;0&#x3D;f(1)\\&amp;f’(1)&#x3D;\\lim_{x \\rightarrow 1}\\frac{f(x)-f(1)}{x-1}&#x3D;2\\end{align}$$ 三、参数方程所确定函数的微分法$$\\begin{align}&amp;设y&#x3D;y(x)由参数方程\\cases{&amp;x&#x3D;x(t)\\&amp;y&#x3D;y(t)\\}所确定，则\\&amp;\\frac{dy}{dx}&#x3D;\\frac{y’(t)}{x’(t)}&#x3D;\\Phi’(\\Phi^{-1}(x),(\\Phi^{-1})(x)),(\\Phi^{-1})(t)&#x3D;\\frac{1}{\\Phi^1(t)},\\frac{y’’(t)x’(t)-y’(t)x’’(t)}{[x’(t)]^3}\\&amp;\\\\end{align}$$ $$\\begin{align}&amp;设y&#x3D;y(x)由参数方程\\cases{&amp;x&#x3D;x(t)\\&amp;y&#x3D;y(t)\\}所确定，则\\&amp;\\frac{dy}{dx}&#x3D;\\frac{\\frac{dx}{dt}}{\\frac{dy}{dt}}&#x3D;\\frac{x’(t)}{y’(t)}\\&amp;\\frac{d^2y}{dx}&#x3D;\\frac{d(\\frac{dy}{dx})}{dx}&#x3D;\\frac{(\\frac{x’(t)}{y’(t)})’}{x’(t)}\\\\end{align}$$ 例题：$$\\begin{align}&amp;y&#x3D;y(x)由参数方程\\begin{cases}&amp;x&#x3D;\\ln (1+t)^2\\&amp;y&#x3D;t-\\arctan t\\\\end{cases}求\\frac{d^2y}{dx}\\&amp;\\frac{dy}{dx}&#x3D;\\frac{\\frac{1+t^2}{1+t^2}-\\frac{1}{1+t^2}}{\\frac{2t}{1+t^2}}&#x3D;\\frac{t}{2}\\&amp;\\frac{d^2y}{dx}&#x3D;\\frac{\\frac{1}{2}}{\\frac{1}{t^+1}2t}&#x3D;\\frac{t^2+1}{4t}\\\\end{align}$$ 四、反函数的微分法$$\\begin{align}&amp;设函数y&#x3D;f(x)二阶可导，且f’(x)\\neq0，其反函数是x&#x3D;f^{-1}(y)\\&amp;则\\frac{dx}{dy}&#x3D;\\frac{1}{\\frac{dy}{dx}}&#x3D;\\frac{1}{f’(x)},\\frac{d^2x}{dy^2}&#x3D;-\\frac{\\frac{d^2y}{dx^2}}{(\\frac{dy}{dx})^3}&#x3D;-\\frac{f’’(x)}{[f’(x)]^3}\\end{align}$$ 五、分段函数的微分法在分段区间内，按初等函数的微分法求;在分段点处，用导数、左右导数定义及导数与左右导数的关系求$$\\begin{align}&amp;设f(x)&#x3D;\\cases{1-2x^2,x\\leq -1\\x^3,-1\\leq x\\leq2\\12x-16,x\\geq 2}\\&amp;(1)求f(x)的反函数g(x)\\&amp;(2)g(x)是否有间断点、不可导点\\end{align}$$","categories":[],"tags":[]},{"title":"","slug":"HM_001_极限","date":"2023-08-09T03:00:39.419Z","updated":"2023-08-07T08:49:44.000Z","comments":true,"path":"2023/08/09/HM_001_极限/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/HM_001_%E6%9E%81%E9%99%90/","excerpt":"","text":"极限极限的定义1）数列极限$$\\begin{align}&amp;\\lim_{n \\rightarrow \\infty}{x_n}&#x3D;A \\Leftrightarrow对于\\forall \\epsilon&gt;0,\\exist N,使得当n&gt;N时,有|x_n-A|&lt;\\epsilon\\&amp;\\lim_{x\\rightarrow \\infty}f(x)&#x3D;A\\Leftrightarrow \\forall \\epsilon&gt;0,\\exist M&gt;0,使得当|x|&gt;M时,有|f(x)-A|&lt;\\epsilon\\&amp;\\lim_{x\\rightarrow +\\infty}f(x)&#x3D;A\\Leftrightarrow \\forall \\epsilon&gt;0,\\exist M&gt;0,使得当x&gt;M时,有|f(x)-A|&lt;\\epsilon\\&amp;\\lim_{x\\rightarrow -\\infty}f(x)&#x3D;A\\Leftrightarrow \\forall \\epsilon&gt;0,\\exist M&gt;0,使得当x&lt;M时,有|f(x)-A|&lt;\\epsilon\\&amp;\\lim_{x\\rightarrow x_0}f(x)&#x3D;A\\Leftrightarrow \\forall \\epsilon&gt;0,\\exist M&gt;0,使得当x&gt;M时,有|f(x)-A|&lt;\\epsilon\\\\end{align}$$ 极限的性质1）局部保号性$$\\begin{align}&amp;若\\lim_{x\\rightarrow x_0}f(x)&#x3D;A&gt;0(&lt;0),\\&amp;则\\exist \\delta &gt;0,使得当x \\in U^0(x_0,\\delta)时,f(x)&gt;0(&lt;0)\\\\end{align}$$ 推论：保序性：$$\\begin{align}&amp;若\\lim_{x\\rightarrow x_0}f(x)&#x3D;A&gt;0(&lt;0),则\\forall \\alpha &lt;A(\\beta &gt;A),\\&amp;\\exist \\delta&gt;0,使得当x \\in U^0(x_0,\\delta)时,f(x)&gt;\\alpha(f(x)&lt;\\beta)\\\\end{align}$$ 2）局部有界性$$\\begin{align}若\\lim_{x\\rightarrow x_0}f(x)&#x3D;A,则\\exist U^0(x_0),使得f(x)在U^0(x_0)内有界\\end{align}$$ 3）不等式性质$$\\begin{align}&amp;若\\lim_{x \\rightarrow x_0}f(x)&#x3D;A,\\&amp;\\lim_{x \\rightarrow x_0}g(x)&#x3D;B都存在,\\&amp;且f(x)\\geq g(x),\\&amp;则A\\geq B\\&amp;\\&amp;注解：若将f(x)\\geq g(x)条件换为f(x)&gt;g(x),结论A&gt;B不一定成立\\&amp;x\\rightarrow +\\infty,\\frac{1}{x}&gt;\\frac{1}{x+1}不能推出\\lim_{x\\rightarrow +\\infty}\\frac{1}{x}&gt;\\lim_{x\\rightarrow +\\infty}\\frac{1}{x+1}\\&amp;但可以推出\\lim_{x\\rightarrow +\\infty}\\frac{1}{x}\\geq\\lim_{x\\rightarrow +\\infty}\\frac{1}{x+1}\\end{align}$$ 推论：$$若\\lim_{x\\rightarrow x_0}存在,且f(x)\\geq0(\\leq 0),则A\\geq 0(A\\leq 0)$$ 4）四则运算$$\\begin{align}&amp;若\\lim f(x)&#x3D;A,\\lim g(x)&#x3D;B,则\\&amp;\\lim [f(x) \\pm g(x)]&#x3D;A \\pm B\\&amp;\\lim f(x)g(x)&#x3D;A*B\\&amp;\\lim \\frac{f(x)}{g(x)}&#x3D;\\frac{A}{B},(B \\neq 0)\\&amp;\\&amp;注解：若\\lim f(x)不存在,\\lim g(x)&#x3D;B存在,\\&amp;则\\lim [f(x) \\pm g(x)]必不存在,\\&amp;但\\lim f(x)g(x)不一定必存在\\&amp;\\\\end{align}$$ 数列极限$$\\begin{align}&amp;定义1 \\lim_{n\\rightarrow \\infty}x_n&#x3D;A:\\&amp;\\forall \\epsilon&gt;0,\\exist N&gt;0,当n&gt;N时,恒有|x_n-A|&lt;\\epsilon\\&amp;注解：\\&amp;(1)\\epsilon与N的作用：\\&amp;\\epsilon刻画x_n与A的接近程度,N是描述n\\rightarrow \\infty的过程\\&amp;(2)几何意义：\\&amp;对任意一个给定的\\epsilon，在\\epsilon领域，当n足够大时，前有限项落在领域外，其余都落在(A-\\epsilon,A+\\epsilon)内\\\\end{align}$$ $$\\begin{align}&amp;(3)一个数列有没有极限与前有限项无关\\\\end{align}$$$$\\begin{align}&amp;(4)\\lim_{n\\rightarrow\\infty}x_n&#x3D;a\\Leftrightarrow\\lim_{k\\rightarrow\\infty}x_{2k-1}&#x3D;\\lim_{k\\rightarrow\\infty}x_{2k}&#x3D;a:\\&amp;数列极限\\exist\\overset{\\Rightarrow}{\\nLeftarrow}奇数列偶数列极限\\exist\\&amp;数列极限\\exist\\overset{\\Rightarrow}{\\Leftarrow}奇数列极限\\exist&#x3D;偶数列极限\\exist\\&amp;eg:a_n&#x3D;(-1)^n,a_{2k-1}&#x3D;-1,-1,-1,…,-1;a_{2k}&#x3D;1,1,1,…,1;\\lim_{k\\rightarrow \\infty}a_{2k-1}\\neq\\lim_{k\\rightarrow \\infty}a_{2k}\\end{align}$$ 例题1$$\\begin{align}&amp;法1：分奇偶数列\\&amp;奇数项：\\lim_{n\\rightarrow\\infty}(\\frac{n+1}{n})^{-1}&#x3D;1\\&amp;偶数项：\\lim_{n\\rightarrow\\infty}(\\frac{n+1}{n})^{1}&#x3D;1\\&amp;法2：缩放法+夹逼原理\\&amp;(\\frac{n+1}{n})^{-1}\\leq(\\frac{n+1}{n})^{(-1)^n}\\leq\\frac{n+1}{n}\\&amp;\\lim_{n\\rightarrow \\infty}(\\frac{n+1}{n})^{-1}&#x3D;1\\leq\\lim_{n\\rightarrow \\infty}(\\frac{n+1}{n})^{(-1)^n}\\leq\\lim_{n\\rightarrow \\infty}\\frac{n+1}{n}&#x3D;1\\&amp;I&#x3D;\\lim_{n\\rightarrow \\infty}(\\frac{n+1}{n})^{(-1)^n}&#x3D;1\\\\end{align}$$ 例题2$$\\begin{align}&amp;(1)解法:重要不等式||a|-|b||\\leq|a-b|\\&amp;因为\\lim_{n\\rightarrow\\infty}x_n&#x3D;a由极限定义可知\\&amp;\\forall \\epsilon&gt;0,\\exist N&gt;0,当n&gt;N时,|x_n-a|&lt;\\epsilon\\&amp;根据||x_n|-|a||\\leq|x_n-a|,\\&amp;则\\forall \\epsilon&gt;0,\\exist N&gt;0,当n&gt;N时,||x_n|-|a||&lt;\\epsilon\\&amp;反之不成立,例如x_n&#x3D;(-1)^n,则\\lim_{n\\rightarrow \\infty}|x_n|&#x3D;1&#x3D;|1|,但\\lim_{n\\rightarrow \\infty}(-1)^n不存在\\&amp;(2)由(1)可知,若\\lim_{n\\rightarrow\\infty}x_n&#x3D;0,则\\lim_{n\\rightarrow \\infty}|x_n|&#x3D;|0|&#x3D;0\\&amp;由\\lim_{n\\rightarrow\\infty}|x_n|&#x3D;0,\\forall \\epsilon&gt;0,\\exist N&gt;0,当n&gt;N时,||x_n|-0|&lt;\\epsilon\\&amp;即|x_n-0|&lt;\\epsilon\\\\end{align}$$ 求数列极限的方法：$$\\begin{align}&amp;（1）将数列极限转化为函数极限\\ \\end{align}$$ 函数极限1)自变量趋于无穷大时函数的极限 例题$$\\begin{align}&amp;极限\\lim_{x\\rightarrow \\infty}\\frac{\\sqrt{x^2+1}}{x}&#x3D;?\\&amp;解：\\&amp;\\sqrt x^2&#x3D;|x|\\&amp;分左右\\&amp;\\lim_{x\\rightarrow +\\infty}\\frac{x\\sqrt{1+\\frac{1}{x^2}}}{x}&#x3D;1\\&amp;\\lim_{x\\rightarrow -\\infty}\\frac{x\\sqrt{1+\\frac{1}{x^2}}}{x}&#x3D;-1\\&amp;\\lim_{x\\rightarrow -\\infty}\\frac{x\\sqrt{1+\\frac{1}{x^2}}}{x}\\neq\\lim_{x\\rightarrow +\\infty}\\frac{x\\sqrt{1+\\frac{1}{x^2}}}{x}\\&amp;\\lim_{x\\rightarrow \\infty}\\frac{\\sqrt{x^2+1}}{x}不存在\\end{align}$$ 2)自变量趋于有限值时函数的极限$$\\begin{align}&amp;注解：(1)\\epsilon的任意性,\\epsilon与\\delta时,恒有|f(x)-A|&lt;\\epsilon\\&amp;\\Rightarrow A-\\epsilon&lt;f(x)&lt;A+\\epsilon\\&amp;(2)几何意义：f(x_0)这一点可无定义，与去心邻域的函数值有关系\\\\end{align}$$ 易错点：$$\\begin{align}&amp;正确：\\lim_{x\\rightarrow 0}\\frac{\\sin x}{x}&#x3D;1(x\\rightarrow 0,x\\neq 0)\\&amp;错误：\\lim_{x\\rightarrow 0}\\frac{\\sin(x\\sin \\frac{1}{x})}{x\\sin\\frac{1}{x}}&#x3D;1\\&amp;需保证x{\\sin\\frac{1}{x}}\\rightarrow 0,x{\\sin\\frac{1}{x}}\\neq 0\\&amp;即在0点的去心领域内x{\\sin\\frac{1}{x}}\\neq0\\&amp;但当x&#x3D;\\frac{1}{n\\pi},使得x{\\sin\\frac{1}{x}}&#x3D;0\\&amp;所以原始极限不存在\\\\end{align}$$ 极限性质$$\\begin{align}&amp;1)有界性\\&amp;(1)数列有界性：如果数列[x_n],那么数列[x_n]一定有界\\&amp;x_n\\rightarrow a,n&gt;N,因为x_n\\leq M\\end{align}$$ $$\\begin{align}&amp;x_n前n项为有限元,必有一个数M大于前n项最大值\\&amp;收敛\\overset{\\Rightarrow}{\\nLeftarrow}有界\\&amp;eg:x_n&#x3D;(-1)^n\\&amp;(2)局部有界性：若\\lim_{x\\rightarrow x_0}f(x)\\exist,则f(x)在x_0某去心邻域有界\\&amp;\\lim_{x\\rightarrow x_0}f(x)\\exist\\overset{\\Rightarrow}{\\nLeftarrow}f(x)局部有界(去心邻域有界)\\&amp;eg:f(x)&#x3D;\\sin\\frac{1}{x},\\lim_{x\\rightarrow 0}{\\sin\\frac{1}{x}}有界,但不存在\\&amp;2)保号性\\&amp;(1)数列极限保号性\\&amp;设\\lim_{n\\rightarrow \\infty}{x_n}&#x3D;A\\&amp;[1]如果A&gt;0(或A&lt;0),则存在N&gt;0,当n&gt;N,x_n&gt;0(或x_n&lt;0)\\&amp;[2]如果存在N&gt;0,当n&gt;N时,x_n\\geq 0(或x_n\\leq0),则A\\geq0(或A\\leq 0)\\&amp;(2)函数极限保号性\\&amp;[1]如果A&gt;0(或A&lt;0),则存在\\delta&gt;0,当x\\in \\dot{U}(x_0,\\delta)时,f(x)&gt;0(或f(x_0)&lt;0)\\&amp;[2]如果存在\\delta&gt;0,当x\\in \\dot{U}(x_0,\\delta)时,f(x)\\geq0(或f(x)\\leq0),那么A\\geq0(或A\\leq0)\\end{align}$$","categories":[],"tags":[]},{"title":"","slug":"DS_0015_singleLinkedTable","date":"2023-08-09T03:00:39.375Z","updated":"2023-08-07T08:49:43.000Z","comments":true,"path":"2023/08/09/DS_0015_singleLinkedTable/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_0015_singleLinkedTable/","excerpt":"","text":"Data Structure Notes 123456Author : &quot;ebxeax&quot;Version : 1.0Refresh Date 2020.11.26Description : Just record and review some points about Data Structure.Have mistakes that please correct it yourself. 链式存储线性表 通过一组任意的存储单元来储存线性表的数据元素 为了建立数据元素之间的线性关系，对每个链表结点，除了存放元素信息外，还需要存放一个指向后继的指针 链式存储的宏定义 123456789struct LNode &#123; ElemType data; LNode* next; LNode() &#123; data = 0; next = nullptr; &#125;&#125;;typedef LNode* LinkList; 单链表的第一个结点之前附加一个结点，称为头结点。头结点不存放任何信息，也可以记录表长等相关信息。头结点的指针域指向线性表的第一个元素结点 头结点和头指针的区别： 不管带不带头结点，头指针始终指向链表的第一个结点，而 头结点是带头结点链表中的第一个结点，结点内通常不存储信息 为什么要设置头结点？ 处理操作起来方便 例如：对在第一元素结点前插入结点和删除第一结点起操作与其它结 点的操作就统一了 无论链表是否为空，其头指针是指向头结点的非空指针，因此空表和非空表的处理也就 统一了。 单链表的操作 1.初始化单链表 1234567int initLinkList(LinkList &amp;L)&#123; L=new LNode; if(!L)&#123; exit(OVERFLOWS); &#125; L-&gt;next=NULL;&#125; 2.销毁单链表 123456789int destroyLinkList(LinkList &amp;L)&#123; LinkList p; while(L)&#123; p=L; L=L-&gt;next; delete p; &#125; return OK;&#125; 3.清空单链表 1234567891011int clearLinkList(LinkList &amp;L)&#123; LinkList p,q; p=L-&gt;next; while(p)&#123; q=p-&gt;next; delete p; p=q; &#125; L-&gt;next=NULL; return OK;&#125; 4.获得单链表长度 12345678910int getLinkListLength(LinkList &amp;L)&#123; LinkList p; p=L-&gt;next; int i=0; while(p)&#123; i++; p=p-&gt;next; &#125; return i;&#125; 5.判断单链表是否为空 123456bool isLinkListEmpty(LinkList &amp;L)&#123; if(L-&gt;next) return false; else return true;&#125; 6.从头部创建单链表 123456789101112131415void createLinkList2Title(LinkList &amp;L,int n)&#123; L=new LNode; if(!L) exit(OVERFLOWS); L-&gt;next=NULL; for(int i=n;i&gt;0;i--)&#123; LNode *p=new LNode; if(!p) exit(OVERFLOWS); cout&lt;&lt;&quot;Please input elem&#x27;s data: \\n&quot;; cin&gt;&gt;p-&gt;data; p-&gt;next=L-&gt;next; L-&gt;next=p; &#125; 7.从头部创建单链表 123456789101112131415161718void crevoid createLinklist2Tail(LinkList&amp; L, int n)&#123; //正位序输入n个元素的值，建立带表头结点的单链表L L = new LNode; if (!L) exit(OVERFLOWS); //存储分配失败 L-&gt;next = NULL; LNode* r = L;//尾指针r指向头结点 for (int i = 0; i &lt; n; i++) &#123; LNode* p = new LNode;//生成新结点 if (!p) exit(OVERFLOWS); //存储分配失败 cout &lt;&lt; &quot;请输入元素的值:&quot;; cin &gt;&gt; p-&gt;data; //输入元素值 p-&gt;next = NULL; r-&gt;next = p; //插入到表尾 r = p; //r指向新的尾结点 &#125;&#125;ateLinkList2Tail(LinkList &amp;L,int n)&#123; &#125; 8.输出单链表全部元素 12345678910int linklistAll(LinkList p, int len)&#123; if (p == NULL) return ERROR; p = p-&gt;next; for (int i = 0; i &lt;len; i++) &#123; cout &lt;&lt; p-&gt;data &lt;&lt; &quot;\\n&quot;; p = p-&gt;next; &#125; return OK;&#125; 9.排序 1234567891011121314151617181920void swap(ElemType&amp; a, ElemType&amp; b) &#123; ElemType temp; temp = a; a = b; b = temp;&#125;void sorted(LinkList &amp;L,int len) &#123; //ElemType temp; int i; LNode* r = L; LNode* p = r-&gt;next; for (i = 0; i &lt; len; i++) &#123; if (r-&gt;data &gt; p-&gt;data) swap(r-&gt;data, p-&gt;data); r = r-&gt;next; p = p-&gt;next; &#125;&#125; 10.按序号查找结点 123456789101112LNode *GetElem(LinkList L,int i)&#123; int j=1; LNode*p=L-&gt;next; if(i==0)return L; if(i&lt;1)return NULL; while(p&amp;&amp;j&lt;i)&#123; p=p-&gt;next; j++; &#125; return p;&#125; 11.按值查找结点 123456LNode *locateElem(LinkList L,ElemType e)&#123; LNode *p=L-&gt;next; while(p!=NULL&amp;&amp;p-&gt;data!=e) p=p-&gt;next; return p;&#125; 12.插入元素 1234567void insert(LinkList &amp;L,ElemType e)&#123; LNode*p=L-&gt;next; LNode*s=p-&gt;next; p=GetElem(L,i-1); s-&gt;next=p-&gt;next; p-&gt;next=s; &#125; 13.删除元素 1234567void insert(LinkList &amp;L,ElemType e)&#123; LNode*p=L-&gt;next; LNode*q=p-&gt;next; p=GetElem(L,i-1); p-&gt;next=q-&gt;next; free(q); &#125; 附录： 完整程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162#include &lt;stdio.h&gt; // EOF(=^Z或F6),NULL#include &lt;stdlib.h&gt; // srand( ) ,rand( ),exit(n)#include &lt;malloc.h&gt; // malloc( ),alloc( ),realloc( )等#include &lt;limits.h&gt; // INT_MAX等#include &lt;string.h&gt;#include &lt;ctype.h&gt;#include &lt;math.h&gt; // floor(),ceil( ),abs( )#include &lt;iostream&gt; // cout,cin#include &lt;time.h&gt; // clock( ),CLK_TCK,clock_t#define TRUE 1#define FALSE 0#define OK 1#define ERROR 0#define INFEASIBLE -1#define OVERFLOWS -2typedef int Status; // Status是函数的类型,其值是函数结果状态代码，如OK等typedef int ElemType;#define MAXSIZE 100 //最大长度#define LIST_INIT_SIZE 100 // 线性表存储空间的初始分配量using namespace std;struct LNode &#123; ElemType data; LNode* next; LNode() &#123; data = 0; next = nullptr; &#125;&#125;;typedef LNode* LinkList;// *LinkList为LNode类型的指针int initLinklist(LinkList&amp; L) &#123; L = new LNode; if (!L) &#123; exit(OVERFLOWS); &#125; L-&gt;next = NULL; return OK;&#125;int destroyLinklist(LinkList&amp; L) &#123; LinkList p; while (L) &#123; p = L; L = L-&gt;next; delete p; &#125; return OK;&#125;int clearLinklist(LinkList&amp; L) &#123; LinkList p, q; p = L-&gt;next; while (p) &#123; q = p-&gt;next; delete p; p = q; &#125; L-&gt;next = NULL; return OK;&#125;int linklistLength(LinkList&amp; L) &#123; LinkList p; p = L-&gt;next; int i = 0; while (p) &#123; i++; p = p-&gt;next; &#125; return i;&#125;bool isLinklistEmpty(LinkList&amp; L) &#123; if (L-&gt;next) return false; else return true;&#125;void createLinklist2Title(LinkList&amp; L, int n) &#123; L = new LNode; if (!L) exit(OVERFLOWS); L-&gt;next = NULL; for (int i = n; i &gt; 0; i--) &#123; LNode* p = new LNode; if (!p) exit(OVERFLOWS); cout &lt;&lt; &quot;请输入元素的值:&quot;; cin &gt;&gt; p-&gt;data; p-&gt;next = L-&gt;next; L-&gt;next = p; &#125;&#125;void createLinklist2Tail(LinkList&amp; L, int n)&#123; //正位序输入n个元素的值，建立带表头结点的单链表L L = new LNode; if (!L) exit(OVERFLOWS); //存储分配失败 L-&gt;next = NULL; LNode* r = L;//尾指针r指向头结点 for (int i = 0; i &lt; n; i++) &#123; LNode* p = new LNode;//生成新结点 if (!p) exit(OVERFLOWS); //存储分配失败 cout &lt;&lt; &quot;请输入元素的值:&quot;; cin &gt;&gt; p-&gt;data; //输入元素值 p-&gt;next = NULL; r-&gt;next = p; //插入到表尾 r = p; //r指向新的尾结点 &#125;&#125;int linklistAll(LinkList p, int len)&#123; if (p == NULL) return ERROR; p = p-&gt;next; for (int i = 0; i &lt;len; i++) &#123; cout &lt;&lt; p-&gt;data &lt;&lt; &quot;\\n&quot;; p = p-&gt;next; &#125; return OK;&#125;void swap(ElemType&amp; a, ElemType&amp; b) &#123; ElemType temp; temp = a; a = b; b = temp;&#125;void sorted(LinkList &amp;L,int len) &#123; //ElemType temp; int i; LNode* r = L; LNode* p = r-&gt;next; for (i = 0; i &lt; len; i++) &#123; if (r-&gt;data &gt; p-&gt;data) swap(r-&gt;data, p-&gt;data); r = r-&gt;next; p = p-&gt;next; &#125;&#125;int main() &#123; LinkList p; initLinklist(p); int n; cout &lt;&lt; &quot;请输入单链表的长度 : &quot;; cin &gt;&gt; n; createLinklist2Tail(p, n); int len=linklistLength(p); linklistAll(p, len); printf(&quot;After Sorted func:\\n&quot;); sorted(p, len); linklistAll(p, len); return 0;&#125;","categories":[],"tags":[]},{"title":"","slug":"DS_0014_RecircleLinkedList","date":"2023-08-09T03:00:39.342Z","updated":"2023-08-07T08:49:42.000Z","comments":true,"path":"2023/08/09/DS_0014_RecircleLinkedList/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_0014_RecircleLinkedList/","excerpt":"","text":"Data Structure Notes 123456Author : &quot;ebxeax&quot;Version : 1.0Refresh Date 2020.11.26Description : Just record and review some points about Data Structure.Have mistakes that please correct it yourself. 链式存储循环单链表 区别：循环链表最后一个结点的指针不是NULL，而是指向头结点，从而形成一个闭环 单链表： 12graph LRHead--&gt;a1--&gt;a2--&gt;a3--&gt;a....-&gt;an--&gt;NULL 循环链表： 123graph LRHead--&gt;a1--&gt;a2--&gt;a3--&gt;a....-&gt;anan--&gt;Head 判空条件： 循环单链表的判空条件不是头结点的后继指针是否为空，而是它是否等于头指针 链式存储循环单链表 循环双链表：类比循环单链表，循环双链表链表区别于双链表就是首尾结点构成环 1234graph LRHead--&gt;a1--&gt;a2--&gt;a3--&gt;a....-&gt;an--&gt;Headan--&gt;a....-&gt;a3--&gt;a2--&gt;a1--&gt;Head--&gt;an 判空条件： 当循环双链表为空表时，其头结点的prior域和next域都等于Head","categories":[],"tags":[]},{"title":"","slug":"DS_0013_doubleLinkedTable","date":"2023-08-09T03:00:39.305Z","updated":"2023-08-07T08:49:42.000Z","comments":true,"path":"2023/08/09/DS_0013_doubleLinkedTable/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_0013_doubleLinkedTable/","excerpt":"","text":"Data Structure Notes 123456Author : &quot;ebxeax&quot;Version : 1.0Refresh Date 2020.11.26Description : Just record and review some points about Data Structure.Have mistakes that please correct it yourself. 链式存储双链表 双链表宏定义 1234typedef struct DNode&#123; ElemType data; struct DNode *prior,*next;&#125;DNode,*DLinkList; 123graph LRNode1--&gt;Node2--&gt;Node3--&gt;more--&gt;NodenNoden--&gt;more--&gt;Node3--&gt;Node2--&gt;Node1 1.插入 123456789void insert(DLinkList &amp;L,ElemType e,int i)&#123; DNode*p,*s; p=getElem(L,i-1); s.data=e; s-&gt;next=p-&gt;next; p-&gt;next-&gt;prior=s; s-&gt;prior=p; p-&gt;next=s;&#125; 12345678910graph LR Node1--&gt;Node2--&gt;|x|Node3--&gt;Node4--&gt;more--&gt;NodenNoden--&gt;more--&gt;Node4--&gt;Node3--&gt;|x|Node2--&gt;Node1 Node5 s--&gt;Node5 p--&gt;Node2 Node5--&gt;|new|Node2 Node5--&gt;|new|Node3Node2--&gt;|new|Node5Node3--&gt;|new|Node5 2.删除 12345678void del(DLinkList &amp;L,int i)&#123; DNode*p,*s; s=getElem(L,i-1); s-&gt;prior-&gt;next=s-&gt;next; p=s-&gt;prior; s-&gt;next-&gt;prior=s-&gt;prior; free(s);&#125; 123456graph LRNode1--&gt;Node2--&gt;|x|Node3--&gt;|x|Node4--&gt;more--&gt;NodenNoden--&gt;more--&gt;Node4--&gt;|x|Node3--&gt;|x|Node2--&gt;Node1s--&gt;|point to|Node3Node4--&gt;|new|Node2Node2--&gt;|new|Node4","categories":[],"tags":[]},{"title":"","slug":"DS_0012_linearTable","date":"2023-08-09T03:00:39.263Z","updated":"2023-08-07T08:49:42.000Z","comments":true,"path":"2023/08/09/DS_0012_linearTable/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_0012_linearTable/","excerpt":"","text":"Data Structure Notes 123456Author : &quot;ebxeax&quot;Version : 1.0Refresh Date 2020.11.26Description : Just record and review some points about Data Structure.Have mistakes that please correct it yourself. 线性表 1.定义：线性表是具有相同数据类型的n个数据类型的有限序列，n为表长 线性表中第一个元素称为表头元素，最后一个元素称为表位元素 除第一个元素外，每个元素仅有一个直接前驱，除最后一个元素外，每个元素有且仅有一个直接后继 顺序存储 线性表的顺序存储又称顺序表 使用一组地址连续的存储单元(数组等)依次存储线性表的数据元素，从而使得逻辑相邻的两个元素在物理位置上也相邻 三个属性： 1.存储空间的起始位置 2.顺序表最大存储容量 3.顺序表当前的长度 宏定义 静态分配大小 123456&gt;&gt;#define MaxSize 100&gt;&gt;typedef int Elemtype&gt;&gt;typedef struct&#123; Elemtype elem[MaxSize]; int length;&gt;&gt;&#125;SqList; 动态分配大小(这里动态指空间大小运行时决定，但分配大小后，空间大小被固定) 12345&gt;&gt;typedef int Elemtype&gt;&gt;typedef struct&#123; Elemtype *elem; int length;&gt;&gt;&#125;SqList; 优点：访问效率高、存储密度高 缺点：插入删除操作复杂 顺序存储线性表操作 1.初始化顺序存储线性表 1234567&gt;&gt;&gt;int initLinklist(SqList &amp;L)&#123;L.elem=new Elemtype[MaxSize]; if(!L.elem) exit(OVERFLOWS); L.length=0; return OK;&gt;&gt;&gt;&#125; （1）创建一个顺序存储表后，需要初始化，首先根据数组大小通过new在堆空间开辟一段连续的空间赋值于先前创建的顺序存储表的elem空间 （2）检查elem是否存在，不存在溢出退出程序 （3）将length元素赋值为0，即设置顺序存储线性表长度为0 2.销毁顺序存储线性表 1234&gt;&gt;&gt;void destroyList(SqList &amp;L)&#123; if(L.elem) delete(L.elem);&gt;&gt;&gt;&#125; 如果线性表存在，删除线性表elem开辟的空间 3.清空顺序存储线性表 123&gt;&gt;&gt;void clearList(SqList &amp;L)&#123; L.length=0;&gt;&gt;&gt;&#125; 将线性表的长度置为0 4.判断顺序存储线性表是否为空 123456&gt;&gt;&gt;bool isEmpty(SqList &amp;L)&#123; if(L.length==0) return 1; else return 0;&gt;&gt;&gt;&#125; 判断线性表长度是否为0，并返回相应bool值 5.引用类型按下表获取顺序存储线性表元素 123456int getElem(SqList L,int i,type&amp;e)&#123; if(i&lt;1||i&gt;L.length) return ERROR; e=L.elem[i-1]; return OK;&#125; （1）先检查传递参数下标量是否正确 （2）通过访问elem内数据存入引用类型变量内 6.按下表获取顺序存储线性表元素 12345Elemtype getElem(SqList L,int i)&#123; if(i&lt;1||i&gt;L.length) return ERROR; return L.elem[i-1];&#125; （1）先检查传递参数下标量是否正确 （2）通过访问elem内数据并返回 7.引用类型按值查询顺序存储线性表元素下标 12345int locateElem(SqList L,Elemtype e,int &amp;i)&#123; for(int i=0;i&lt;L.length;i++) if(L.elem[i]==e) return OK;&#125; 按照elem开辟空间进行迭代，当迭代元素与目标元素值相等时，将迭代量赋值于引用类型下标变量 8.按值获取顺序存储线性表元素下标 12345int locateElem(SqList L,Elemtype e)&#123; for(int i=0;i&lt;L.length;i++) if(L.elem[i]==e) return i;&#125; 按照elem开辟空间进行迭代，当迭代元素与目标元素值相等时，将迭代量返回 9.按下标插入元素 123456789int listInsert(SqList &amp;L,type e,int i)&#123; if(i&lt;1||i&gt;L.length) return ERROR; ++L.length; for(int j=L.length-1;j&gt;=i-1;j--) L.elem[j+1]=L.elem[j]; L.elem[i-1]=e; return OK;&#125; （1）先检查传递参数下标量是否正确 （2）增加线性表长度 （3）按照目标元素位置，将其尾部元素后移1偏移量 （4）将目标元素存入下标位置 时间复杂度分析: （1）$$最好情况：在表尾插入(即i&#x3D;n+1)$$ $$元素后移语句执行的时间复杂度为O(1)$$ （2）$$最坏情况：在表头插入(即i&#x3D;1)$$ $$元素后移语句执行n次，时间复杂度为O(n)$$ （3）$$平均情况：假设p_i(p_i&#x3D;1&#x2F;(n+1))$$ $$是第i个位置上插入一个结点的概率$$ $$则在长度为n的线性表中插入一个节点是需要移动结点的平均次数为$$ $$\\begin{equation*} f &#x3D; \\sum_{i&#x3D;1}^{n+1}p_i(n-i-1) \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\sum_{i&#x3D;1}^{n+1}{\\frac{n+1}{n-i+1}} \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\frac{1}{n+1} \\sum_{i&#x3D;1}^{n+1}(n-i-1) \\end{equation*}$$ $$&#x3D;\\frac{1}{n+1}\\frac{n(n+2)}{2}&#x3D;\\frac{n}{2}$$ $$因此顺序存储线性表的插入算法平均时间复杂度为O(n)$$ 10.按下标删除元素 123456789int listDelete(SqList &amp;L, int i) &#123; if ((i &lt; 1) || (i &gt; L.length)) return ERROR; for (int j = i; i &lt;= L.length - 1; j++) &#123; L.elem[j - 1] = L.elem[j]; --L.length; &#125; return OK;&#125; （1）先检查传递参数下标量是否正确 （2）按照目标元素位置，将其头部元素前移1偏移量 （3）减少线性表长度 时间复杂度分析: （1）$$最好情况：在表尾插入(即i&#x3D;n)$$ $$无需移动元素，时间复杂度为O(1)$$ （2）$$最坏情况：在表头插入(即i&#x3D;1)$$ $$需移动除第一个元素外的所有元素，时间复杂度为O(n)$$ （3）$$平均情况：假设p_i(p_i&#x3D;1&#x2F;(n+1))$$ $$是第i个位置上插入一个结点的概率$$ $$则在长度为n的线性表中插入一个节点是需要移动结点的平均次数为$$ $$\\begin{equation*} f &#x3D; \\sum_{i&#x3D;1}^{n}p_i(n-i) \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\sum_{i&#x3D;1}^{n}{\\frac{n}{n-i}} \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\frac{1}{n} \\sum_{i&#x3D;1}^{n}(n-i) \\end{equation*}$$ $$&#x3D;\\frac{1}{n}\\frac{n(n-1)}{2}&#x3D;\\frac{n-1}{2}$$ $$因此顺序存储线性表的插入算法平均时间复杂度为O(n)$$ 11.创建顺序存储线性表 12345678910int createList(SqList &amp;L, int n) &#123; type e; L.length = n; for (int i = 0; i &lt; n; i++) &#123; cout &lt;&lt; &quot;Please in put an element:&quot;; cin &gt;&gt; e; L.elem[i] = e; &#125; return OK;&#125; 11.打印顺序存储线性表内元素 123456void printList(SqList L) &#123; printf(&quot;\\nList&#x27;s element：\\n&quot;); for (int i = 0; i &lt; L.length; i++) &#123; cout &lt;&lt; &quot;elem[&quot; &lt;&lt; i &lt;&lt; &quot;] =&quot; &lt;&lt; L.elem[i] &lt;&lt; endl; &#125;&#125;","categories":[],"tags":[]},{"title":"","slug":"DS_0011_basicConcept","date":"2023-08-09T03:00:39.229Z","updated":"2023-08-07T08:49:42.000Z","comments":true,"path":"2023/08/09/DS_0011_basicConcept/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_0011_basicConcept/","excerpt":"","text":"Data Structure Notes 123456Author : &quot;ebxeax&quot;Version : 1.0Refresh Date 2020.11.26Description : Just record and review some points about Data Structure.Have mistakes that please correct it yourself. 数据结构的基本概念 1.数据 2.数据元素： 数据的基本单位，一个数据元素可有若干个数据项构成，数据项是不可分割的最小单位 3.数据类型 4.抽象数据类型(ADT[Abstract Data Type]): 数学模型在计算机的一种实现，包括数据对象、数据关系、基本操作，如建立一个有限状态机模型 5.数据结构：数据元素之间的关系称之为结构，数据结构包括三方面：逻辑结构、存储结构、数据运算(程序&#x3D;算法+数据结构) 6.逻辑结构：数据间的逻辑关系，与数据存储独立，分为线性结构和非线性结构 12345678910111213141516171819&gt;graph TD&gt;逻辑结构--&gt;线性结构&gt;逻辑结构--&gt;非线性结构&gt;线性结构--&gt;一般线性表&gt;线性结构--&gt;受限线性表&gt;线性结构--&gt;线性表推广&gt;受限线性表--&gt;栈和队列&gt;受限线性表--&gt;串&gt;线性表推广--&gt;数组&gt;线性表推广--&gt;广义表&gt;非线性结构--&gt;非线性表&gt;非线性表--&gt;集合&gt;非线性表--&gt;树形结构&gt;非线性表--&gt;图形结构&gt;树形结构--&gt;一般树&gt;树形结构--&gt;二叉树&gt;图形结构--&gt;有向图&gt;图形结构--&gt;无向图 7.物理结构：数据元素的表示以及关系的表示，主要有：顺序存储、链式存储、索引存储、散列存储 8.算法评估 （1）特性：有穷、确定、可行、输入、输出 （2）时间复杂度：衡量算法随问题规模的增大，算法执行的时间增长的快慢 T(n)&#x3D;O(f(n))，f(n)为算法运算频度，一般采用最坏情况下的时间复杂度 计算方法：取算法时间增长最快的函数项，忽略其系数 a加法规则： $$T(n)&#x3D;T_1(n)+T_2(n)&#x3D;O(f(n))+O(g(n))&#x3D;O(max(f(n),g(n)))$$ 多项式相加，只保留最高阶的项，且系数变为1 b乘法规则： $$T(n)&#x3D;T_1(n)*T_2(n)&#x3D;O(f(n))*O(g(n))&#x3D;O(f(n)*g(n))$$ 多项式相乘，都保留 从左到右性能依次降低： $$O(1)&lt;O(log_2n)&lt;O(n)&lt;O(nlog_2n)&lt;O(n^2)&lt;O(n^3)&lt;O(2^n)$$ 单循环体型： 例题1：计算下列程序的时间复杂度 int i,sum //执行1次 sum=0 //执行1次 for(i=0;i&lt;=n;i++)//int i=0执行1次，i&lt;=n执行n+2次，i++执行n+1次 sum+=i; //执行n+1次 时间分析： 该算法执行了3n+6个语句。 假设每个语句执行时间一致，均为常数t。则总时间$$T&#x3D;(3n+6)*t$$随着问题规模n的增大，总时间的增长率与n的增长率一致，所以复杂度为$$O(n)$$ 结论： 复杂度是关于增长率的，所以可以直接忽视常数项 一般可以直接关注循环段基本操作语句 sum+=i; 的执行次数 例题2： int sum,i; sum=0; for (i = 1;i &lt;= n;i=2*i)&#123; sum=sum+i; 时间分析： i 取值：1,2,4,8…满足条件：2^𝑘 ≤ nK&gt;𝑙𝑜𝑔_2𝑛时， 跳出循环所以循环体执行次数：⌈𝑙𝑜𝑔_2𝑛⌉ 故时间复杂度为O(logn).i 取值：1,2,4,8 多循环体型 两个运算法则：乘法规则（嵌套循环）、加法规则（若干循环） 例题3： int x,y,i,j; for(i=1;i&lt;=n;i++) x++; for(i=1;i&lt;=n;i++) for(j=1;j&lt;=n;j++) y++; 两个循环体是独立的，采用加法规则：$$ T(n)&#x3D;T_1(n)+T_2(n)$$ $$&#x3D;max(T_1(n),T_2(n)) &#x3D;O(n^2)$$ 例题4： int i,j,sum; &gt;&gt;for (i=1;i&lt;=n;i++) for(j=1;j&lt;=n;j=2*j) sum=sum+j; 两个循环体是嵌套的，采用乘法规则： $$T(n)&#x3D;T_1(n)*T_2(n)$$ $$ &#x3D;O(nlogn)$$ （3）空间复杂度：衡量算法随问题规模的增大，算法所需空间的快慢 S(n)&#x3D;O(g(n))，算法所需空间的增长率和g(n)的增长率相同 空间复杂度S(n)指算法运行过程中所使用的辅助空间的大小 线性表 1.定义：线性表是具有相同数据类型的n个数据类型的有限序列，n为表长 线性表中第一个元素称为表头元素，最后一个元素称为表位元素 除第一个元素外，每个元素仅有一个直接前驱，除最后一个元素外，每个元素有且仅有一个直接后继 顺序存储 线性表的顺序存储又称顺序表 使用一组地址连续的存储单元(数组等)依次存储线性表的数据元素，从而使得逻辑相邻的两个元素在物理位置上也相邻 三个属性： 1.存储空间的起始位置 2.顺序表最大存储容量 3.顺序表当前的长度 宏定义 静态分配大小 123456&gt;&gt;#define MaxSize 100&gt;&gt;typedef int Elemtype&gt;&gt;typedef struct&#123;&gt;&gt;Elemtype elem[MaxSize];&gt;&gt;int length;&gt;&gt;&#125;SqList; 动态分配大小(这里动态指空间大小运行时决定，但分配大小后，空间大小被固定) 12345&gt;&gt;typedef int Elemtype&gt;&gt;typedef struct&#123;&gt;&gt;Elemtype *elem;&gt;&gt;int length;&gt;&gt;&#125;SqList; 优点：访问效率高、存储密度高 缺点：插入删除操作复杂 顺序存储线性表操作 1.初始化顺序存储线性表 1234567&gt;&gt;&gt;int initLinklist(SqList &amp;L)&#123;L.elem=new Elemtype[MaxSize];&gt;&gt;&gt;if(!L.elem) exit(OVERFLOWS);&gt;&gt;&gt;L.length=0;&gt;&gt;&gt;return OK;&gt;&gt;&gt;&#125; （1）创建一个顺序存储表后，需要初始化，首先根据数组大小通过new在堆空间开辟一段连续的空间赋值于先前创建的顺序存储表的elem空间 （2）检查elem是否存在，不存在溢出退出程序 （3）将length元素赋值为0，即设置顺序存储线性表长度为0 2.销毁顺序存储线性表 1234&gt;&gt;&gt;void destroyList(SqList &amp;L)&#123;&gt;&gt;&gt;if(L.elem) delete(L.elem);&gt;&gt;&gt;&#125; 如果线性表存在，删除线性表elem开辟的空间 3.清空顺序存储线性表 123&gt;&gt;&gt;void clearList(SqList &amp;L)&#123;&gt;&gt;&gt;L.length=0;&gt;&gt;&gt;&#125; 将线性表的长度置为0 4.判断顺序存储线性表是否为空 123456&gt;&gt;&gt;bool isEmpty(SqList &amp;L)&#123;&gt;&gt;&gt;if(L.length==0) return 1;&gt;&gt;&gt;else return 0;&gt;&gt;&gt;&#125; 判断线性表长度是否为0，并返回相应bool值 5.引用类型按下表获取顺序存储线性表元素 123456int getElem(SqList L,int i,type&amp;e)&#123; if(i&lt;1||i&gt;L.length) return ERROR; e=L.elem[i-1]; return OK;&#125; （1）先检查传递参数下标量是否正确 （2）通过访问elem内数据存入引用类型变量内 6.按下表获取顺序存储线性表元素 12345Elemtype getElem(SqList L,int i)&#123; if(i&lt;1||i&gt;L.length) return ERROR; return L.elem[i-1];&#125; （1）先检查传递参数下标量是否正确 （2）通过访问elem内数据并返回 7.引用类型按值查询顺序存储线性表元素下标 12345int locateElem(SqList L,Elemtype e,int &amp;i)&#123; for(int i=0;i&lt;L.length;i++) if(L.elem[i]==e) return OK;&#125; 按照elem开辟空间进行迭代，当迭代元素与目标元素值相等时，将迭代量赋值于引用类型下标变量 8.按值获取顺序存储线性表元素下标 12345int locateElem(SqList L,Elemtype e)&#123; for(int i=0;i&lt;L.length;i++) if(L.elem[i]==e) return i;&#125; 按照elem开辟空间进行迭代，当迭代元素与目标元素值相等时，将迭代量返回 9.按下标插入元素 123456789int listInsert(SqList &amp;L,type e,int i)&#123; if(i&lt;1||i&gt;L.length) return ERROR; ++L.length; for(int j=L.length-1;j&gt;=i-1;j--) L.elem[j+1]=L.elem[j]; L.elem[i-1]=e; return OK;&#125; （1）先检查传递参数下标量是否正确 （2）增加线性表长度 （3）按照目标元素位置，将其尾部元素后移1偏移量 （4）将目标元素存入下标位置 时间复杂度分析: （1）$$最好情况：在表尾插入(即i&#x3D;n+1)$$ $$元素后移语句执行的时间复杂度为O(1)$$ （2）$$最坏情况：在表头插入(即i&#x3D;1)$$ $$元素后移语句执行n次，时间复杂度为O(n)$$ （3）$$平均情况：假设p_i(p_i&#x3D;1&#x2F;(n+1))$$ $$是第i个位置上插入一个结点的概率$$ $$则在长度为n的线性表中插入一个节点是需要移动结点的平均次数为$$ $$\\begin{equation*} f &#x3D; \\sum_{i&#x3D;1}^{n+1}p_i(n-i-1) \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\sum_{i&#x3D;1}^{n+1}{\\frac{n+1}{n-i+1}} \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\frac{1}{n+1} \\sum_{i&#x3D;1}^{n+1}(n-i-1) \\end{equation*}$$ $$&#x3D;\\frac{1}{n+1}\\frac{n(n+2)}{2}&#x3D;\\frac{n}{2}$$ $$因此顺序存储线性表的插入算法平均时间复杂度为O(n)$$ 10.按下标删除元素 123456789int listDelete(SqList &amp;L, int i) &#123; if ((i &lt; 1) || (i &gt; L.length)) return ERROR; for (int j = i; i &lt;= L.length - 1; j++) &#123; L.elem[j - 1] = L.elem[j]; --L.length; &#125; return OK;&#125; （1）先检查传递参数下标量是否正确 （2）按照目标元素位置，将其头部元素前移1偏移量 （3）减少线性表长度 时间复杂度分析: （1）$$最好情况：在表尾插入(即i&#x3D;n)$$ $$无需移动元素，时间复杂度为O(1)$$ （2）$$最坏情况：在表头插入(即i&#x3D;1)$$ $$需移动除第一个元素外的所有元素，时间复杂度为O(n)$$ （3）$$平均情况：假设p_i(p_i&#x3D;1&#x2F;(n+1))$$ $$是第i个位置上插入一个结点的概率$$ $$则在长度为n的线性表中插入一个节点是需要移动结点的平均次数为$$ $$\\begin{equation*} f &#x3D; \\sum_{i&#x3D;1}^{n}p_i(n-i) \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\sum_{i&#x3D;1}^{n}{\\frac{n}{n-i}} \\end{equation*}$$ $$\\begin{equation*} &#x3D;\\frac{1}{n} \\sum_{i&#x3D;1}^{n}(n-i) \\end{equation*}$$ $$&#x3D;\\frac{1}{n}\\frac{n(n-1)}{2}&#x3D;\\frac{n-1}{2}$$ $$因此顺序存储线性表的插入算法平均时间复杂度为O(n)$$ 11.创建顺序存储线性表 12345678910int createList(SqList &amp;L, int n) &#123; type e; L.length = n; for (int i = 0; i &lt; n; i++) &#123; cout &lt;&lt; &quot;Please in put an element:&quot;; cin &gt;&gt; e; L.elem[i] = e; &#125; return OK;&#125; 11.打印顺序存储线性表内元素 123456void printList(SqList L) &#123; printf(&quot;\\nList&#x27;s element：\\n&quot;); for (int i = 0; i &lt; L.length; i++) &#123; cout &lt;&lt; &quot;elem[&quot; &lt;&lt; i &lt;&lt; &quot;] =&quot; &lt;&lt; L.elem[i] &lt;&lt; endl; &#125;&#125;","categories":[],"tags":[]},{"title":"","slug":"DS_010-search-algorithm","date":"2023-08-09T03:00:39.197Z","updated":"2023-08-07T08:49:44.000Z","comments":true,"path":"2023/08/09/DS_010-search-algorithm/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_010-search-algorithm/","excerpt":"","text":"查找1.顺序查找一般表（1）代码 12345678910typedef struct&#123; ElemType *elem; int tableLen;&#125;SSTable;int searchSeq(SSTable ST, ElemType key)&#123; ST.elem[e] = key; //设置哨兵 for(int i = 0; i&lt;ST.tableLen; i++) return i; //存在返回, 不存在返回1&#125; （2）设置哨兵：可以不必判断是否越界，注意数据下表从1开始（3）ASL$$如果不能知道查找概率，可先对记录的查找概率进行排序，是表中的记录按查找概率从小到大\\ASL_{success} &#x3D; \\sum_{i&#x3D;1}^{n} P_i(n-i+1) &#x3D; \\frac{n+1}{2}\\ASL_{unsuccess} &#x3D; n+1\\$$（4）优缺点优点：对数据的存储无要求，顺序存储或者链式存储皆可缺点：当n较大，平均查找长度较大，效率低 有序表12345678910111213graph LRid1((10))--&gt;id2((20))id1((10)).-&gt;-infinity,10id2((20))--&gt;id3((30))id2((20)).-&gt;-infinity,20id3((30))--&gt;id4((40))id3((30)).-&gt;-infinity,30id4((40))--&gt;id5((50))id4((40)).-&gt;-infinity,40id5((50))--&gt;id6((60))id5((50)).-&gt;-infinity,50id6((60))--&gt;60,=id6((60)).-&gt;-infinity,60 （1）一旦查到某个元素大于该元素便停止查找（2）方框是虚构的节点，查找长度&#x3D;方框上的圆环（3)ASL$$ASL_{success} &#x3D; \\sum_{i&#x3D;1}^{n} P_i(n-i+1) &#x3D; \\frac{n+1}{2}\\ASL_{unsuccess} &#x3D; \\sum_{j&#x3D;1}^{n} Q_j(l_j-1) &#x3D; \\frac{1+2+…+n+n}{n+1} &#x3D; \\frac{n}{2} + \\frac{n}{n+1}\\$$ 折半查找（二分查找）12345678910111213graph LRid29((29))--&gt;id37((37))--&gt;id41((41))--&gt;id43((43))id43((43))--&gt;43,+infinityid43((43))--&gt;37,43id37((37))--&gt;id32((32))--&gt;id33((33))id32((32))--&gt;29,32id33((33))--&gt;33,37id33((33))--&gt;32,33id13((13))--&gt;id16((16))--&gt;id19((19))--&gt;19,29id19((19))--&gt;16,19id29((29))--&gt;id13((13))--&gt;id7((7))--&gt;id10((10))--&gt;10,13id10((10))--&gt;7,10id7((7))--&gt;-infinity,7 （1）仅适用于顺序表（2）代码 12345678910111213int binarySearch(SeqList L, ElemType key)&#123; int low, high, mid = 0, L.tableLen, 0; while(low &lt;= high)&#123; mid = (low + high) / 2; if(L.elem[mid] == key) return mid; else if(L.elem[mid] &gt; key) high = mid - 1; else low = mid + 1; &#125; return -1;&#125; （3）ASL$$ASL &#x3D; \\frac{1}{n}\\sum_{i&#x3D;1}^{n} l_i &#x3D; \\frac{1}{n}(11+22+…+h*2^{h-1}) &#x3D; \\frac{n+1}{n} log_2(n+1)-1 &#x3D; log_2(n+1)-1\\h&#x3D;[log_2(n+1)]（向上取整）$$ 查找11low&#x3D;7, high&#x3D;43, mid&#x3D;2911&lt;2912345678910111213graph 7--&gt;low1013161929--&gt;mid3233374143--&gt;high low&#x3D;7, high&#x3D;mid-1&#x3D;19, mid&#x3D;1311&lt;1312345678910111213graph 7--&gt;low1013--&gt;mid1619--&gt;high293233374143 low&#x3D;7, high&#x3D;mid-1&#x3D;7, mid&#x3D;1011&gt;712345678910111213graph 7--&gt;low--&gt;mid10--&gt;high131619293233374143 low&#x3D;mid+1&#x3D;10, high&#x3D;10, mid&#x3D;1010&gt;10 ×12345678910111213graph 710--&gt;low--&gt;mid--&gt;high131619293233374143 没找到，停在low分块查找（1）将查找表分为若干子块，块内可以无序，但块之间有序的 12345678910111213141516graph id24((24))--&gt;id((索引块24,54,78,88))id21((21))id6((6))id11((11))id8((8))id22((22))id32((32))--&gt;id((索引块24,54,78,88))id31((31))id54((54))id72((72))--&gt;id((索引块24,54,78,88))id61((61))id78((78))id88((88))--&gt;id((索引块24,54,78,88))id83((83)) （2）ASL$$n:长度\\b:分块个数\\s:每块s个记录\\P:等概率\\ASL &#x3D; L_I+L_S &#x3D; \\frac{b+1}{2}+\\frac{s+1}{2}&#x3D;\\frac{s^2+2s+n}{2s}\\s&#x3D;\\sqrt{n},ASL&#x3D;\\sqrt{n}+1\\采用折半查找：ASL&#x3D;L_I+L_S&#x3D;[log_2(b+1)]+\\frac{s+1}{2}（向上取整）$$ ###B树（多路平衡查找树）$$m阶B树或空树\\每棵子树至多m棵子树，最多包含m-1个关键字\\若根节点不是终端节点，至少两棵子树\\除根结点外所有非叶节点至少[\\frac{m}{2}]（向上取整）棵子树（关键字）\\$$ 123456789graph id[22]--&gt;id0[5,11]id[22]--&gt;id1[36,45]id0[5,11]--&gt;id00[1,3]id0[5,11]--&gt;id01[6,8,9]id0[5,11]--&gt;id02[13,15]id1[36,45]--&gt;id10[30,35]id1[36,45]--&gt;id11[40,42]id1[36,45]--&gt;id12[47,48,50,56]","categories":[],"tags":[]},{"title":"","slug":"DS_003_stack_queue","date":"2023-08-09T03:00:39.164Z","updated":"2023-08-07T08:49:43.000Z","comments":true,"path":"2023/08/09/DS_003_stack_queue/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_003_stack_queue/","excerpt":"","text":"栈 栈(Stack)：只允许在一端插入或删除的线性表 栈顶：线性表允许进行插入或删除的那一端 栈底：固定的，不允许进行插入和删除的另一端 特点：是受限的线性表，拥有线性关系；后进先出LIFO 顺序栈 使用顺序存储，自底向上存储数据元素，指针指向栈顶元素的位置 操作 1234s.top = -1; //判空s.data[++s.top] = x; //进栈x = s.data[s.top--]; //出栈x = s.data[s.top]; //读取栈顶元素 共享栈 两个栈共享一个一维数组空间 两个栈分别设置在共享空间两端 栈顶指向中间延伸位置 有利于空间使用 链式栈 采用链式存储 便于多个栈共享存储空间 效率高 队列 允许在一端插入，另一端删除的线性表 队头：允许删除的一端 队尾：允许插入的一端 先进先出FIFO 顺序队列 连续的存储单元 头指针指向队头元素 尾指针指向队尾元素 循环队列 首尾相连的顺序存储的队列 操作 1234Q.rear = Q.front = 0; // 初始化rear = (rear + 1) % MaxSize; // 入队front = (front + 1) % MaxSize; // 出队queueLen = (rear + MaxSize - front) % MaxSize; // 队列长度 判断空队列或满队列 123456789// 使用一个单元区分队空或队满(Q.rear + 1) % MaxSize = Q.front; //Q.front = Q.rear; //// 类型中增加表示个数的数据成员Q.size = 0; //Q.size = MaxSize; //// 增加tag成员tag = 0; //tag = 1; // 链式队列双端队列 允许两端可以入队和出队 输出受限的双端队列：允许一端进行插入和删除，另一端只允许插入的双端队列 输入受限的双端队列：允许一端进行插入和删除，另一端只允许删除的双端队列 应用栈在括号匹配的应用算法思想 空栈，一次读入一个符号 右括号：使栈顶元素消解，或不合法（序列不匹配，退出程序） 左括号：放入栈顶，作为更高优先级的一个元素，栈为空，否则括号序列不匹配 栈在表达式中的应用 中缀表达式转换后缀表达式 栈在递归中的应用 原理：将原始问题转换为相同属性的小规模问题 求出递归表达式 边界条件（递归出口） 队列队列在层次遍历的应用队列在计算机系统中的应用 解决主机与外设之间速度不匹配的问题 解决多用户引起的资源竞争问题 特殊矩阵压缩存储数组的存储结构 行优先：先存储行号较小的元素，行号相等先存储列号小的元素 列优先：先存储列好较小的元素，列号相等先存储行号小的元素 n阶对称矩阵 上三角、主对角线、下三角，其中上下三角元素相同 通常不使用二维数组存储，使用一维数组存储，元素$a_{ij}$在数组中下标为$k$ 元素下标之间的对于关系$i \\ge j , k &#x3D; \\frac{i*(i-1)}{2+j}-1(下三角区和主对角线元素)$$i &lt; j , k &#x3D; \\frac{j*(j-1)}{2+i}-1(上三角区元素)$ n阶三角矩阵 下三角矩阵（上三角区元素为常量）和上三角矩阵（下三角矩阵元素为常量） 通常不使用二维数组存储，使用一维数组存储，元素$a_{ij}$在数组中下标为$k 下三角矩阵的的元素下表之间的对应关系","categories":[],"tags":[]},{"title":"","slug":"DS_002_linearTable","date":"2023-08-09T03:00:39.131Z","updated":"2023-08-07T08:49:43.000Z","comments":true,"path":"2023/08/09/DS_002_linearTable/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_002_linearTable/","excerpt":"","text":"线性表一、逻辑结构和基本操作1. 逻辑结构 具有相同数据类型的n个数据元素的有限序列，表长n，n&#x3D;0为空表 表头：第一个元素 表尾：最后一个元素 除第一个元素外，每个元素有且仅有一个直接前驱 除最后一个元素外，每个元素有且仅有一个直接后继 2. 基本操作123456789initList(&amp;L);len(L);locateElem(L, i);getElem(L, i);listInsert(&amp;L, i, e);listDelete(&amp;L, i, &amp;e);printList(L);isEmptyList(L);destroyList(&amp;L); 二、顺序存储结构1. 定义：又称顺序表，使用一组地址连续的存储单元，依次存储线性表的数据元素，使逻辑上相邻的两个元素物理位置也相邻 存储空间的起始位置data[ ] 顺序表最大存储容量MaxSize 顺序表当前最大长度len特点 随机访存，O(1)时间复杂度访问 存储密度高，每个结点只存储数据元素 无需花费空间建立数据之间的逻辑关系，由物理位置相邻特性决定 逻辑上物理上均相邻，插入删除操作需要移动大量元素 2. 基本操作（1）插入元素 1234567891011//插入元素boolean listInsert(SqList &amp;L, int i, Elemtype e)&#123; if(i &lt; 1 || i &gt; L.len + 1) return -1; if(L.len &gt;= MaxSize) return -1; for(int j = L.len; j &gt; i; j--) L.data[j] = L.data[j - 1]; L.data[i - 1] = e; L.len++;&#125; 分析 最好情况：在表尾插入 **(i&#x3D;n+1)**，不需要移动元素，时间复杂度为O(1) 最坏情况：在表头插入 **(i&#x3D;1)**，元素后移n次，时间复杂度O(n) 平均情况：假设$P_i$ **($P_i &#x3D; \\frac{1}{n+1}$)**，是在第i个位置上插入一个结点的概率，则在长度为n的线性表中插入一个结点所需移动的平均次数为$\\frac{n}{2}$，其时间复杂度为O(n)（2）删除元素12345678910//删除元素boolean listDelete(SqList &amp;L, int i, Elemtype e)&#123; if(i &lt; 1 || i &gt; L.len + 1) return -1; e = L.data[i-1]; for(int j = i; j &lt; L.len; j++) L.data[j-1] = L.data[j]; L.len--; return true;&#125; 分析 最好情况：在表尾插入 **(i&#x3D;n)**，不需要移动元素，时间复杂度为O(1) 最坏情况：在表头插入 **(i&#x3D;1)**，元素后移n次，时间复杂度O(n) 平均情况：假设$P_i$ **($P_i &#x3D; \\frac{1}{n+1}$)**，是在第i个位置上插入一个结点的概率，则在长度为n的线性表中插入一个结点所需移动的平均次数为$\\frac{n-1}{2}$，其时间复杂度为O(n)（3）查找元素123456int locateElem(SqList &amp;L, Elemtype e)&#123; int i; for(i = 0; i &lt; L.len; i++) if(e == L.data[i]) return i+1;&#125; 分析 最好情况：查找的元素在表头，仅需比较1次，时间复杂度O(1) 最坏情况：查找的元素在表尾或不存在，需要比较n次，时间复杂度O(n) 平均情况：假设$P_i$ **($P_i &#x3D; \\frac{1}{n}$)**是在第i个位置上结点的概率，则在长度为n的线性表中插入一个结点所需移动的平均次数为$\\frac{n+1}{2}$，其时间复杂度为O(n) 链式存储结构1.创建单链表（1）头插法 为新插入的结点分配内存空间 每次都是把插入的新结点放入表头(头结点位置) 链表结点的次序与输入的顺序相反 （2）尾插法 为新插入的结点分配内存空间 每次都是把插入的新结点放入表尾(尾结点位置) 链表中的结点顺序与输入顺序一致 2.按值查找结点 在链表中从第一个结点出发，顺指针next逐个向下搜索，直到找到第i个结点，否则返回最后一个结点的指针域NULL 3.按序号查找结点 从链表第一个结点开始，由前往后按照序号递增定位到相应结点的位置，返回该值，需检查序号是否越界 4.插入 插入操作是将值为x的新结点插入到单链表的第i个位置 先检查插入位置是否合法 找到待插入位置的前驱结点 在其后将结点插入123p = getElem(L, i-1)s-&gt;next = p-&gt;next;p-&gt;next = s; 5.删除 将单链表的第i个结点删除 先检查插入位置是否合法 找到待删除位置的前驱结点 删除其后结点1234p = getElem(L, i-1)q = p-&gt;next;q-&gt;next = p-&gt;next;free(q); 双链表 双链表有两个指针prior和next，分别指向前驱和后继结点 插入操作1234s-&gt;next = p-&gt;next;p-&gt;next-&gt;prior = s;s-&gt;prior = p;p-&gt;next = s; 删除操作123p-&gt;next = q-&gt;next;q-&gt;next-&gt;prior = p;free(q); 循环链表 循环双链表和循环单链表 静态链表使用数组来描述线性表的链式存储结构","categories":[],"tags":[]},{"title":"","slug":"DS_001_introduction","date":"2023-08-09T03:00:39.091Z","updated":"2023-08-07T08:49:43.000Z","comments":true,"path":"2023/08/09/DS_001_introduction/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DS_001_introduction/","excerpt":"","text":"绪论一、数据结构：相互存在一种或多种特定关系的集合 结构：任何问题，数据元素不孤立存在，之间存在关系 逻辑结构 存储结构（物理结构） 数据的运算 逻辑结构和存储结构密不可分 算法设计取决于逻辑结构，实现依赖存储结构 二、逻辑结构：数据元素之间的逻辑关系 与存储无关，独立于计算机 分为线性结构和非线性结构线性结构：线性表、栈、队列、串、数组、广义表非线性结构：树、二叉树、有向图、无向图 三、存储结构（物理结构）：数据结构在计算机中的映像 数据结构的表示 关系的表示 依赖于计算机语言 分为顺序存储、链式存储、索引存储、散列存储 1. 顺序存储：存储的物理位置相邻，逻辑上相邻的元素物理位置也相邻 优：实现随机存取、每个元素占用的内存少 缺：只能使用相邻的一块存储单元，产生较多的外部碎片 2.链式存储：存储的物理位置未必相邻，逻辑上的相邻的元素在物理位置上未必相邻，通过记录相邻元素的物理位置来找到相邻元素 优：无碎片产生、充分利用存储单元 缺：只能顺序存储 3. 索引存储：类似目录4. 散列存储：通过关键字直接计算出元素的物理地址四、数据运算五、算法的五个特征 有穷性：执行有限步后结束 确定性：每条指令都有确定的含义，相同输入得到相同的输出 可行性：通过实现的基本运算执行有限次得到确定的结果 输入：有零或多个输入 输出：一个或多个程序输出结果 六、算法的复杂度1. 时间复杂度：衡量算法随问题规模增大，算法执行时间增加的快慢2. 空间复杂度：衡量算法随问题规模增大，算法占用空间增加的快慢","categories":[],"tags":[]},{"title":"","slug":"DB_001_introduce-database","date":"2023-08-09T03:00:39.043Z","updated":"2023-08-07T08:49:42.000Z","comments":true,"path":"2023/08/09/DB_001_introduce-database/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/DB_001_introduce-database/","excerpt":"数据库DB一、数据库系统概论基本概念数据库的四个基本概念：数据、数据库、数据库管理系统、数据库系统 数据DATA：描述事物的符号记录，数据的含义称为数据的语义，数据与其语义不可分 数据库DB：长期存储在计算机内、有组织的、可共享的大量数据的集合，数据库中的数据按一定数据模型组织、描述和存储，具有较小的冗余度、较高的数据独立性、易扩展性 数据库管理系统DBMS：位于操作系统和用户之间的一层数据管理软件，包括：数据定义、组织、存储、管理、操纵；数据库的事务管理、运行管理、建立、维护；其他； 数据库系统DBS：由数据库、数据库管理系统、应用程序、数据库管理员DBA组成的存储、管理、处理和维护的系统","text":"数据库DB一、数据库系统概论基本概念数据库的四个基本概念：数据、数据库、数据库管理系统、数据库系统 数据DATA：描述事物的符号记录，数据的含义称为数据的语义，数据与其语义不可分 数据库DB：长期存储在计算机内、有组织的、可共享的大量数据的集合，数据库中的数据按一定数据模型组织、描述和存储，具有较小的冗余度、较高的数据独立性、易扩展性 数据库管理系统DBMS：位于操作系统和用户之间的一层数据管理软件，包括：数据定义、组织、存储、管理、操纵；数据库的事务管理、运行管理、建立、维护；其他； 数据库系统DBS：由数据库、数据库管理系统、应用程序、数据库管理员DBA组成的存储、管理、处理和维护的系统 数据库技术发展人工管理阶段、文件系统阶段、数据库系统阶段 数据库系统特点 数据结构化：是与文件系统的本质区别 数据的共享性高、冗余度低且易扩充 数据独立性高 数据由数据库管理系统统一管理和控制：安全性保护、完整性保护、并发控制、数据库恢复 数据模型：是对现实世界数据特征的抽象，是数据库系统的核心和基础两类数据模型 第一类概念模型（信息模型）：按用户的观点对数据和信息建模，主要用于数据库设计——面向用户 第二类逻辑模型：包括层次模型、网状模型、关系模型、面向对象模型、对象关系模型、半结构化模型；是按计算机系统的观点对数据建模——面向设计人员 第二类物理模型：是对数据最底层的抽象，面向计算机系统 概念模型基本概念 实体：客观存在并且可相互区别的事物 属性：实体具有的某一特性 码：唯一标识实体的属性集 实体型：用实体名及属性名集合来抽象和刻画同类实体 实体集：同一类型实体的集合 联系：实体之间的联系通常是指不同实体集之间的联系，一对一、一对多、多对多等类型 一种表示方法：实体-联系方法 E-R模型 组成要素：数据模型通常由数据结构、数据操作、数据的完整性约束条件组成 数据结构：描述数据库的组成对象以及对象之间的关系 数据操作：对数据库中的各种对象的实例允许执行的操作的集合，包括操作及有关的操作规则（SQL） 数据的完整性约束条件：是一组完整性规则，保证数据的正确、有效和相容，任何关系需满足实体完整性和参照完整性（DBMS自动完成，之外的由设计人员完成，用户自己的完整性也需设计人员完成）；在某个表中某一个属性不能重复且不能为空 常用数据模型 层次模型（非关系模型） 网状模型（非关系模型） 关系模型（表） 面向对象模型（对象） 对象关系模型 半结构化模型 层次模型早期的数据组织方式，IBM的IMS，在数据库中定义满足 有且只有一个结点没有双亲结点，这个结点叫根结点 根以外的其他结点有且只有一个双亲结点 层次模型更像是一颗倒立的树 网状模型 允许一个以上的结点无双亲 一个结点可以有多于一个的双亲 层次模型中子女结点与双亲结点的联系是唯一的，网状模型不唯一 关系模型IBM E.F.Codd 数据结构 关系：一个关系对应一个表 元组：表中的一行 属性：表中的一列 码：表中某个属性组，可以唯一确定一个元组 域：一组有相同数据类型的集合 分量：元组的一个属性值 关系模式：关系的描述（关系名（属性1，属性2，…，属性n）） 要求关系必须规范化，每一个分量必须是一个不可分的数据项 数据库系统的结构 单用户结构 主从式结构 分布式结构 客户-浏览器结构 浏览器-应用服务器、数据库服务器多层结构 系统模式的概念在数据模型中有“型”和“值”的概念，型是指对某一类数据的结构和属性的说明，值是型的一个具体赋值模式是数据库中全体数据的逻辑结构和特征的描述，涉及型的描述，不涉及具体的值模式是相对稳定的，而实例是相对变动的 数据库系统的三级模式：外模式、模式、内模式 模式：又称逻辑模式或用户模式，是数据库用户能够看见和使用的局部数据的逻辑结构和特征的描述，是数据库用户的数据试图，是与某一用户有关的数据的逻辑表示 外模式：通常是模式的子集，一个数据库可以有多个外模式，如果不同的用户在应用需求、看待的数据方式、对数据保密的要求等方面存在差异，则其外模式描述是不同的；同一模式也可以为某一用户的多个应用系统所使用，但一个应用程序只能使用一个外模式 内模式：又称存储模式或物理模式，一个数据库只有一个内模式，是数据物理结构和存储方式的描述，是数据在数据库内部的组织方式 数据库二级映像功能与数据独立性 外模式&#x2F;模式映像：模式描述的是数据全局逻辑结构，外模式描述的是数据的局部逻辑结构，对于每一个外模式，数据库系统都有一个外模式&#x2F;模式映像，定义了外模式与模式之间的对应关系，通常包含在各自的外模式描述中；当模式改变时，有数据库管理员对各个外模式&#x2F;模式的映像做相应改变，可使外模式保持不变。应用程序是依据数据的外模式编写的，从而应用程序不必修改，保证了数据与程序的逻辑独立性，简称数据的逻辑独立性 模式&#x2F;内模式映像：数据库只有一个模式，也只有一个内模式，模式&#x2F;内模式是唯一的，定义了数据全局逻辑结构与存储结构之间的对应关系；当数据库的存储结构发生改变，由数据库管理员对模式&#x2F;内模式映像作相应改变，可使模式保持不变，从而应用程序不必改变，保证了数据与程序的物理独立性，简称数据的物理独立性 数据库系统的组成 硬件平台&amp;数据库 软件 人员 二、关系数据库《Communications of the ACM》E.F.Codd 关系数据结构及形式化定义:单一的数据结构，在用户看来逻辑结构是一张二维表 域：一组具有相同数据结构的值的集合$$\\begin{aligned}&amp;D_1, D_2, \\dots, D_n\\\\end{aligned}$$ 笛卡尔积：域上的一种集合运算$$\\begin{aligned}&amp;D_1 * D_2 * \\dots * D_n &#x3D; {(d_1, d_2, \\dots, d_n)|d_i\\in D_i, i &#x3D; 1, 2, \\dots, n}\\&amp;每一个元素(d_1, d_2, \\dots, d_n)叫做一个n元组\\&amp;元组每一个值d_i叫做一个分量\\&amp;基数：一个域允许的不同取值个数M&#x3D;\\prod_{i&#x3D;1}^{n}{m_i}\\end{aligned}$$ 关系：$$\\begin{aligned}&amp;D_1 * D_2 * \\dots * D_n的子集叫做在域D_1, D_2, \\dots\\&amp;D_n上的关系，表示为R(D_1, D_2, \\dots, D_n)\\&amp;R表示为关系的名字，n是关系的目或度\\&amp;n&#x3D;1，单元关系\\&amp;n&#x3D;2，二元关系\\&amp;某一个属性组的值能唯一标识一个元组，而其子集不能，则称该属性组为候选码\\&amp;若一个关系有多个候选码，则选定一个为主码\\&amp;候选码的诸属性成为主属性，不包含在任何候选码中的属性成为非主属性或非码属性\\&amp;在最简单情况下，候选码只包含一个属性；在最极端情况下，关系模式的所有属性是这个关系模式的候选码，称为全码\\&amp;关系有三种类型：\\&amp;基本关系（基本表或基表）：实际存在的表，实际存储数据的逻辑表示；\\&amp;查询表：查询结果对应的表；\\&amp;视图表：基本表或其他视图表导出的表，虚表，不对应实际存储的数据\\&amp;关系是一个无限的集合，由于笛卡尔积的域不满足交换律\\&amp;(d_1, d_2, \\dots, d_n) \\neq (d_2, d_1, \\dots, d_n)(i, j &#x3D;1, 2, \\dots, n)\\&amp;对关系数据模型的限定和扩充：\\&amp;(1)无限关系在数据库系统中是无意义的，限定关系数据模型中的关系必须是有限集合\\&amp;(2)通过关系的每个列附加一个属性名的方法取消属性的有序性\\&amp;基本关系的性质：\\&amp;(1)列是同质的\\&amp;(2)不同列可出自同一个域\\&amp;(3)列顺序可无序\\&amp;(4)任意两个元组的候选码不能取同值\\&amp;(5)行的顺序可无序\\&amp;(6)分量必须取原子值\\&amp;这些规范中最基本的是每一个分量必须是一个不可分的数据项\\\\end{aligned}$$ 关系模式：必须指出这个元组集合的结构，即由哪些属性构成，属性来自哪些域，属性和域之间的映像关系$$\\begin{aligned}&amp;关系模式：R(U, D, DOM, F)\\&amp;R：关系名\\&amp;D：U中属性所来自的域\\&amp;DOM：属性向域的映射集合\\&amp;F：属性间数据的依赖关系\\\\end{aligned}$$关系模式是静态的、稳定的，而关系是的动态的、随时间变化的； 关系数据库:有型和值的区分 型：关系数据库模式，是对关系数据库的描述 值：关系模式在某一时刻对应的关系的集合 关系操作基本操作：查询、插入、删除、修改查询：选择、投影、连接、除、并、差、笛卡尔积；{选择、投影、并、差、笛卡尔积}是5种基本操作关系操作的特点：集合操作方式，一次一集合操作；非关系模型的数据操作为一次一记录方式 关系数据语言：关系代数、关系演算 关系代数：用对关系的运算来表达查询要求 关系演算：用谓词来表达查询要求，按谓词变元的基本对象是元组变量还是域变量分为元组关系演算和域关系演算 SQL(Structured Query Language)是集查询、数据定义语言、数据操纵语言和数据控制语言DCL(Data Control Language)于一体的关系数据语言 关系代数语言:ISBL 关系演算语言:元组关系演算语言：ALPHA、QUEL；域关系演算语言：QBE 关系代数&amp;关系演算-语言:SQL 关系的完整性：是对关系的某种约束条件，有三类约束：实体完整性：若属性A是基本关系R的主属性，则A不能取空值；关系数据库中每个元组是可区分的，是唯一的，这点用实体的完整性来保证实体完整性规则说明：（1）实体完整性是针对基本关系而言，一个基本表对应现实世界的一个实体集（2）现实世界中的实体是可区分的，即它们具有某种唯一标识（3）关系模型中以主码作为唯一性标识（4）主码中的属性即主属性不能取空值 参照完整性：定义外码与主码之间的引用规则$$\\begin{aligned}&amp;设F是基本关系R的一个或一组属性，但不是关系R的码，K_s是基本关系S的主码\\&amp;如果F与K_s相对应，则称F是R的外码，并称基本关系R为参照关系，基本关系S为被参照关系或目标关系\\&amp;关系R和S不一定是不同的关系\\&amp;目标关系S的主码与K_s和参照关系R的外码F必须定义在同一个（同一组）\\\\end{aligned}$$ 参照完整性规则$$\\begin{aligned}&amp;若属性（或属性组）F是基本关系R的外码，它与基本关系S的主码K_s相对应（基本关系R和S不一定是不同的关系），则对于R中每个元组在F上的值必须\\&amp;取空值 \\space 等于S中某个元组的主码值 \\space空值 \\space非空值\\\\end{aligned}$$ 用户定义的完整：与现实语义相结合关系代数：抽象的查询语言，使用对关系的运算来表达查询[运算对象、运算符、运算结果]；按运算符分为传统的集合运算和专门的关系运算传统的集合运算（二目运算）：并、差、交、笛卡尔积$$\\begin{aligned}&amp;设关系R和S具有相同的目n（即两个关系都有的n个属性）\\&amp;具有相应的属性取自同一个域，t是元组变量，t\\in R表示t是R的一个元组\\&amp;（1）并\\&amp;R \\cup S &#x3D; { t | t\\in R \\vee t\\in S}\\&amp;（2）差\\&amp;R - S &#x3D; { t | t\\in R \\wedge t\\notin S}\\&amp;（3）交\\&amp;R \\cap S &#x3D; { t | t\\in R \\wedge t\\in S}\\&amp;（4）广义笛卡尔积\\&amp;R \\times S &#x3D; { \\widehat{t_r t_s} | t_r\\in R \\wedge t_S\\in S}\\\\end{aligned}$$$\\large{R}$: A B C $a_1$ $b_1$ $c_1$ $a_1$ $b_2$ $c_2$ $a_2$ $b_2$ $c_1$ $\\large{S}$: A B C – – – $a_1$ $b_2$ $c_2$ $a_1$ $b_3$ $c_2$ $a_2$ $b_2$ $c_1$ $\\large{R\\cup S}$: A B C – – – $a_1$ $b_1$ $c_1$ $a_1$ $b_2$ $c_2$ $a_2$ $b_2$ $c_1$ $a_1$ $b_3$ $c_2$ $\\large{R\\cap S}$: A B C – – – $a_1$ $b_2$ $c_2$ $a_2$ $b_2$ $c_1$ $\\large{R\\times S}$: R.A R.B R.C – – – $a_1$ $b_1$ $c_1$ $a_1$ $b_1$ $c_1$ $a_1$ $b_1$ $c_1$ $a_1$ $b_2$ $c_2$ $a_1$ $b_2$ $c_2$ $a_1$ $b_2$ $c_2$ $a_2$ $b_2$ $c_1$ $a_2$ $b_2$ $c_1$ $a_2$ $b_2$ $c_1$ 专门的关系运算：选择、投影、连接、除运算（1）设关系模式为R($A_1$,$A_2$,$\\dots$,$A_n$)，它的一个关系为R $t\\in R$表示t是R的一个元组 t$[A_i]$表示元组中相应的属性$A_i$的一个分量（2）若$A&#x3D;{A_{i1}, A_{i2}, \\dots, A_{ik}}$ ，其中$A_{i1}, A_{i2}, \\dots, A_{ik}$是$A_1, A_2, \\dots, A_n$中的一部分，则$A$称为属性列或属性组 $t[A]&#x3D;(t[A_{i1}], t[A_{i2}], \\dots, t[A_{ik}])$表示元组$t$在属性列$A$上诸多分量的集合 $\\overline{A}$则表示$A&#x3D;{A_{1}, A_{2}, \\dots, A_{n}}$中去掉$A&#x3D;{A_{i1}, A_{i2}, \\dots, A_{ik}}$后剩余的属性组（3）$R$为$n$目关系，$S$为$m$目关系，$t_r\\in R, t_s \\in S$，$\\widehat{t_r t_s}$ 为元组的连接（串接） 是一个$n+m$的元组（4）给定一个关系$R(X,Z)$，$X$和$Z$为属性组 $t[X]&#x3D;x$，$x$在$R$中的象集：$Z_x&#x3D;{t[Z]|t\\in R, t[X]&#x3D;x}$ 表示R中属性组X上的值为x的诸多元组在Z上的分量集合$\\large{R}$ $x$ $Z$ $x_1$ $Z_1$ $x_1$ $Z_2$ $x_1$ $Z_3$ $x_2$ $Z_2$ $x_2$ $Z_3$ $x_3$ $Z_1$ $x_3$ $Z_3$ $x_1$在$R$中的象集$Z_{x_1}&#x3D;{Z_1, Z_2, Z_3}$$x_2$在$R$中的象集$Z_{x_2}&#x3D;{Z_2, Z_3}$$x_3$在$R$中的象集$Z_{x_3}&#x3D;{Z_1, Z_3}$ （1）选择（限制）：在关系$R$中选择满足条件的诸多元组$\\sigma_{F}(R)&#x3D;{t|t\\in R \\cap F(t)&#x3D;’True’}$ $F(t)&#x3D;X_1 \\theta Y_1$ $\\theta$:比较远算符（2）投影：关系$R$选择出若干属性列组成新的关系$\\prod_A(R)&#x3D;{t[A|t \\in R}$ $A$是$R$的属性列投影之后不仅取消原关系中的某些列，而且还可能取消某些组 查询关系Student在所在属性上的投影$\\large{\\prod_{Sdept}(Stud)}$ $\\LARGE{b &#x3D; \\prod_{Sdept}(a)}$ $\\LARGE{a}$ Sname Sdept Nick CS Cay CS John MA West IS $\\LARGE{b}$ Sdept – CS – MA – IS – （3）连接（$\\theta$连接）：从两个关系的笛卡尔积中选取属性间满足一定条件的元组$R\\mathop{\\bowtie}\\limits_{A \\theta B} S &#x3D;{\\widehat{t_r t_s}|t_r \\in R \\cap t_s \\in S \\cap t_r[A] \\theta t_s[B]}$","categories":[],"tags":[]},{"title":"","slug":"CP_002_DFA-[Deterministic-Finite-Automaton]","date":"2023-08-09T03:00:39.011Z","updated":"2023-08-07T08:49:41.000Z","comments":true,"path":"2023/08/09/CP_002_DFA-[Deterministic-Finite-Automaton]/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/CP_002_DFA-[Deterministic-Finite-Automaton]/","excerpt":"","text":"DFA-[Deterministic Finite Automaton]在计算理论中，确定有限状态自动机或确定有限自动机（英语：deterministic finite automaton, DFA）是一个能实现状态转移的自动机。对于一个给定的属于该自动机的状态和一个属于该自动机字母表Σ的字符，它都能根据事先给定的转移函数转移到下一个状态（这个状态可以是先前那个状态）。可以通过建立状态机来解决问题。每次输入都会引起状态的改变或者不变。再次输入一个值，状态又会改变。我们把所有状态罗列出来，每次输入都改变他的状态。如果最后的状态是合法的，那么证明这个输入符合条件。相应的数学建模中也用到类似的算法思想，解决商人过河问题、人狼羊菜过河问题，以及操作系统中也有涉猎，输出进程安全序列也是类似的思想 DFA是一个由五元组定义的数学模型：M&#x3D;(S，∑，δ，s0，F) （1）S是一个有穷状态集合 （2）∑是一个有穷的输入字母表 （3）δ是状态转换函数，是SX∑-&gt;S上的单值部分映射，δ(s,a)&#x3D;s’(s∈S,s’∈S) （4）s0是唯一初态 （5）F是终态集合，F是S的子集 NFAM &#x3D; ( S，Σ ，δ，s0，F ) （1） S：有穷状态集（2）Σ：输入符号集合，即输入字母表。假设ε 不是Σ中的元素（3）δ：将S×Σ映射到2S的转换函数。s∈S, a∈Σ, δ(s,a)表示从状态s出发，沿着标记为a的边所能到达的状态集合（4）s0：开始状态 (或初始状态)，s0∈S（5）F：接收状态（或终止状态）集合，F⊆ S dfa和nfa的区别 NFA DFA 初始状态 不唯一 唯一 弧上的标记 字(单字符字&#x2F;ε) 字符(串) 转换关系 非确定 确定 两个DFA的等价指的是对任何两个DFA,M和M’，如果L(M)&#x3D;L(M’)，则称两者是等价的 NFA的确定化-子集法思路：DFA的每一个状态对应于NFA的一组状态，用DFA的一个状态去记录NFA输入一个符号后可能达到的状态集合 （1）状态集合I的闭包ε-Closure(I)，定义 若q∈I，q∈ε-Closure(I) 若q∈I，设从q出发经过任意条ε弧能达到的状态为q‘，则q’∈ε-Closure(I） （2）状态集合I的a弧转换表示为Ia，Ia&#x3D;ε-Closure(move(I,a)) DFA的最小化对于给定的DFA M,寻找一个状态数比M小的DFA M’使得L(M)&#x3D;L(M’) 1.状态的等价性： 假设s和t为M的两个状态(终态和非终态) ①若分别从状态s和状态t出发都能读出某个字α而停止于终态，则称s和t等价 ②存在一个字α，使得s和t一个读出α停止于终态，另一个读出α停止于非终态，则称s和t可区别 2.基本思想： ①把M的状态集分为一些不相交的子集，使任何两个不同子集状态是可区别的，而同一子集的任何两个状态是等价的 ②让每个子集选出一个代表，同时消去其他状态 3.划分 ①把S划分为终态和非终态两个子集，形成基本划分∏ ②假定某个时候∏已含m个子集，记为∏&#x3D;{I(1),I(2),…,I(m)},检查∏中的每个子集能否进一步划分： (a)假定s1和s2是I(i)&#x3D;{s1,s2,…sk}中的两个状态，它们经过a弧分别到达t1和t2,而t1和t2属于现行∏中的两个不同子集，则s1和s2不等价 (b)一般地，对于某个I(i),若Ia(i)落于现行∏中N个不同的子集，则应把I(i)划分成N个不相交的组 例： 正则表达式语言 L &#x3D; { a } { a , b } ∗ ( { ϵ } ∪ ( { . , _ } { a , b } { a , b } ∗ ) ) L&#x3D;{a}{a,b}^*({\\epsilon } \\cup ({.,_}{a,b}{a,b}^*))L&#x3D;{a}{a,b}∗({ϵ}∪({.,_}{a,b}{a,b}∗)) 这个语言是指，由a开头，后接任意长度的a、b串，然后再接空串（代表结束）。或者是接以.或_开头的，后接长度大于等于1的a、b串。 正则表达式（Regular Expression, RE）是一种用来描述正则语言的更紧凑的表示方法。 以上面的语言举例，写成正则表达式则可表示成：r &#x3D; a ( a ∣ b ) ∗ ( ϵ ∣ ( . ∣ ) ( a ∣ b ) ( a ∣ b ) ∗ ) r&#x3D;a(a|b)^*(\\epsilon | (.|_)(a|b)(a|b)^*)r&#x3D;a(a∣b)∗(ϵ∣(.∣)(a∣b)(a∣b)∗) 正则表达式可以由较小的正则表达式按照特定规则递归地构建。每个正则表达式r定义一个语言。记为L(r)。这个语言也是根据r的子表达式所表示的语言递归定义的。 定义 如果ϵ \\epsilonϵ是一个RE，L ( ϵ ) &#x3D; { ϵ } L(\\epsilon) &#x3D; {\\epsilon}L(ϵ)&#x3D;{ϵ} 如果α ∈ ∑ \\alpha \\in \\sumα∈∑，则α \\alphaα是一个RE,L ( α ) &#x3D; { α } L(\\alpha)&#x3D;{\\alpha}L(α)&#x3D;{α} 假设 1r 和 1s 都是RE，表示的语言分别是 1L(r) 和 1L(s) ，则 r ∣ s r|sr∣s是一个RE，L ( r ∣ s ) &#x3D; L ( r ) ∪ L ( s ) L(r|s) &#x3D; L(r) \\cup L(s)L(r∣s)&#x3D;L(r)∪L(s) rs（r连接s）是一个RE，L ( r ∣ s ) &#x3D; L ( r ) L ( s ) L(r|s) &#x3D; L(r)L(s)L(r∣s)&#x3D;L(r)L(s) r ∗ r^r∗是一个RE，L ( r ∗ ) &#x3D; ( L ( r ) ) ∗ L(r^) &#x3D; (L(r))^*L(r∗)&#x3D;(L(r))∗ ( r ) (r)(r)是一个RE，L ( ( r ) ) &#x3D; L ( r ) L((r)) &#x3D; L(r)L((r))&#x3D;L(r) 注：运算的优先级：*、连接、| 例：∑ &#x3D; { a , b } \\sum &#x3D; {a,b}∑&#x3D;{a,b}，则 L ( a ∣ b ) &#x3D; L ( a ) ∪ L ( b ) &#x3D; { a } ∪ { b } &#x3D; { a , b } L(a|b) &#x3D; L(a) \\cup L(b) &#x3D; {a} \\cup {b} &#x3D; {a,b}L(a∣b)&#x3D;L(a)∪L(b)&#x3D;{a}∪{b}&#x3D;{a,b} L ( ( a ∣ b ) ( a ∣ b ) ) &#x3D; L ( a ∣ b ) L ( a ∣ b ) &#x3D; { a , b } { a , b } &#x3D; { a a , a b , b a , b b } L((a|b)(a|b)) &#x3D; L(a|b)L(a|b) &#x3D; {a,b}{a,b} &#x3D; {aa,ab,ba,bb}L((a∣b)(a∣b))&#x3D;L(a∣b)L(a∣b)&#x3D;{a,b}{a,b}&#x3D;{aa,ab,ba,bb} L ( a ∗ ) &#x3D; ( L ( a ) ) ∗ &#x3D; { a } ∗ &#x3D; { ϵ , a , a a , a a a , . . . } L(a^*) &#x3D; (L(a))^* &#x3D; {a}^* &#x3D; {\\epsilon,a,aa,aaa,…}L(a∗)&#x3D;(L(a))∗&#x3D;{a}∗&#x3D;{ϵ,a,aa,aaa,…} L ( ( a ∣ b ) ∗ ) &#x3D; ( L ( a ∣ b ) ) ∗ &#x3D; a , b ∗ &#x3D; { ϵ , a , b , a a , a b , b a , b b , . . . } L((a|b)^*) &#x3D; (L(a|b))^* &#x3D; {a,b}^* &#x3D; {\\epsilon,a,b,aa,ab,ba,bb,…}L((a∣b)∗)&#x3D;(L(a∣b))∗&#x3D;a,b∗&#x3D;{ϵ,a,b,aa,ab,ba,bb,…} L ( a ∣ a ∗ b ) &#x3D; L ( a ) ∪ L ( a ∗ b ) &#x3D; L ( a ) ∪ L ( a ∗ ) L ( b ) &#x3D; { a , b , a b , a a b , a a a b , . . . } L(a|a^b) &#x3D; L(a) \\cup L(a^b) &#x3D; L(a) \\cup L(a^*)L(b) &#x3D; {a,b,ab,aab,aaab,…}L(a∣a∗b)&#x3D;L(a)∪L(a∗b)&#x3D;L(a)∪L(a∗)L(b)&#x3D;{a,b,ab,aab,aaab,…} 注：可以用RE定义的语言叫做正则语言(regular language)或正则集合(regular set)。 RE的代数定理 定律 描述 r | s &#x3D; s | r | 是可以交换的 r | (s | t) &#x3D; (r | s) | t | 是可以结合的 r ( s t ) &#x3D; ( r s ) t 连接是可以结合的 r (s | t) &#x3D; rs | rt 连接对 | 是可分配的 ϵ r &#x3D; r ϵ &#x3D; r \\epsilon r &#x3D; r \\epsilon &#x3D; rϵr&#x3D;rϵ&#x3D;r ϵ \\epsilonϵ是连接的单位元 $r^* &#x3D; (r \\epsilon)^*$ r ∗ ∗ &#x3D; r ∗ r^{**} &#x3D; r^*r∗∗&#x3D;r∗ *具有幂等性 正则文法与正则表达式等价 对任何正则文法G，存在定义同一语言的正则表达式r 对任何正则表达式 1r ，存在生成同一语言的正则文法 Thompson算法 : RE –&gt;NFA基础规则 \\1. 对于 ε，构造为 \\2. 对于a （输入的字符以a为例），构造为 一个圆圈前加一个箭头表示初始状态，两个圆圈表示终结状态，中间用箭头连接，箭头上标明要输入的字符 归纳规则 \\1. 对于 a | b，构造为 \\2. 对于 ab，构造为 \\3. 对于 a* &#x3D; a^n | ε，构造为 1.增加一个新的开始状态和一个新的结束状态。 2.将新的开始状态指向原来所有的开始状态。原来的终结状态指向新的终结状态。弧上为 ε 。 例题 设正则表达式 10(0|1), 构造等价的NFA ​ ​ ​ ​","categories":[],"tags":[]},{"title":"","slug":"Cache和主存的映射·替换算法·写策略","date":"2023-08-09T03:00:38.950Z","updated":"2023-08-07T08:49:41.000Z","comments":true,"path":"2023/08/09/Cache和主存的映射·替换算法·写策略/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/Cache%E5%92%8C%E4%B8%BB%E5%AD%98%E7%9A%84%E6%98%A0%E5%B0%84%C2%B7%E6%9B%BF%E6%8D%A2%E7%AE%97%E6%B3%95%C2%B7%E5%86%99%E7%AD%96%E7%95%A5/","excerpt":"","text":"Cache和主存的映射·替换算法·写策略映射Cache行中的信息是主存某个块的副本地址映射：将主存地址空间映射到Cache地址空间，按某种规则将主存的信息装入CacheCache的行数比主存块少的多，只有一部分主存块的信息可以装入Cache中，因此在Cache中需要为每块加一个标记，指明它是主存的哪一块的副本，此标记相当于主存块的编号。为了说明Cache行中的信息是否有效还需要一个有效位，地址映射以下3种： 直接映射 全相联映射 组相联映射 直接映射Cache 行号 &#x3D; 主存块号 mod Cache 总行数假设 Cache : $2^c$行 主存 : $2^m$块$0,2^c,2^{c+1},\\dots$块映射到第0行$1,2^c+1,2^{c+1}+1$块映射到第1行低c位为装入的Cache行号，给每行设置一个长为t&#x3D;m-c的标记，当某个主存块调入Cache后，将主存块号的高t位设置为对应Cache行中的标记 t c b 标记 Cache行号 块内地址 CPU访存过程： 根据访存地址中间的c位找到对应Cache行 Cache行标记与主存地址高t位标记比较，确定是否有效 相等且有效位为1，访问Cache命中，根据主存地址低b位的块内地址，在对应Cache行中存取信息 不相等或有效位为0，不命中，CPU将主存读出该地址所在的一块信息送到相应的Cache行中，置有效位1，并将标记设置为地址高t位，同时将地址中的内容送CPU 全相联映射主存的每一块可以装入Cache中的任何位置，每行标记用于指出该行取自主存的哪一块，CPU访存时需要与所有Cache行的标记进行对比 t b 标记 块内地址 组相联映射将Cache分为Q个大小相等的组，每个主存块可以装入固定组的任意一行，即组间采用直接映射，而组内采用全相联映射，Q&#x3D;1变为全相联映射，Q&#x3D;Cache行数变为直接映射假设r个Cache行，则称之为r路组相联映射Cache组号&#x3D;主存块号 mod Cache组数(Q) t g b 标记 组号 块内地址 CPU访存： 根据访存地址中间的g位组号找到对应的Cache组 将对应的Cache组的每个行标记与主存的高t位标记进行比较 相等且有效位为1，命中，根据主存地址中的低b位块内地址，在对应的Cache行中存取信息 不相等或有效位为0，未命中，CPU从主存读出该地址所在的一块信息送到对应的Cache组的任意一个空行，有效位置1，设置标记，同时将地址中的内容送CPU 替换算法在采用全相联映射或组相联映射方式时，主存向Cache传送一个新块，Cache或Cache组空间已满替换的策略 RAND FIFO LRU 写策略因为Cache中的内容是主存块副本，当对Cache中的内容进行更新时，就需选用写操作策略，使Cache内容和主存保持一致对于Cache写命中： 全写法：当CPU对Cache写命中时，必须八数据同时写入Cache和内存。当某一块需要替换时，不必把这一块写回主存，用新调入的块直接覆盖。 回写法：当CPU对Cache写命中时，只把数据写入Cache，而不立即写入主存，只有当此块被换出时，才写入主存。每个Cache行设置一个修改位（脏位），若修改位为1，Cache行被修改过，替换时需要写回主存，若为0，则说明Cache行中的块未被命中，替换时无需写回 对于Cache写不命中： 写分配法：加载主存中的块到Cache中，然后更新这个Cache块 非写分配法：只写入主存，不进行调块","categories":[],"tags":[]},{"title":"","slug":"2023-03-24-docker-jekyll-make","date":"2023-08-09T03:00:38.850Z","updated":"2023-08-07T08:49:41.000Z","comments":true,"path":"2023/08/09/2023-03-24-docker-jekyll-make/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2023-03-24-docker-jekyll-make/","excerpt":"","text":"I always faced issues when running jekyll locally, my blog uses jekyll to generate static pages, but every time my os is upgraded or I change my laptop, I always get into trouble of installing&#x2F;managing ruby version, bundles etc. To my rescue comes the docker, I have created a simple a docker file and script to serve my jekyll pages locally, and it now works like a charm. I always have been admirer of docker, I tend to use docker more often if have to install any dev app, databases etc or package but still want to keep my machine clean. In this case as well I wanted a consistent mechanism to run my blogs locally. DockerfileThe first thing I do is create a simple dockerfile in my Jekyll app. I copy my Gemfile and Gemfile.lock to install the bundles. 123456789FROM jekyll/jekyll:3.8COPY Gemfile* ./RUN gem install bundler:2.2.24 &amp;&amp; bundle installENTRYPOINT [ &quot;jekyll&quot; ]CMD [ &quot;build&quot; ] The reason I copied Gemfile is because sometimes the bundle version is different, so now I install the required bundler (in my case its 2.2.24), and install all the dependencies. Execution ScriptNext I created a simple script to build and serve my code, so that i don’t keep on typing the commands every time.local_serve.sh 1234567891011121314151617181920212223242526272829303132#!/usr/bin/env bashset -ecase $1 in &#x27;prod-build&#x27;) echo &quot;Building docker image of Jekyll&quot; docker build -t jekyll . echo &quot;Building Jekyll site&quot; docker run \\ --volume=&quot;$PWD:/srv/jekyll:Z&quot; \\ -e JEKYLL_ENV=prod \\ -t jekyll \\ build ;; &#x27;build&#x27;) echo &quot;Building docker image of Jekyll&quot; docker build -t jekyll . echo &quot;Building Jekyll site&quot; docker run \\ --volume=&quot;$PWD:/srv/jekyll:Z&quot; \\ -t jekyll ;; &#x27;serve&#x27;) docker run \\ --volume=&quot;$PWD:/srv/jekyll:Z&quot; \\ -p 4000:4000 \\ -it jekyll serve ;;esac In the build command, I am creating the jekyll image. I can mapping the current directory to &#x2F;srv&#x2F;jekyll and running my jekyll container to build the site. In the serve command, I using the same directory to serve my pages at port 4000 and using interactive mode to explicitly invoke serve command. Run the Jekyll containerBefore we proceed do not forget to give executable permission to local_service.sh 1234567$ chmod a+x local_serve.shSo, first thing to do is to build our Jekyll app so that all the dependencies are downloaded. We will just run$ ./local_serve.sh buildOnce the dependencies are downloaded, now lets start our Jekyll server$ ./local_serve.sh serve And we can see the server started 123456789101112ruby 2.6.3p62 (2019-04-16 revision 67580) [x86_64-linux-musl]Configuration file: /srv/jekyll/_config.yml Source: /srv/jekyll Destination: /srv/jekyll/_site Incremental build: disabled. Enable with --incremental Generating... Jekyll Feed: Generating feed for posts GitHub Metadata: No GitHub API authentication could be found. Some fields may be missing or have incorrect data. done in 17.778 seconds. Auto-regeneration: enabled for &#x27;/srv/jekyll&#x27; Server address: http://0.0.0.0:4000/ Server running... press ctrl-c to stop. Enjoy running Jekyll locally without installing ruby or jekyll directly on your system.","categories":[],"tags":[]},{"title":"","slug":"2023-01-11-torch-repeat","date":"2023-08-09T03:00:38.760Z","updated":"2023-08-07T08:49:40.000Z","comments":true,"path":"2023/08/09/2023-01-11-torch-repeat/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2023-01-11-torch-repeat/","excerpt":"","text":"1import torch 1x, y = torch.arange(12), torch.arange(24) 1x, y (tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]), tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])) 1x.shape, y.shape (torch.Size([12]), torch.Size([24])) 1x.numel(), y.numel() (12, 24) 1x.reshape(3, 4), y.reshape(6, 4) (tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]), tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]])) 1zeros, ones, randn = torch.zeros(2, 3), torch.ones(2, 3), torch.randn(2, 3) 1zeros, ones, randn (tensor([[0., 0., 0.], [0., 0., 0.]]), tensor([[1., 1., 1.], [1., 1., 1.]]), tensor([[-0.7930, 0.2799, 0.7478], [-1.2043, 0.7893, 2.0885]])) 1tensorX = torch.tensor([[1, 2, 3], [4, 5, 6]]) 1tensorX tensor([[1, 2, 3], [4, 5, 6]]) 1 1 1 1X, Y = torch.tensor([1.0, 2, 4, 8]), torch.tensor([2, 2, 2, 2]) 1X + Y, X - Y, X * Y, X / Y, X ** Y (tensor([ 3., 4., 6., 10.]), tensor([-1., 0., 2., 6.]), tensor([ 2., 4., 8., 16.]), tensor([0.5000, 1.0000, 2.0000, 4.0000]), tensor([ 1., 4., 16., 64.])) 1torch.exp(X) tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03]) 1a, b =torch.arange(12, dtype = torch.float64).reshape(3, 4), torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) 1torch.cat((a, b), dim = 0), torch.cat((a, b), dim = 1) (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 2., 1., 4., 3.], [ 1., 2., 3., 4.], [ 4., 3., 2., 1.]], dtype=torch.float64), tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.], [ 4., 5., 6., 7., 1., 2., 3., 4.], [ 8., 9., 10., 11., 4., 3., 2., 1.]], dtype=torch.float64)) 1a == b tensor([[False, True, False, True], [False, False, False, False], [False, False, False, False]]) 1a.sum(), b.sum() (tensor(66., dtype=torch.float64), tensor(30.)) 1p, q = torch.arange(3).reshape((3, 1)), torch.arange(2).reshape((1, 2)) 1p, q (tensor([[0], [1], [2]]), tensor([[0, 1]])) 1q + p tensor([[0, 1], [1, 2], [2, 3]]) 12arr = torch.randn(3,4)arr tensor([[-0.2175, 1.2975, -1.4377, -0.6559], [-0.4125, 0.2430, 0.6188, 0.8181], [-0.7755, 0.2852, 0.8682, 0.5547]]) 1arr[:] tensor([[-0.2175, 1.2975, -1.4377, -0.6559], [-0.4125, 0.2430, 0.6188, 0.8181], [-0.7755, 0.2852, 0.8682, 0.5547]]) 1arr[1:3] tensor([[-0.4125, 0.2430, 0.6188, 0.8181], [-0.7755, 0.2852, 0.8682, 0.5547]]) 1arr[-1] tensor([-0.7755, 0.2852, 0.8682, 0.5547]) 1arr[1, 2] tensor(0.6188) 1arr[1, 2] = 0.0 1arr[:] tensor([[-0.2175, 1.2975, -1.4377, -0.6559], [-0.4125, 0.2430, 0.0000, 0.8181], [-0.7755, 0.2852, 0.8682, 0.5547]]) 1arr[0:2, :] tensor([[-0.2175, 1.2975, -1.4377, -0.6559], [-0.4125, 0.2430, 0.0000, 0.8181]]) 1arr[0:2, :] = 12. 1arr[:] tensor([[12.0000, 12.0000, 12.0000, 12.0000], [12.0000, 12.0000, 12.0000, 12.0000], [-0.7755, 0.2852, 0.8682, 0.5547]]) 12before = id(arr)brr = torch.randn(3,4) 1arr = arr + brr 1id(arr) == before False 12Z = torch.zeros_like(arr)print(id(Z)) 140595909928264 12Z[:] = arr + brrprint(id(Z)) 140595909928264 123before = id(arr)arr += brrid(arr) == before True 123A = arr.numpy()B = torch.tensor(A)type(A), type(B) (numpy.ndarray, torch.Tensor) 12a = torch.tensor([3.5])a, a.item(), float(a), int(a) (tensor([3.5000]), 3.5, 3.5, 3) 1 1 1 1 1 1x = torch.arange(4.0) 1x tensor([0., 1., 2., 3.]) 12x.requires_grad_(True)x.grad 12y = 2 * torch.dot(x, x)y tensor(28., grad_fn=&lt;MulBackward0&gt;) 12y.backward()x.grad tensor([ 0., 4., 8., 12.]) 1x.grad == 4 * x tensor([True, True, True, True]) 1x.grad.zero_() tensor([0., 0., 0., 0.]) 1y = x.sum() 1y.backward() 1x.grad tensor([1., 1., 1., 1.]) 1 1 1 1 12x, y = torch.tensor([3.0]), torch.tensor([2.0])x + y, x - y, x * y, x / y, x ** y (tensor([5.]), tensor([1.]), tensor([6.]), tensor([1.5000]), tensor([9.])) 12x = torch.arange(4)x tensor([0, 1, 2, 3]) 1x[3] tensor(3) 1len(x) 4 1x.shape torch.Size([4]) 1A = torch.arange(20).reshape(5,4) 1A tensor([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]) 1A.T tensor([[ 0, 4, 8, 12, 16], [ 1, 5, 9, 13, 17], [ 2, 6, 10, 14, 18], [ 3, 7, 11, 15, 19]]) 12B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])B, B.T (tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]]), tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])) 1B == B.T tensor([[True, True, True], [True, True, True], [True, True, True]]) 1X = torch.arange(24).reshape(2, 3, 4) 1X tensor([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) 12A = torch.arange(20, dtype = torch.float64).reshape(5, 4)B = A.clone() 1A, A + B (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=torch.float64), tensor([[ 0., 2., 4., 6.], [ 8., 10., 12., 14.], [16., 18., 20., 22.], [24., 26., 28., 30.], [32., 34., 36., 38.]], dtype=torch.float64)) 1A * B #对应乘积 tensor([[ 0., 1., 4., 9.], [ 16., 25., 36., 49.], [ 64., 81., 100., 121.], [144., 169., 196., 225.], [256., 289., 324., 361.]], dtype=torch.float64) 12x = torch.arange(4, dtype = torch.float64)x, x.sum() (tensor([0., 1., 2., 3.], dtype=torch.float64), tensor(6., dtype=torch.float64)) 1A.shape, A.sum() (torch.Size([5, 4]), tensor(190., dtype=torch.float64)) 1","categories":[],"tags":[]},{"title":"","slug":"2023-01-10-wsl2-Ubuntu1804-install-CUDA-Pytorch","date":"2023-08-09T03:00:38.712Z","updated":"2023-08-07T08:49:40.000Z","comments":true,"path":"2023/08/09/2023-01-10-wsl2-Ubuntu1804-install-CUDA-Pytorch/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2023-01-10-wsl2-Ubuntu1804-install-CUDA-Pytorch/","excerpt":"","text":"Init Ubuntu and change deb&amp;pip source12wget https://github.com/ebxeax/ebxeax.github.io/blob/main/toolbox/initUbuntu/initUbuntu.shbash ./initUbuntu.sh CUDA11.61234567wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pinsudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600wget https://developer.download.nvidia.com/compute/cuda/11.6.2/local_installers/cuda-repo-wsl-ubuntu-11-6-local_11.6.2-1_amd64.debsudo dpkg -i cuda-repo-wsl-ubuntu-11-6-local_11.6.2-1_amd64.debsudo apt-key add /var/cuda-repo-wsl-ubuntu-11-6-local/7fa2af80.pubsudo apt-get updatesudo apt-get -y install cuda Load library path1234gedit ~/.bashrcexport PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATsource ~/.bashrc Test nvidia-smi1nvidia-smi Test nvcc -V1nvcc -V Pytorch1pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113 get file when network worse12345wget https://download.pytorch.org/whl/cu113/torch-1.10.2%2Bcu113-cp36-cp36m-linux_x86_64.whlwget https://download.pytorch.org/whl/cu113/torchvision-0.11.3%2Bcu113-cp36-cp36m-linux_x86_64.whlwget pip3 install ./torch-1.10.2+cu113-cp36-cp36m-linux_x86_64.whlpip3 install ./torchvision-0.11.3+cu113-cp36-cp36m-linux_x86_64.whl","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-yolov1","date":"2023-08-09T03:00:38.665Z","updated":"2023-08-07T08:49:40.000Z","comments":true,"path":"2023/08/09/2022-06-15-yolov1/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-yolov1/","excerpt":"","text":"前言当我们谈起计算机视觉时，首先想到的就是图像分类，没错，图像分类是计算机视觉最基本的任务之一，但是在图像分类的基础上，还有更复杂和有意思的任务，如目标检测，物体定位，图像分割等，见图1所示。其中目标检测是一件比较实际的且具有挑战性的计算机视觉任务，其可以看成图像分类与定位的结合，给定一张图片，目标检测系统要能够识别出图片的目标并给出其位置，由于图片中目标数是不定的，且要给出目标的精确位置，目标检测相比分类任务更复杂。目标检测的一个实际应用场景就是无人驾驶，如果能够在无人车上装载一个有效的目标检测系统，那么无人车将和人一样有了眼睛，可以快速地检测出前面的行人与车辆，从而作出实时决策。 图1 计算机视觉任务（来源: cs231n） 近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。这可以在图2中看到。本文介绍的是Yolo算法，其全称是You Only Look Once: Unified, Real-Time Object Detection，其实个人觉得这个题目取得非常好，基本上把Yolo算法的特点概括全了：You Only Look Once说的是只需要一次CNN运算，Unified指的是这是一个统一的框架，提供end-to-end的预测，而Real-Time体现是Yolo算法速度快。这里我们谈的是Yolo-v1版本算法，其性能是差于后来的SSD算法的，但是Yolo后来也继续进行改进，产生了Yolo9000算法。本文主要讲述Yolo-v1算法的原理，特别是算法的训练与预测中详细细节，最后将给出如何使用TensorFlow实现Yolo算法。 图2 目标检测算法进展与对比 滑动窗口与CNN在介绍Yolo算法之前，首先先介绍一下滑动窗口技术，这对我们理解Yolo算法是有帮助的。采用滑动窗口的目标检测算法思路非常简单，它将检测问题转化为了图像分类问题。其基本原理就是采用不同大小和比例（宽高比）的窗口在整张图片上以一定的步长进行滑动，然后对这些窗口对应的区域做图像分类，这样就可以实现对整张图片的检测了，如下图3所示，如DPM就是采用这种思路。但是这个方法有致命的缺点，就是你并不知道要检测的目标大小是什么规模，所以你要设置不同大小和比例的窗口去滑动，而且还要选取合适的步长。但是这样会产生很多的子区域，并且都要经过分类器去做预测，这需要很大的计算量，所以你的分类器不能太复杂，因为要保证速度。解决思路之一就是减少要分类的子区域，这就是R-CNN的一个改进策略，其采用了selective search方法来找到最有可能包含目标的子区域（Region Proposal），其实可以看成采用启发式方法过滤掉很多子区域，这会提升效率。 图3 采用滑动窗口进行目标检测（来源：deeplearning.ai） 如果你使用的是CNN分类器，那么滑动窗口是非常耗时的。但是结合卷积运算的特点，我们可以使用CNN实现更高效的滑动窗口方法。这里要介绍的是一种全卷积的方法，简单来说就是网络中用卷积层代替了全连接层，如图4所示。输入图片大小是16x16，经过一系列卷积操作，提取了2x2的特征图，但是这个2x2的图上每个元素都是和原图是一一对应的，如图上蓝色的格子对应蓝色的区域，这不就是相当于在原图上做大小为14x14的窗口滑动，且步长为2，共产生4个字区域。最终输出的通道数为4，可以看成4个类别的预测概率值，这样一次CNN计算就可以实现窗口滑动的所有子区域的分类预测。这其实是overfeat算法的思路。之所可以CNN可以实现这样的效果是因为卷积操作的特性，就是图片的空间位置信息的不变性，尽管卷积过程中图片大小减少，但是位置对应关系还是保存的。说点题外话，这个思路也被R-CNN借鉴，从而诞生了Fast R-cNN算法。 图4 滑动窗口的CNN实现（来源：deeplearning.ai） 上面尽管可以减少滑动窗口的计算量，但是只是针对一个固定大小与步长的窗口，这是远远不够的。Yolo算法很好的解决了这个问题，它不再是窗口滑动了，而是直接将原始图片分割成互不重合的小方块，然后通过卷积最后生产这样大小的特征图，基于上面的分析，可以认为特征图的每个元素也是对应原始图片的一个小方块，然后用每个元素来可以预测那些中心点在该小方格内的目标，这就是Yolo算法的朴素思想。下面将详细介绍Yolo算法的设计理念。 设计理念整体来看，Yolo算法采用一个单独的CNN模型实现end-to-end的目标检测，整个系统如图5所示：首先将输入图片resize到448x448，然后送入CNN网络，最后处理网络预测结果得到检测的目标。相比R-CNN算法，其是一个统一的框架，其速度更快，而且Yolo的训练过程也是end-to-end的。 图5 Yolo检测系统 具体来说，Yolo的CNN网络将输入的图片分割成 网格，然后每个单元格负责去检测那些中心点落在该格子内的目标，如图6所示，可以看到狗这个目标的中心落在左下角一个单元格内，那么该单元格负责预测这个狗。每个单元格会预测 个边界框（bounding box）以及边界框的置信度（confidence score）。所谓置信度其实包含两个方面，一是这个边界框含有目标的可能性大小，二是这个边界框的准确度。前者记为 ，当该边界框是背景时（即不包含目标），此时 。而当该边界框包含目标时， 。边界框的准确度可以用预测框与实际框（ground truth）的IOU（intersection over union，交并比）来表征，记为 。因此置信度可以定义为 。很多人可能将Yolo的置信度看成边界框是否含有目标的概率，但是其实它是两个因子的乘积，预测框的准确度也反映在里面。边界框的大小与位置可以用4个值来表征： ，其中 是边界框的中心坐标，而 和 是边界框的宽与高。还有一点要注意，中心坐标的预测值 是相对于每个单元格左上角坐标点的偏移值，并且单位是相对于单元格大小的，单元格的坐标定义如图6所示。而边界框的 和 预测值是相对于整个图片的宽与高的比例，这样理论上4个元素的大小应该在 范围。这样，每个边界框的预测值实际上包含5个元素： ，其中前4个表征边界框的大小与位置，而最后一个值是置信度。 图6 网格划分 还有分类问题，对于每一个单元格其还要给出预测出 个类别概率值，其表征的是由该单元格负责预测的边界框其目标属于各个类别的概率。但是这些概率值其实是在各个边界框置信度下的条件概率，即 。值得注意的是，不管一个单元格预测多少个边界框，其只预测一组类别概率值，这是Yolo算法的一个缺点，在后来的改进版本中，Yolo9000是把类别概率预测值与边界框是绑定在一起的。同时，我们可以计算出各个边界框类别置信度（class-specific confidence scores）: 。 边界框类别置信度表征的是该边界框中目标属于各个类别的可能性大小以及边界框匹配目标的好坏。后面会说，一般会根据类别置信度来过滤网络的预测框。 总结一下，每个单元格需要预测 个值。如果将输入图片划分为 网格，那么最终预测值为 大小的张量。整个模型的预测值结构如下图所示。对于PASCAL VOC数据，其共有20个类别，如果使用 ，那么最终的预测结果就是 大小的张量。在下面的网络结构中我们会详细讲述每个单元格的预测值的分布位置。 图7 模型预测值结构 网络设计Yolo采用卷积网络来提取特征，然后使用全连接层来得到预测值。网络结构参考GooLeNet模型，包含24个卷积层和2个全连接层，如图8所示。对于卷积层，主要使用1x1卷积来做channle reduction，然后紧跟3x3卷积。对于卷积层和全连接层，采用Leaky ReLU激活函数： 。但是最后一层却采用线性激活函数。 图8 网络结构 可以看到网络的最后输出为 大小的张量。这和前面的讨论是一致的。这个张量所代表的具体含义如图9所示。对于每一个单元格，前20个元素是类别概率值，然后2个元素是边界框置信度，两者相乘可以得到类别置信度，最后8个元素是边界框的 。大家可能会感到奇怪，对于边界框为什么把置信度 和 都分开排列，而不是按照 这样排列，其实纯粹是为了计算方便，因为实际上这30个元素都是对应一个单元格，其排列是可以任意的。但是分离排布，可以方便地提取每一个部分。这里来解释一下，首先网络的预测值是一个二维张量 ，其shape为 。采用切片，那么 就是类别概率部分，而 是置信度部分，最后剩余部分 是边界框的预测结果。这样，提取每个部分是非常方便的，这会方面后面的训练及预测时的计算。 图9 预测张量的解析 网络训练在训练之前，先在ImageNet上进行了预训练，其预训练的分类模型采用图8中前20个卷积层，然后添加一个average-pool层和全连接层。预训练之后，在预训练得到的20层卷积层之上加上随机初始化的4个卷积层和2个全连接层。由于检测任务一般需要更高清的图片，所以将网络的输入从224x224增加到了448x448。整个网络的流程如下图所示： 图10 Yolo网络流程 下面是训练损失函数的分析，Yolo算法将目标检测看成回归问题，所以采用的是均方差损失函数。但是对不同的部分采用了不同的权重值。首先区分定位误差和分类误差。对于定位误差，即边界框坐标预测误差，采用较大的权重 。然后其区分不包含目标的边界框与含有目标的边界框的置信度，对于前者，采用较小的权重值 。其它权重值均设为1。然后采用均方误差，其同等对待大小不同的边界框，但是实际上较小的边界框的坐标误差应该要比较大的边界框要更敏感。为了保证这一点，将网络的边界框的宽与高预测改为对其平方根的预测，即预测值变为 。 另外一点时，由于每个单元格预测多个边界框。但是其对应类别只有一个。那么在训练时，如果该单元格内确实存在目标，那么只选择与ground truth的IOU最大的那个边界框来负责预测该目标，而其它边界框认为不存在目标。这样设置的一个结果将会使一个单元格对应的边界框更加专业化，其可以分别适用不同大小，不同高宽比的目标，从而提升模型性能。大家可能会想如果一个单元格内存在多个目标怎么办，其实这时候Yolo算法就只能选择其中一个来训练，这也是Yolo算法的缺点之一。要注意的一点时，对于不存在对应目标的边界框，其误差项就是只有置信度，坐标项误差是没法计算的。而只有当一个单元格内确实存在目标时，才计算分类误差项，否则该项也是无法计算的。 综上讨论，最终的损失函数计算如下： 其中第一项是边界框中心坐标的误差项， 指的是第 个单元格存在目标，且该单元格中的第 个边界框负责预测该目标。第二项是边界框的高与宽的误差项。第三项是包含目标的边界框的置信度误差项。第四项是不包含目标的边界框的置信度误差项。而最后一项是包含目标的单元格的分类误差项， 指的是第 个单元格存在目标。这里特别说一下置信度的target值 ，如果是不存在目标，此时由于 ，那么 。如果存在目标， ，此时需要确定 ，当然你希望最好的话，可以将IOU取1，这样 ，但是在YOLO实现中，使用了一个控制参数rescore（默认为1），当其为1时，IOU不是设置为1，而就是计算truth和pred之间的真实IOU。不过很多复现YOLO的项目还是取 ，这个差异应该不会太影响结果吧。 网络预测在说明Yolo算法的预测过程之前，这里先介绍一下非极大值抑制算法（non maximum suppression, NMS），这个算法不单单是针对Yolo算法的，而是所有的检测算法中都会用到。NMS算法主要解决的是一个目标被多次检测的问题，如图11中人脸检测，可以看到人脸被多次检测，但是其实我们希望最后仅仅输出其中一个最好的预测框，比如对于美女，只想要红色那个检测结果。那么可以采用NMS算法来实现这样的效果：首先从所有的检测框中找到置信度最大的那个框，然后挨个计算其与剩余框的IOU，如果其值大于一定阈值（重合度过高），那么就将该框剔除；然后对剩余的检测框重复上述过程，直到处理完所有的检测框。Yolo预测过程也需要用到NMS算法。 图11 NMS应用在人脸检测 下面就来分析Yolo的预测过程，这里我们不考虑batch，认为只是预测一张输入图片。根据前面的分析，最终的网络输出是 ，但是我们可以将其分割成三个部分：类别概率部分为 ，置信度部分为 ，而边界框部分为 （对于这部分不要忘记根据原始图片计算出其真实值）。然后将前两项相乘（矩阵 乘以 可以各补一个维度来完成 ）可以得到类别置信度值为 ，这里总共预测了 个边界框。 所有的准备数据已经得到了，那么我们先说第一种策略来得到检测框的结果，我认为这是最正常与自然的处理。首先，对于每个预测框根据类别置信度选取置信度最大的那个类别作为其预测标签，经过这层处理我们得到各个预测框的预测类别及对应的置信度值，其大小都是 。一般情况下，会设置置信度阈值，就是将置信度小于该阈值的box过滤掉，所以经过这层处理，剩余的是置信度比较高的预测框。最后再对这些预测框使用NMS算法，最后留下来的就是检测结果。一个值得注意的点是NMS是对所有预测框一视同仁，还是区分每个类别，分别使用NMS。Ng在deeplearning.ai中讲应该区分每个类别分别使用NMS，但是看了很多实现，其实还是同等对待所有的框，我觉得可能是不同类别的目标出现在相同位置这种概率很低吧。 上面的预测方法应该非常简单明了，但是对于Yolo算法，其却采用了另外一个不同的处理思路（至少从C源码看是这样的），其区别就是先使用NMS，然后再确定各个box的类别。其基本过程如图12所示。对于98个boxes，首先将小于置信度阈值的值归0，然后分类别地对置信度值采用NMS，这里NMS处理结果不是剔除，而是将其置信度值归为0。最后才是确定各个box的类别，当其置信度值不为0时才做出检测结果输出。这个策略不是很直接，但是貌似Yolo源码就是这样做的。Yolo论文里面说NMS算法对Yolo的性能是影响很大的，所以可能这种策略对Yolo更好。但是我测试了普通的图片检测，两种策略结果是一样的。 图12 Yolo的预测处理流程 算法性能分析这里看一下Yolo算法在PASCAL VOC 2007数据集上的性能，这里Yolo与其它检测算法做了对比，包括DPM，R-CNN，Fast R-CNN以及Faster R-CNN。其对比结果如表1所示。与实时性检测方法DPM对比，可以看到Yolo算法可以在较高的mAP上达到较快的检测速度，其中Fast Yolo算法比快速DPM还快，而且mAP是远高于DPM。但是相比Faster R-CNN，Yolo的mAP稍低，但是速度更快。所以。Yolo算法算是在速度与准确度上做了折中。 表1 Yolo在PASCAL VOC 2007上与其他算法的对比 为了进一步分析Yolo算法，文章还做了误差分析，将预测结果按照分类与定位准确性分成以下5类： Correct：类别正确，IOU&gt;0.5；（准确度） Localization：类别正确，0.1 &lt; IOU&lt;0.5（定位不准）； Similar：类别相似，IOU&gt;0.1； Other：类别错误，IOU&gt;0.1； Background：对任何目标其IOU&lt;0.1。（误把背景当物体） Yolo与Fast R-CNN的误差对比分析如下图所示： 图13 Yolo与Fast R-CNN的误差对比分析 可以看到，Yolo的Correct的是低于Fast R-CNN。另外Yolo的Localization误差偏高，即定位不是很准确。但是Yolo的Background误差很低，说明其对背景的误判率较低。Yolo的那篇文章中还有更多性能对比，感兴趣可以看看。 现在来总结一下Yolo的优缺点。首先是优点，Yolo采用一个CNN网络来实现检测，是单管道策略，其训练与预测都是end-to-end，所以Yolo算法比较简洁且速度快。第二点由于Yolo是对整张图片做卷积，所以其在检测目标有更大的视野，它不容易对背景误判。其实我觉得全连接层也是对这个有贡献的，因为全连接起到了attention的作用。另外，Yolo的泛化能力强，在做迁移时，模型鲁棒性高。 最后不得不谈一下Yolo的缺点，首先Yolo各个单元格仅仅预测两个边界框，而且属于一个类别。对于小物体，Yolo的表现会不如人意。这方面的改进可以看SSD，其采用多尺度单元格。也可以看Faster R-CNN，其采用了anchor boxes。Yolo对于在物体的宽高比方面泛化率低，就是无法定位不寻常比例的物体。当然Yolo的定位不准确也是很大的问题。 算法的TF实现Yolo的源码是用C实现的，但是好在Github上有很多开源的TF复现。这里我们参考gliese581gg的YOLO_tensorflow的实现来分析Yolo的Inference实现细节。我们的代码将构建一个end-to-end的Yolo的预测模型，利用的已经训练好的权重文件，你将可以用自然的图片去测试检测效果。 首先，我们定义Yolo的模型参数： 12345678910111213141516171819202122232425class Yolo(object): def __init__(self, weights_file, verbose=True): self.verbose = verbose # detection params self.S = 7 # cell size self.B = 2 # boxes_per_cell self.classes = [&quot;aeroplane&quot;, &quot;bicycle&quot;, &quot;bird&quot;, &quot;boat&quot;, &quot;bottle&quot;, &quot;bus&quot;, &quot;car&quot;, &quot;cat&quot;, &quot;chair&quot;, &quot;cow&quot;, &quot;diningtable&quot;, &quot;dog&quot;, &quot;horse&quot;, &quot;motorbike&quot;, &quot;person&quot;, &quot;pottedplant&quot;, &quot;sheep&quot;, &quot;sofa&quot;, &quot;train&quot;,&quot;tvmonitor&quot;] self.C = len(self.classes) # number of classes # offset for box center (top left point of each cell) self.x_offset = np.transpose(np.reshape(np.array([np.arange(self.S)]*self.S*self.B), [self.B, self.S, self.S]), [1, 2, 0]) self.y_offset = np.transpose(self.x_offset, [1, 0, 2]) self.threshold = 0.2 # confidence scores threhold self.iou_threshold = 0.4 # the maximum number of boxes to be selected by non max suppression self.max_output_size = 10 self.sess = tf.Session() self._build_net() self._build_detector() self._load_weights(weights_file) 然后是我们模型的主体网络部分，这个网络将输出[batch,7730]的张量: 1234567891011121314151617181920212223242526272829303132333435363738def _build_net(self): &quot;&quot;&quot;build the network&quot;&quot;&quot; if self.verbose: print(&quot;Start to build the network ...&quot;) self.images = tf.placeholder(tf.float32, [None, 448, 448, 3]) net = self._conv_layer(self.images, 1, 64, 7, 2) net = self._maxpool_layer(net, 1, 2, 2) net = self._conv_layer(net, 2, 192, 3, 1) net = self._maxpool_layer(net, 2, 2, 2) net = self._conv_layer(net, 3, 128, 1, 1) net = self._conv_layer(net, 4, 256, 3, 1) net = self._conv_layer(net, 5, 256, 1, 1) net = self._conv_layer(net, 6, 512, 3, 1) net = self._maxpool_layer(net, 6, 2, 2) net = self._conv_layer(net, 7, 256, 1, 1) net = self._conv_layer(net, 8, 512, 3, 1) net = self._conv_layer(net, 9, 256, 1, 1) net = self._conv_layer(net, 10, 512, 3, 1) net = self._conv_layer(net, 11, 256, 1, 1) net = self._conv_layer(net, 12, 512, 3, 1) net = self._conv_layer(net, 13, 256, 1, 1) net = self._conv_layer(net, 14, 512, 3, 1) net = self._conv_layer(net, 15, 512, 1, 1) net = self._conv_layer(net, 16, 1024, 3, 1) net = self._maxpool_layer(net, 16, 2, 2) net = self._conv_layer(net, 17, 512, 1, 1) net = self._conv_layer(net, 18, 1024, 3, 1) net = self._conv_layer(net, 19, 512, 1, 1) net = self._conv_layer(net, 20, 1024, 3, 1) net = self._conv_layer(net, 21, 1024, 3, 1) net = self._conv_layer(net, 22, 1024, 3, 2) net = self._conv_layer(net, 23, 1024, 3, 1) net = self._conv_layer(net, 24, 1024, 3, 1) net = self._flatten(net) net = self._fc_layer(net, 25, 512, activation=leak_relu) net = self._fc_layer(net, 26, 4096, activation=leak_relu) net = self._fc_layer(net, 27, self.S*self.S*(self.C+5*self.B)) self.predicts = net 接下来，我们要去解析网络的预测结果，这里采用了第一种预测策略，即判断预测框类别，再NMS，多亏了TF提供了NMS的函数tf.image.non_max_suppression，其实实现起来很简单，所有的细节前面已经交代了： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def _build_detector(self): &quot;&quot;&quot;Interpret the net output and get the predicted boxes&quot;&quot;&quot; # the width and height of orignal image self.width = tf.placeholder(tf.float32, name=&quot;img_w&quot;) self.height = tf.placeholder(tf.float32, name=&quot;img_h&quot;) # get class prob, confidence, boxes from net output idx1 = self.S * self.S * self.C idx2 = idx1 + self.S * self.S * self.B # class prediction class_probs = tf.reshape(self.predicts[0, :idx1], [self.S, self.S, self.C]) # confidence confs = tf.reshape(self.predicts[0, idx1:idx2], [self.S, self.S, self.B]) # boxes -&gt; (x, y, w, h) boxes = tf.reshape(self.predicts[0, idx2:], [self.S, self.S, self.B, 4]) # convert the x, y to the coordinates relative to the top left point of the image # the predictions of w, h are the square root # multiply the width and height of image boxes = tf.stack([(boxes[:, :, :, 0] + tf.constant(self.x_offset, dtype=tf.float32)) / self.S * self.width, (boxes[:, :, :, 1] + tf.constant(self.y_offset, dtype=tf.float32)) / self.S * self.height, tf.square(boxes[:, :, :, 2]) * self.width, tf.square(boxes[:, :, :, 3]) * self.height], axis=3) # class-specific confidence scores [S, S, B, C] scores = tf.expand_dims(confs, -1) * tf.expand_dims(class_probs, 2) scores = tf.reshape(scores, [-1, self.C]) # [S*S*B, C] boxes = tf.reshape(boxes, [-1, 4]) # [S*S*B, 4] # find each box class, only select the max score box_classes = tf.argmax(scores, axis=1) box_class_scores = tf.reduce_max(scores, axis=1) # filter the boxes by the score threshold filter_mask = box_class_scores &gt;= self.threshold scores = tf.boolean_mask(box_class_scores, filter_mask) boxes = tf.boolean_mask(boxes, filter_mask) box_classes = tf.boolean_mask(box_classes, filter_mask) # non max suppression (do not distinguish different classes) # ref: https://tensorflow.google.cn/api_docs/python/tf/image/non_max_suppression # box (x, y, w, h) -&gt; box (x1, y1, x2, y2) _boxes = tf.stack([boxes[:, 0] - 0.5 * boxes[:, 2], boxes[:, 1] - 0.5 * boxes[:, 3], boxes[:, 0] + 0.5 * boxes[:, 2], boxes[:, 1] + 0.5 * boxes[:, 3]], axis=1) nms_indices = tf.image.non_max_suppression(_boxes, scores, self.max_output_size, self.iou_threshold) self.scores = tf.gather(scores, nms_indices) self.boxes = tf.gather(boxes, nms_indices) self.box_classes = tf.gather(box_classes, nms_indices) 其他的就比较容易了，详细代码附在xiaohu2015&#x2F;DeepLearning_tutorials上了，欢迎给点个赞，权重文件在这里下载。 最后就是愉快地测试你自己的图片了： 当然，如果你对训练过程感兴趣，你可以参考这里的实现thtrieu&#x2F;darkflow，如果你看懂了预测过程的代码，这里也会很容易阅读。 小结这篇长文详细介绍了Yolo算法的原理及实现，当然Yolo-v1还是有很多问题的，所以后续可以读读Yolo9000算法，看看其如何改进的。Ng说Yolo的paper是比较难读的，其实是很多实现细节，如果不看代码是很难理解的。所以，文章中如果有错误也可能是难免的，欢迎交流指正。 参考文献 You Only Look Once: Unified, Real-Time Object Detection. Yolo官网. Yolo的TF实现. YOLO: You only look once (How it works).（注：很多实现细节，需要墙） Ng的deeplearning.ai课程.","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-trafficReg","date":"2023-08-09T03:00:38.621Z","updated":"2023-08-07T08:49:40.000Z","comments":true,"path":"2023/08/09/2022-06-15-trafficReg/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-trafficReg/","excerpt":"","text":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD原文链接：https://medium.com/geoai/an-end-to-end-solution-on-the-cloud-to-monitor-traffic-flow-using-deep-learning-9dfdfd00b621 本文将大致介绍如何结合监控视频流，ArcGIS，ArcGIS API for API，AWS等技术来监测车流量。 目录： 交通治理以及研究问题描述实时视频流以及数据标注目标检测：在AWS上训练YOLO3模型流程架构使用Dashboard应用实时监测路况基于历史数据的异常行为监测结论以及展望致谢以及参考文献交通治理以及问题研究描述车流量是监测城市环境状态的一个重要要素。控制道路上车流量是一个非常基本的需求。在一些大城市，通常使用监控相机监测繁忙的道路，高速公路，以及十字路口。交通局工作人员通常对事故，路面覆盖物（雨，冰，雪），路面犯罪，抛锚，超速，拥堵，行人数量等信息感兴趣。监控相机可以帮助更好治理路况维持公共安全。国家高速公路安全管理局的一项研究表明，36%的碰撞事故都发生在道路交汇处。因此，十字路口时城市交通拥堵的罪魁祸首，也是交管中心重点监测对象。为了监测和管理路况，交通十字路口通常安装了许多相机。监测相机可以时固定，也可以是可遥控的PTZ相机。 监控抓拍图-昼 监控抓拍图-夜于是，华盛顿区域的交通部门需要Esri定制一个云解决方案，方案需要满足以下需求：1）监测110个交通路口的路况（小汽车，公交，卡车，摩托车，行人），并且使用GIS将其可视化。2）监测路口流量异常。3）监测处在危险区域的行人。这个解决方案不仅需要监控相机，还需要将空间数据和深度学习框架结合。 本文将介绍，如何使用ArcGIS，ArcGIS API for Pyhon，AWS以及Keras深度学习框架实现这个解决方案。解决方案是使用AWS环境中的GPU来加速实时处理视频流，从而进行模型训练和推断预测。ArcGIS API for Python将空间信息如视频流的位置与深度学习框架结合，并且使用ArcGIS Enterprise将时间信息一同保存。 实时视频流以及数据标注深度学习模型需要大量的训练数据。作者通过Traffic Land的REST API服务获取华盛顿111个路口的实视监控视频。作者使用Python代码，从TrafficLand服务上面获取了这111个监控相机的1000多张日夜抓拍图。作者将这些训练数据图片放到一个文件夹里面，然后使用LabelImg的工具人工标注图片中的目标物。最后将标注信息导出为txt文件，txt文件可以被绝大多数的目标检测算法使用。 使用LabelImg软件标注目标检测：在AWS上训练YOLO3模型我们的目的是从实时的视频中识别目标物。YOLO是一个目标检测很火的算法，该算法在实时应用中的精准度十分的高。YOLO可以生成目标物在图片中的位置，并且告诉用户该目标物的类别。YOLO只需要在网络中进行一次前向衍生就能够提供预测。早些版本的YOLO如YOLO2无法识别细小的目标物，因为YOLO2的计算层降低了输入图片的分辨率。除此之外，YOLO2还缺少一些牛叉的技术，如residual blocks，skip connections 以及 upsampling。YOLO3增加计算层以及YOLO2中那些没有的牛叉的功能。YOLO3算法在模型里面3个不同的位置，对尺寸不同的要素图运用1*1的识别窗口来实现目标检测。关于YOLO的原理有很多的博客和资源，这里不不做赘述。你可以在这些参考文献里了解YOLO的原理。 YOLO模型来自111个监控相机的1000多张带有标签的日夜抓拍图将被用作训练数据，在AWS上面训练YOLO3模型。笔者使用了AWS的EC2实例，EC2实例提供专门用来深度学习的镜像，这些镜像通常预装自带了Tensorflow，PyTorch，Keras等框架，可以用于训练复杂的深度学习模型。笔者使用了预训练的YOLO3模型以及转移学习技术。随后作者对比了预测结果和实际结果。训练模型的IOU达95%。笔者使用了现成的开源Github代码训练YOLO3 模型。 流程架构为了在AWS上面搭建一个实时流程，我们使用了如下架构来实现路况监测：1）我们使用并行处理来加速从TrafficLand REST API取视频流程的过程。2）紧接着，抓拍图被传到AWS EC2实例上的YOLO3模型里面。YOLO3对每一个片中的目标识别并且分类。总体上使用一个NVIDIA Tesla K80 GPU，我们可以在10内完成111张图片的抓取以及预测。3）最后我们将YOLO3的预测结果传到AWS的GeoEvent中的大数据库中，从而可以在大屏幕上对每一类数据进行可视化。每一个监控相机的相关照片都保存在S3 bucket云存储中。 流程架构设计图我们的IT团队在AWS配置好了深度学习EC2实例以及ArcGIS GeoEvent Server。GeoEvent Server将实时的流数据与带有位置数据的要素类或大数据库相结合。为了将GeoEvent Server和EC2实例上的YOLO3深度学习模型相连接，笔者在GeoEvent服务中配置了输入连接器，处理器，输出连接器。GeoEvent服务可以通过用户图形界面创建，类似Model Builder的创建方法。 GeoEvent输入连接器定义了来自YOLO3模型的事件数据结构，并且把事件数据传送给GeoEvent处理器。如果你的数据结构有差异，GeoEvent将无法读取事件数据。GeoEvent中有好几种常用格式（文本，RSS，ESRI要素JSON，JSON）和协议（系统文件，HTTP，TCP，UDP，WebSocket，ESRI要素服务）的输入连接器。建立输入连接器是，用户要新建一个GeoEvent Definition，GeoEvent Definition里面定义了事件数据的数据结构。下图分别展示了GeoJSON格式的GeoEvent Definition，和RestAPI的数据通信渠道。因此，每一个相机的YOLO3模型的输出结果都会使用相机位置信息转换成GeoJSON。 GeoEvent Definition GeoEvent Input ConnectorGeoEvent处理器是GeoEvent服务里面的一个可配置元素。GeoEvent服务提供基于事件数据的分析，比如对事件数据进行识别，对事件数据进行扩充。由于我们的流程没有对事件数据做任何处理，因此我们的流程中将不会使用任何GeoEvent处理器。 GeoEvent输出连接器的作用是将GeoEvent数据重新转成符合各种协议的流数据。我们配置了两个GeoEvent服务：1）一个实时GeoEvent服务，用于获取实时数据以及大屏可视化。2）一个历史GeoEvent服务，用于保存历史数据要素类可以后续用于异常分析。这两个服务的不同之处在于，实时服务只保存最新的111条记录，然而历史服务会保存之前所有生成的记录。我们可以算一下按每秒111条记录的速率，一天数据量将达到9.6百万（111相机24小时60分*60秒）。 Update a Feature Ouput Connector Add a Feature Output Connector使用Dashboard应用实时监测路况我们使用了Dashboard应用来实现自动化监测实时交通路况。Dashboard应用展示了111个实时视频流的位置，以及每个路口各种车型的计数。Dashboard的数据来自GeoEvent服务生成的要素类。用户通过Dashboard可以判断华盛顿区域行人或者车辆拥堵的具体位置。用户要可以看到每个监控相机的实时画面。下图展示了Dashboard应用的界面。Dashboard根据用户当前浏览的地图区域更新左侧的统计数据。 Dashboard应用 查询某一个路口基于历史数据的异常行为监测交通部门还想知道每一种车型和行人在路口的动态流量是如何的。为了解决这个问题，我们在一个礼拜后使用历史GeoEvent服务生成的数据来计算每一种车型和行人每天每一分钟的流量状况。我们简单计算一下，会有约1百万种可能的组合（111相机7天24小时*60分钟）。你可以把它比作一个异常监测的查询表。我们将每一种车型的计数与历史技术做比较，如果计数高于历史计数30%，那么我们称为流量异常，并且将异常展现在地图上。我们将异常事件写进一个单独的要素类里面。下图展示了某个路口的行人和车辆的异常状况。 交通部门还特别在意行人路口行为。他们主要想知道是否有行人不使用人行道。解决这个问题有很多方法。一半方法是检测处图像中的人行道，然后将人行道范围外的行人标为异常。 某路口异常行为笔者使用了另外一种方法。笔者找了一个路口连续运行YOLO3识别目标物五个小时，然后将所有行人类别的图片坐标（矩形框四个角的像素坐标）提取出，计算每一个矩形框右下，左下像素坐标的平均值，然后将每一个矩形框转换成一个像素点，我们之所以使用左下，右下的坐标，是因为这两个坐标更加贴近地面，可以更好的代表地面上的人行道。下图中每一个红色点代表这个小时内所有行人的位置。这份数据可以揭示行人过马路的规律。由于大多数人都使用人行道过马路，图中可以看到红点都聚集在人行道附近。反之，图片中的其他位置没有红点。 原始路口抓拍图 红点代表行人的历史位置为了将图像中高密度和低密度的红点分开，笔者使用了DBSCAN分析算法。DBSCAN更具点的空间分布以及每个点周围的噪声情况来识别高密度点聚类。DBSCAN还会将一些距离点聚类区域较远的点标为outlier。下图就是使用DBSCAN标记出了不在人行道区域的行人。 未使用人行道的行人 1 未使用人行道的行人 2结论以及展望本文，笔者介绍了GeoAI团队开发的交通路况监测云解决方案。该解决方案可以1）访问获取实时视频数据，2）使用AWS上的YOLO3模型实时识别小汽车，巴士，卡车，摩托车，以及行人，3）将YOLO3的结果发送到AWS上面的GeoEvent服务，在Dashboard应用上会展示路况，并且使用大数据库中的历史数据进行分析，4）根据目标物的数量来分析异常事件，例如监测处于危险位置的行人。GIS在展示相机地理位置以及实时路况起到了关键的作用。未来我们还会基于此方案进行目标追踪，测速，统计车道流量等。 致谢以及参考文献感谢Daniel Wilson配置AWS以及S3图片云存储，让整个流程快了10倍。感谢Joel McCune配置的Dashboard应用。感谢RJ Sunderman 在ArcGIS GeoEvent Server上的帮助，感谢Alberto Nieto联系交通部门启动了这个项目，使得我们可以将YOLO3，ArcGIS GeoEvent Server，AWS添加到Alberto之前的成果中去，实现实时云处理。最后感谢Mark Carlson配置了ArcGIS GeoEvent Server，以及AWS的深度学习镜像。我们还将这个解决方案复制到了Azure上面。如果你有疑问，或者有意向合作，欢迎联系我们。 1] https://developers.arcgis.com/python2] https://aws.amazon.com/machine-learning/amis3] http://www.arcgis.com/index.html4] http://www.trafficland.com5] https://github.com/tzutalin/labelImg6] https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b7] https://github.com/qqwweee/keras-yolo38] https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm9] https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm10] http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm11] https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview原文链接：https://medium.com/geoai/an-end-to-end-solution-on-the-cloud-to-monitor-traffic-flow-using-deep-learning-9dfdfd00b621 本文将大致介绍如何结合监控视频流，ArcGIS，ArcGIS API for API，AWS等技术来监测车流量。 目录： 交通治理以及研究问题描述实时视频流以及数据标注目标检测：在AWS上训练YOLO3模型流程架构使用Dashboard应用实时监测路况基于历史数据的异常行为监测结论以及展望致谢以及参考文献交通治理以及问题研究描述车流量是监测城市环境状态的一个重要要素。控制道路上车流量是一个非常基本的需求。在一些大城市，通常使用监控相机监测繁忙的道路，高速公路，以及十字路口。交通局工作人员通常对事故，路面覆盖物（雨，冰，雪），路面犯罪，抛锚，超速，拥堵，行人数量等信息感兴趣。监控相机可以帮助更好治理路况维持公共安全。国家高速公路安全管理局的一项研究表明，36%的碰撞事故都发生在道路交汇处。因此，十字路口时城市交通拥堵的罪魁祸首，也是交管中心重点监测对象。为了监测和管理路况，交通十字路口通常安装了许多相机。监测相机可以时固定，也可以是可遥控的PTZ相机。 监控抓拍图-昼 监控抓拍图-夜于是，华盛顿区域的交通部门需要Esri定制一个云解决方案，方案需要满足以下需求：1）监测110个交通路口的路况（小汽车，公交，卡车，摩托车，行人），并且使用GIS将其可视化。2）监测路口流量异常。3）监测处在危险区域的行人。这个解决方案不仅需要监控相机，还需要将空间数据和深度学习框架结合。 本文将介绍，如何使用ArcGIS，ArcGIS API for Pyhon，AWS以及Keras深度学习框架实现这个解决方案。解决方案是使用AWS环境中的GPU来加速实时处理视频流，从而进行模型训练和推断预测。ArcGIS API for Python将空间信息如视频流的位置与深度学习框架结合，并且使用ArcGIS Enterprise将时间信息一同保存。 实时视频流以及数据标注深度学习模型需要大量的训练数据。作者通过Traffic Land的REST API服务获取华盛顿111个路口的实视监控视频。作者使用Python代码，从TrafficLand服务上面获取了这111个监控相机的1000多张日夜抓拍图。作者将这些训练数据图片放到一个文件夹里面，然后使用LabelImg的工具人工标注图片中的目标物。最后将标注信息导出为txt文件，txt文件可以被绝大多数的目标检测算法使用。 使用LabelImg软件标注目标检测：在AWS上训练YOLO3模型我们的目的是从实时的视频中识别目标物。YOLO是一个目标检测很火的算法，该算法在实时应用中的精准度十分的高。YOLO可以生成目标物在图片中的位置，并且告诉用户该目标物的类别。YOLO只需要在网络中进行一次前向衍生就能够提供预测。早些版本的YOLO如YOLO2无法识别细小的目标物，因为YOLO2的计算层降低了输入图片的分辨率。除此之外，YOLO2还缺少一些牛叉的技术，如residual blocks，skip connections 以及 upsampling。YOLO3增加计算层以及YOLO2中那些没有的牛叉的功能。YOLO3算法在模型里面3个不同的位置，对尺寸不同的要素图运用1*1的识别窗口来实现目标检测。关于YOLO的原理有很多的博客和资源，这里不不做赘述。你可以在这些参考文献里了解YOLO的原理。 YOLO模型来自111个监控相机的1000多张带有标签的日夜抓拍图将被用作训练数据，在AWS上面训练YOLO3模型。笔者使用了AWS的EC2实例，EC2实例提供专门用来深度学习的镜像，这些镜像通常预装自带了Tensorflow，PyTorch，Keras等框架，可以用于训练复杂的深度学习模型。笔者使用了预训练的YOLO3模型以及转移学习技术。随后作者对比了预测结果和实际结果。训练模型的IOU达95%。笔者使用了现成的开源Github代码训练YOLO3 模型。 流程架构为了在AWS上面搭建一个实时流程，我们使用了如下架构来实现路况监测：1）我们使用并行处理来加速从TrafficLand REST API取视频流程的过程。2）紧接着，抓拍图被传到AWS EC2实例上的YOLO3模型里面。YOLO3对每一个片中的目标识别并且分类。总体上使用一个NVIDIA Tesla K80 GPU，我们可以在10内完成111张图片的抓取以及预测。3）最后我们将YOLO3的预测结果传到AWS的GeoEvent中的大数据库中，从而可以在大屏幕上对每一类数据进行可视化。每一个监控相机的相关照片都保存在S3 bucket云存储中。 流程架构设计图我们的IT团队在AWS配置好了深度学习EC2实例以及ArcGIS GeoEvent Server。GeoEvent Server将实时的流数据与带有位置数据的要素类或大数据库相结合。为了将GeoEvent Server和EC2实例上的YOLO3深度学习模型相连接，笔者在GeoEvent服务中配置了输入连接器，处理器，输出连接器。GeoEvent服务可以通过用户图形界面创建，类似Model Builder的创建方法。 GeoEvent输入连接器定义了来自YOLO3模型的事件数据结构，并且把事件数据传送给GeoEvent处理器。如果你的数据结构有差异，GeoEvent将无法读取事件数据。GeoEvent中有好几种常用格式（文本，RSS，ESRI要素JSON，JSON）和协议（系统文件，HTTP，TCP，UDP，WebSocket，ESRI要素服务）的输入连接器。建立输入连接器是，用户要新建一个GeoEvent Definition，GeoEvent Definition里面定义了事件数据的数据结构。下图分别展示了GeoJSON格式的GeoEvent Definition，和RestAPI的数据通信渠道。因此，每一个相机的YOLO3模型的输出结果都会使用相机位置信息转换成GeoJSON。 GeoEvent Definition GeoEvent Input ConnectorGeoEvent处理器是GeoEvent服务里面的一个可配置元素。GeoEvent服务提供基于事件数据的分析，比如对事件数据进行识别，对事件数据进行扩充。由于我们的流程没有对事件数据做任何处理，因此我们的流程中将不会使用任何GeoEvent处理器。 GeoEvent输出连接器的作用是将GeoEvent数据重新转成符合各种协议的流数据。我们配置了两个GeoEvent服务：1）一个实时GeoEvent服务，用于获取实时数据以及大屏可视化。2）一个历史GeoEvent服务，用于保存历史数据要素类可以后续用于异常分析。这两个服务的不同之处在于，实时服务只保存最新的111条记录，然而历史服务会保存之前所有生成的记录。我们可以算一下按每秒111条记录的速率，一天数据量将达到9.6百万（111相机24小时60分*60秒）。 Update a Feature Ouput Connector Add a Feature Output Connector使用Dashboard应用实时监测路况我们使用了Dashboard应用来实现自动化监测实时交通路况。Dashboard应用展示了111个实时视频流的位置，以及每个路口各种车型的计数。Dashboard的数据来自GeoEvent服务生成的要素类。用户通过Dashboard可以判断华盛顿区域行人或者车辆拥堵的具体位置。用户要可以看到每个监控相机的实时画面。下图展示了Dashboard应用的界面。Dashboard根据用户当前浏览的地图区域更新左侧的统计数据。 Dashboard应用 查询某一个路口基于历史数据的异常行为监测交通部门还想知道每一种车型和行人在路口的动态流量是如何的。为了解决这个问题，我们在一个礼拜后使用历史GeoEvent服务生成的数据来计算每一种车型和行人每天每一分钟的流量状况。我们简单计算一下，会有约1百万种可能的组合（111相机7天24小时*60分钟）。你可以把它比作一个异常监测的查询表。我们将每一种车型的计数与历史技术做比较，如果计数高于历史计数30%，那么我们称为流量异常，并且将异常展现在地图上。我们将异常事件写进一个单独的要素类里面。下图展示了某个路口的行人和车辆的异常状况。 交通部门还特别在意行人路口行为。他们主要想知道是否有行人不使用人行道。解决这个问题有很多方法。一半方法是检测处图像中的人行道，然后将人行道范围外的行人标为异常。 某路口异常行为笔者使用了另外一种方法。笔者找了一个路口连续运行YOLO3识别目标物五个小时，然后将所有行人类别的图片坐标（矩形框四个角的像素坐标）提取出，计算每一个矩形框右下，左下像素坐标的平均值，然后将每一个矩形框转换成一个像素点，我们之所以使用左下，右下的坐标，是因为这两个坐标更加贴近地面，可以更好的代表地面上的人行道。下图中每一个红色点代表这个小时内所有行人的位置。这份数据可以揭示行人过马路的规律。由于大多数人都使用人行道过马路，图中可以看到红点都聚集在人行道附近。反之，图片中的其他位置没有红点。 原始路口抓拍图 红点代表行人的历史位置为了将图像中高密度和低密度的红点分开，笔者使用了DBSCAN分析算法。DBSCAN更具点的空间分布以及每个点周围的噪声情况来识别高密度点聚类。DBSCAN还会将一些距离点聚类区域较远的点标为outlier。下图就是使用DBSCAN标记出了不在人行道区域的行人。 未使用人行道的行人 1 未使用人行道的行人 2结论以及展望本文，笔者介绍了GeoAI团队开发的交通路况监测云解决方案。该解决方案可以1）访问获取实时视频数据，2）使用AWS上的YOLO3模型实时识别小汽车，巴士，卡车，摩托车，以及行人，3）将YOLO3的结果发送到AWS上面的GeoEvent服务，在Dashboard应用上会展示路况，并且使用大数据库中的历史数据进行分析，4）根据目标物的数量来分析异常事件，例如监测处于危险位置的行人。GIS在展示相机地理位置以及实时路况起到了关键的作用。未来我们还会基于此方案进行目标追踪，测速，统计车道流量等。 致谢以及参考文献感谢Daniel Wilson配置AWS以及S3图片云存储，让整个流程快了10倍。感谢Joel McCune配置的Dashboard应用。感谢RJ Sunderman 在ArcGIS GeoEvent Server上的帮助，感谢Alberto Nieto联系交通部门启动了这个项目，使得我们可以将YOLO3，ArcGIS GeoEvent Server，AWS添加到Alberto之前的成果中去，实现实时云处理。最后感谢Mark Carlson配置了ArcGIS GeoEvent Server，以及AWS的深度学习镜像。我们还将这个解决方案复制到了Azure上面。如果你有疑问，或者有意向合作，欢迎联系我们。 1] https://developers.arcgis.com/python2] https://aws.amazon.com/machine-learning/amis3] http://www.arcgis.com/index.html4] http://www.trafficland.com5] https://github.com/tzutalin/labelImg6] https://arxiv.org/abs/1506.02640https://arxiv.org/abs/1612.08242https://arxiv.org/abs/1804.02767https://lilianweng.github.io/lil-log/2018/12/27/object-detection-part-4.html#yolo-you-only-look-oncehttps://towardsdatascience.com/yolo-v3-object-detection-53fb7d3bfe6b7] https://github.com/qqwweee/keras-yolo38] https://enterprise.arcgis.com/en/geoevent/latest/get-started/a-quick-tour-of-geoevent-server.htm9] https://enterprise.arcgis.com/en/geoevent/latest/administer/managing-big-data-stores.htm10] http://desktop.arcgis.com/en/arcmap/10.3/analyze/modelbuilder/what-is-modelbuilder.htm11] https://www.esri.com/en-us/arcgis/products/operations-dashboard/overview 7700261 (first commit)12] https://pro.arcgis.com/en/pro-app/tool-reference/spatial-statistics/densitybasedclustering.htm","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-Perceptron","date":"2023-08-09T03:00:38.586Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-06-15-Perceptron/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-Perceptron/","excerpt":"","text":"单层感知机$$\\begin{aligned}&amp; y &#x3D; XW + b \\&amp; y &#x3D; \\sum x_i*w_i+b\\\\end{aligned}$$ Derivative$$\\begin{aligned}&amp;E&#x3D;\\frac{1}{2}(O^1_0-t)^2\\&amp;\\frac{\\delta E}{\\delta W_{j0}}&#x3D;(O_0-t)\\frac{\\delta O_0}{\\delta w_{j0}}\\&amp;&#x3D;(O_0-t)\\frac{\\delta O_0}{\\delta w_{j0}}\\&amp;&#x3D;(O_0-t)\\delta(x_0)(1-\\delta(x_0))\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_0-t)O_0(1-O_0)\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_0-t)O_0(1-O_0)x_j^0\\end{aligned}$$ 1import torch,torch.nn.functional as F 1234x = torch.randn(1, 10)w = torch.randn(1, 10, requires_grad=True)o = torch.sigmoid(x@w.t())o.shape torch.Size([1, 1]) 12loss = F.mse_loss(torch.ones(1, 1), o)loss.shape torch.Size([]) 1loss.backward() 1w.grad tensor([[-0.1801, 0.1923, 0.2480, -0.0919, 0.1487, 0.0196, -0.1588, -0.1652, 0.3811, -0.2290]]) Multi-output Perceptron Derivative$$\\begin{aligned}&amp;E&#x3D;\\frac{1}{2}(O^1_i-t)^2\\&amp;\\frac{\\delta E}{\\delta W_{jk}}&#x3D;(O_k-t_k)\\frac{\\delta O_k}{\\delta w_{jk}}\\&amp;&#x3D;(O_k-t)\\frac{\\delta O_0}{\\delta w_{j0}}\\&amp;&#x3D;(O_k-t)\\delta(x_0)(1-\\delta(x_0))\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_k-t)O_0(1-O_0)\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_k-t)O_0(1-O_0)x_j^0\\end{aligned}$$","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-Minima","date":"2023-08-09T03:00:38.552Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-06-15-Minima/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-Minima/","excerpt":"","text":"&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD Minima$$f(x,y)&#x3D;(x^2+y-11)^2+(x+y^2-7)^2$$$$f(3.0, 2.0)&#x3D;0.0\\f(-2.8505118, 3.131312)&#x3D;0.0\\f(-3.779310, -3.283186)&#x3D;0.0\\f(3.584428, -1.84126)&#x3D;0.0\\$$ 1234%matplotlib inlineimport numpy as np, torch, torch.nn.functional as Ffrom matplotlib import pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D 123# plotdef himmelblau(x): return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2 123x = np.arange(-6, 6, 0.1)y = np.arange(-6, 6, 0.1)print(&quot;x, y range:&quot;, x.shape, y.shape) 1x, y range: (120,) (120,) 123X, Y = np.meshgrid(x, y)print(&quot;X, Y maps:&quot;, X.shape, Y.shape)Z = himmelblau([X, Y]) 1X, Y maps: (120, 120) (120, 120) 1234567fig = plt.figure(&#x27;himmelblau&#x27;)ax = fig.gca(projection = &#x27;3d&#x27;)ax.plot_surface(X, Y, Z)ax.view_init(60, -30)ax.set_xlabel(&#x27;x&#x27;)ax.set_ylabel(&#x27;y&#x27;)plt.show() 1234567891011# Gradient Descent#[1., 0] [-4, 0.] [4, 0.]x = torch.tensor([0., 0.], requires_grad=True)opt = torch.optim.Adam([x], lr=1e-3)for step in range(20001): pred = himmelblau(x) opt.zero_grad() pred.backward() opt.step() if step % 2000 == 0: print(&quot;step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;&quot;.format(step, x.tolist(), pred.item())) 1234567891011121314151617181920212223242526272829step 0: x = [0.0009999999310821295, 0.0009999999310821295], f(x) = 170.0step 2000: x = [2.3331806659698486, 1.9540694952011108], f(x) = 13.730916023254395step 4000: x = [2.9820079803466797, 2.0270984172821045], f(x) = 0.014858869835734367step 6000: x = [2.999983549118042, 2.0000221729278564], f(x) = 1.1074007488787174e-08step 8000: x = [2.9999938011169434, 2.0000083446502686], f(x) = 1.5572823031106964e-09step 10000: x = [2.999997854232788, 2.000002861022949], f(x) = 1.8189894035458565e-10step 12000: x = [2.9999992847442627, 2.0000009536743164], f(x) = 1.6370904631912708e-11step 14000: x = [2.999999761581421, 2.000000238418579], f(x) = 1.8189894035458565e-12step 16000: x = [3.0, 2.0], f(x) = 0.0step 18000: x = [3.0, 2.0], f(x) = 0.0step 20000: x = [3.0, 2.0], f(x) = 0.0=======#### Minima $$f(x,y)=(x^2+y-11)^2+(x+y^2-7)^2$$![himff](https://raw.githubusercontent.com/ebxeax/images/main/himff.png)$$f(3.0, 2.0)=0.0\\\\f(-2.8505118, 3.131312)=0.0\\\\f(-3.779310, -3.283186)=0.0\\\\f(3.584428, -1.84126)=0.0\\\\$$```python%matplotlib inlineimport numpy as np, torch, torch.nn.functional as Ffrom matplotlib import pyplot as pltfrom mpl_toolkits.mplot3d import Axes3D 123# plotdef himmelblau(x): return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2 123x = np.arange(-6, 6, 0.1)y = np.arange(-6, 6, 0.1)print(&quot;x, y range:&quot;, x.shape, y.shape) 1x, y range: (120,) (120,) 123X, Y = np.meshgrid(x, y)print(&quot;X, Y maps:&quot;, X.shape, Y.shape)Z = himmelblau([X, Y]) 1X, Y maps: (120, 120) (120, 120) 1234567fig = plt.figure(&#x27;himmelblau&#x27;)ax = fig.gca(projection = &#x27;3d&#x27;)ax.plot_surface(X, Y, Z)ax.view_init(60, -30)ax.set_xlabel(&#x27;x&#x27;)ax.set_ylabel(&#x27;y&#x27;)plt.show() 1234567891011# Gradient Descent#[1., 0] [-4, 0.] [4, 0.]x = torch.tensor([0., 0.], requires_grad=True)opt = torch.optim.Adam([x], lr=1e-3)for step in range(20001): pred = himmelblau(x) opt.zero_grad() pred.backward() opt.step() if step % 2000 == 0: print(&quot;step &#123;&#125;: x = &#123;&#125;, f(x) = &#123;&#125;&quot;.format(step, x.tolist(), pred.item())) 123456789101112step 0: x = [0.0009999999310821295, 0.0009999999310821295], f(x) = 170.0step 2000: x = [2.3331806659698486, 1.9540694952011108], f(x) = 13.730916023254395step 4000: x = [2.9820079803466797, 2.0270984172821045], f(x) = 0.014858869835734367step 6000: x = [2.999983549118042, 2.0000221729278564], f(x) = 1.1074007488787174e-08step 8000: x = [2.9999938011169434, 2.0000083446502686], f(x) = 1.5572823031106964e-09step 10000: x = [2.999997854232788, 2.000002861022949], f(x) = 1.8189894035458565e-10step 12000: x = [2.9999992847442627, 2.0000009536743164], f(x) = 1.6370904631912708e-11step 14000: x = [2.999999761581421, 2.000000238418579], f(x) = 1.8189894035458565e-12step 16000: x = [3.0, 2.0], f(x) = 0.0step 18000: x = [3.0, 2.0], f(x) = 0.0step 20000: x = [3.0, 2.0], f(x) = 0.0&gt;&gt;&gt;&gt;&gt;&gt;&gt; 7700261 (first commit)","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-cnn原则2","date":"2023-08-09T03:00:38.523Z","updated":"2023-08-07T08:49:40.000Z","comments":true,"path":"2023/08/09/2022-06-15-cnn原则2/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-cnn%E5%8E%9F%E5%88%992/","excerpt":"","text":"原则#2 - 局部性$$h_{i,j}&#x3D;\\sum{a,b}v_{a,b}x_{i+a,j+b}$$","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-021gpu_accelerated","date":"2023-08-09T03:00:38.493Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-06-15-021gpu_accelerated/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-021gpu_accelerated/","excerpt":"","text":"12import torchimport torch.nn.functional as F 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transforms#超参数batch_size=200learning_rate=0.01epochs=10#获取训练数据train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=True, download=True, #train=True则得到的是训练集 transform=transforms.Compose([ #transform进行数据预处理 transforms.ToTensor(), #转成Tensor类型的数据 transforms.Normalize((0.1307,), (0.3081,)) #进行数据标准化(减去均值除以方差) ])), batch_size=batch_size, shuffle=True) #按batch_size分出一个batch维度在最前面,shuffle=True打乱顺序#获取测试数据test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.model = nn.Sequential( #定义网络的每一层，nn.ReLU可以换成其他激活函数，比如nn.LeakyReLU() nn.Linear(784, 200), nn.ReLU(inplace=True), nn.Linear(200, 200), nn.ReLU(inplace=True), nn.Linear(200, 10), nn.ReLU(inplace=True), ) def forward(self, x): x = self.model(x) return xnet = MLP()#定义sgd优化器,指明优化参数、学习率，net.parameters()得到这个类所定义的网络的参数[[w1,b1,w2,b2,...]optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss()for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) #将二维的图片数据摊平[样本数,784] logits = net(data) #前向传播 loss = criteon(logits, target) #nn.CrossEntropyLoss()自带Softmax optimizer.zero_grad() #梯度信息清空 loss.backward() #反向传播获取梯度 optimizer.step() #优化器更新 if batch_idx % 100 == 0: #每100个batch输出一次信息 print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) test_loss = 0 correct = 0 #correct记录正确分类的样本数 for data, target in test_loader: data = data.view(-1, 28 * 28) logits = net(data) test_loss += criteon(logits, target).item() #其实就是criteon(logits, target)的值，标量 pred = logits.data.max(dim=1)[1] #也可以写成pred=logits.argmax(dim=1) correct += pred.eq(target.data).sum() test_loss /= len(test_loader.dataset) print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\\n&#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) C:\\Users\\ygx79\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] 找不到指定的模块。 warn(f&quot;Failed to load image Python extension: &#123;e&#125;&quot;) Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz 9913344it [09:45, 16922.35it/s] Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz 29696it [00:00, 112126.01it/s] Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz 1649664it [00:06, 236143.14it/s] Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz 5120it [00:00, ?it/s] Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw Train Epoch: 0 [0/60000 (0%)] Loss: 2.307192 Train Epoch: 0 [20000/60000 (33%)] Loss: 2.138816 Train Epoch: 0 [40000/60000 (67%)] Loss: 1.768016 Test set: Average loss: 0.0070, Accuracy: 6058/10000 (61%) Train Epoch: 1 [0/60000 (0%)] Loss: 1.505597 Train Epoch: 1 [20000/60000 (33%)] Loss: 1.149395 Train Epoch: 1 [40000/60000 (67%)] Loss: 1.039293 Test set: Average loss: 0.0047, Accuracy: 7143/10000 (71%) Train Epoch: 2 [0/60000 (0%)] Loss: 1.061429 Train Epoch: 2 [20000/60000 (33%)] Loss: 0.741140 Train Epoch: 2 [40000/60000 (67%)] Loss: 0.901448 Test set: Average loss: 0.0041, Accuracy: 7299/10000 (73%) Train Epoch: 3 [0/60000 (0%)] Loss: 0.809117 Train Epoch: 3 [20000/60000 (33%)] Loss: 0.892138 Train Epoch: 3 [40000/60000 (67%)] Loss: 0.659411 Test set: Average loss: 0.0030, Accuracy: 8170/10000 (82%) Train Epoch: 4 [0/60000 (0%)] Loss: 0.622007 Train Epoch: 4 [20000/60000 (33%)] Loss: 0.592337 Train Epoch: 4 [40000/60000 (67%)] Loss: 0.445400 Test set: Average loss: 0.0027, Accuracy: 8225/10000 (82%) Train Epoch: 5 [0/60000 (0%)] Loss: 0.519135 Train Epoch: 5 [20000/60000 (33%)] Loss: 0.491247 Train Epoch: 5 [40000/60000 (67%)] Loss: 0.562315 Test set: Average loss: 0.0026, Accuracy: 8295/10000 (83%) Train Epoch: 6 [0/60000 (0%)] Loss: 0.509583 Train Epoch: 6 [20000/60000 (33%)] Loss: 0.553628 Train Epoch: 6 [40000/60000 (67%)] Loss: 0.484189 Test set: Average loss: 0.0025, Accuracy: 8336/10000 (83%) Train Epoch: 7 [0/60000 (0%)] Loss: 0.619250 Train Epoch: 7 [20000/60000 (33%)] Loss: 0.634936 Train Epoch: 7 [40000/60000 (67%)] Loss: 0.440220 Test set: Average loss: 0.0024, Accuracy: 8370/10000 (84%) Train Epoch: 8 [0/60000 (0%)] Loss: 0.410350 Train Epoch: 8 [20000/60000 (33%)] Loss: 0.460459 Train Epoch: 8 [40000/60000 (67%)] Loss: 0.395150 Test set: Average loss: 0.0024, Accuracy: 8395/10000 (84%) Train Epoch: 9 [0/60000 (0%)] Loss: 0.515630 Train Epoch: 9 [20000/60000 (33%)] Loss: 0.546718 Train Epoch: 9 [40000/60000 (67%)] Loss: 0.496167 Test set: Average loss: 0.0023, Accuracy: 8433/10000 (84%) 12345device = torch.device(&#x27;cuda:0&#x27;)net = MLP().to(device)#定义sgd优化器,指明优化参数、学习率，net.parameters()得到这个类所定义的网络的参数[[w1,b1,w2,b2,...]optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss().to(device) GPU acc1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transforms#超参数batch_size=200learning_rate=0.01epochs=10#获取训练数据train_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=True, download=True, #train=True则得到的是训练集 transform=transforms.Compose([ #transform进行数据预处理 transforms.ToTensor(), #转成Tensor类型的数据 transforms.Normalize((0.1307,), (0.3081,)) #进行数据标准化(减去均值除以方差) ])), batch_size=batch_size, shuffle=True) #按batch_size分出一个batch维度在最前面,shuffle=True打乱顺序#获取测试数据test_loader = torch.utils.data.DataLoader( datasets.MNIST(&#x27;../data&#x27;, train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True)class MLP(nn.Module): def __init__(self): super(MLP, self).__init__() self.model = nn.Sequential( #定义网络的每一层, nn.Linear(784, 200), nn.ReLU(inplace=True), nn.Linear(200, 200), nn.ReLU(inplace=True), nn.Linear(200, 10), nn.ReLU(inplace=True), ) def forward(self, x): x = self.model(x) return xdevice = torch.device(&#x27;cuda:0&#x27;)net = MLP().to(device)#定义sgd优化器,指明优化参数、学习率，net.parameters()得到这个类所定义的网络的参数[[w1,b1,w2,b2,...]optimizer = optim.SGD(net.parameters(), lr=learning_rate)criteon = nn.CrossEntropyLoss().to(device)for epoch in range(epochs): for batch_idx, (data, target) in enumerate(train_loader): data = data.view(-1, 28*28) #将二维的图片数据摊平[样本数,784] data, target = data.to(device), target.cuda() logits = net(data) #前向传播 loss = criteon(logits, target) #nn.CrossEntropyLoss()自带Softmax optimizer.zero_grad() #梯度信息清空 loss.backward() #反向传播获取梯度 optimizer.step() #优化器更新 if batch_idx % 100 == 0: #每100个batch输出一次信息 print(&#x27;Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\\tLoss: &#123;:.6f&#125;&#x27;.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item())) test_loss = 0 correct = 0 #correct记录正确分类的样本数 for data, target in test_loader: data = data.view(-1, 28 * 28) data, target = data.to(device), target.cuda() logits = net(data) test_loss += criteon(logits, target).item() #其实就是criteon(logits, target)的值，标量 pred = logits.data.max(dim=1)[1] #也可以写成pred=logits.argmax(dim=1) correct += pred.eq(target.data).sum() test_loss /= len(test_loader.dataset) print(&#x27;\\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\\n&#x27;.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset))) Train Epoch: 0 [0/60000 (0%)] Loss: 2.291108 Train Epoch: 0 [20000/60000 (33%)] Loss: 2.003711 Train Epoch: 0 [40000/60000 (67%)] Loss: 1.419139 Test set: Average loss: 0.0038, Accuracy: 8229/10000 (82%) Train Epoch: 1 [0/60000 (0%)] Loss: 0.754257 Train Epoch: 1 [20000/60000 (33%)] Loss: 0.655030 Train Epoch: 1 [40000/60000 (67%)] Loss: 0.444529 Test set: Average loss: 0.0021, Accuracy: 8884/10000 (89%) Train Epoch: 2 [0/60000 (0%)] Loss: 0.439030 Train Epoch: 2 [20000/60000 (33%)] Loss: 0.355868 Train Epoch: 2 [40000/60000 (67%)] Loss: 0.366360 Test set: Average loss: 0.0017, Accuracy: 9037/10000 (90%) Train Epoch: 3 [0/60000 (0%)] Loss: 0.439010 Train Epoch: 3 [20000/60000 (33%)] Loss: 0.344060 Train Epoch: 3 [40000/60000 (67%)] Loss: 0.255032 Test set: Average loss: 0.0015, Accuracy: 9116/10000 (91%) Train Epoch: 4 [0/60000 (0%)] Loss: 0.331074 Train Epoch: 4 [20000/60000 (33%)] Loss: 0.301065 Train Epoch: 4 [40000/60000 (67%)] Loss: 0.276514 Test set: Average loss: 0.0014, Accuracy: 9169/10000 (92%) Train Epoch: 5 [0/60000 (0%)] Loss: 0.281249 Train Epoch: 5 [20000/60000 (33%)] Loss: 0.316320 Train Epoch: 5 [40000/60000 (67%)] Loss: 0.248902 Test set: Average loss: 0.0013, Accuracy: 9210/10000 (92%) Train Epoch: 6 [0/60000 (0%)] Loss: 0.317820 Train Epoch: 6 [20000/60000 (33%)] Loss: 0.315888 Train Epoch: 6 [40000/60000 (67%)] Loss: 0.302683 Test set: Average loss: 0.0013, Accuracy: 9258/10000 (93%) Train Epoch: 7 [0/60000 (0%)] Loss: 0.290187","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-019cross_entropy","date":"2023-08-09T03:00:38.460Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-06-15-019cross_entropy/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-019cross_entropy/","excerpt":"","text":"EntropyUncetainlymeasure of surprisehigher entropy &#x3D; less info$$Entropy &#x3D; -\\sum_i P(i)\\log P(i)$$ Lottery1import torch 1a = torch.full([4], 1/4.) 1a * torch.log2(a) tensor([-0.5000, -0.5000, -0.5000, -0.5000]) 1-(a * torch.log2(a)).sum() tensor(2.) 12a = torch.tensor([0.1, 0.1, 0.1, 0.7])-(a * torch.log2(a)).sum() tensor(1.3568) 12a = torch.tensor([0.001, 0.001, 0.001, 0.999])-(a * torch.log2(a)).sum() tensor(0.0313) Croos Entropy$$\\begin{aligned}&amp;H(p,q)&#x3D;-\\sum p(x)\\log q(x)\\&amp;H(p,q)&#x3D;H(p)+D_{KL}(p|q)\\\\end{aligned}$$ P&#x3D;Qcross Entropy &#x3D; Entropy for one-hot encodingentropy &#x3D; log1 &#x3D;0 Binary Classification$$\\begin{aligned}&amp;H(P,Q)&#x3D;-P(cat)\\log Q(cat)-(1-P(cat))\\log(1-Q(cat))\\&amp;P(dog)&#x3D;(1-P(cat))\\&amp;H(P,Q)&#x3D;-\\sum_{i&#x3D;(cat,dog)}P(i)\\log(Q(i))\\&amp;&#x3D;-P(cat)\\log Q(cat)-P(dog)\\log Q(dog)-(y\\log(p)+(1-y)\\log (1-p))\\\\end{aligned}$$ 1","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-016chain_rules","date":"2023-08-09T03:00:38.426Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-06-15-016chain_rules/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-016chain_rules/","excerpt":"","text":"Derivative Rules$$\\begin{aligned}&amp;\\frac{\\delta E}{\\delta w^1_{jk}}&#x3D;\\frac{\\delta E}{\\delta O_k^1}\\frac{\\delta O_k^1}{\\delta w^1_{jk}}&#x3D;\\frac{\\delta E}{\\delta O_k^2}\\frac{\\delta O_k^2}{\\delta O_k^1}\\frac{\\delta O_k^1}{\\delta w^1_{jk}}\\\\end{aligned}$$ 1import torch, torch.nn.functional as F 123x = torch.tensor(1.)w1, w2 = torch.tensor(2., requires_grad=True), torch.tensor(2., requires_grad=True)b1, b2 = torch.tensor(1.), torch.tensor(1.) 12y1 = x * w1 + b1 y2 = y1 * w2 +b2 123dy2_dy1 = torch.autograd.grad(y2, [y1], retain_graph=True)[0]dy1_dw1 = torch.autograd.grad(y1, [w1], retain_graph=True)[0]dy2_dw1 = torch.autograd.grad(y2, [w1], retain_graph=True)[0] 1dy2_dy1 * dy1_dw1 tensor(2.) 1dy2_dw1 tensor(2.)","categories":[],"tags":[]},{"title":"","slug":"2022-06-15-015Perceptron","date":"2023-08-09T03:00:38.388Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-06-15-015Perceptron/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-06-15-015Perceptron/","excerpt":"","text":"单层感知机$$\\begin{aligned}&amp; y &#x3D; XW + b \\&amp; y &#x3D; \\sum x_i*w_i+b\\\\end{aligned}$$ Derivative$$\\begin{aligned}&amp;E&#x3D;\\frac{1}{2}(O^1_0-t)^2\\&amp;\\frac{\\delta E}{\\delta W_{j0}}&#x3D;(O_0-t)\\frac{\\delta O_0}{\\delta w_{j0}}\\&amp;&#x3D;(O_0-t)\\frac{\\delta O_0}{\\delta w_{j0}}\\&amp;&#x3D;(O_0-t)\\delta(x_0)(1-\\delta(x_0))\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_0-t)O_0(1-O_0)\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_0-t)O_0(1-O_0)x_j^0\\end{aligned}$$ 1import torch,torch.nn.functional as F 1234x = torch.randn(1, 10)w = torch.randn(1, 10, requires_grad=True)o = torch.sigmoid(x@w.t())o.shape torch.Size([1, 1]) 12loss = F.mse_loss(torch.ones(1, 1), o)loss.shape torch.Size([]) 1loss.backward() 1w.grad tensor([[-0.1801, 0.1923, 0.2480, -0.0919, 0.1487, 0.0196, -0.1588, -0.1652, 0.3811, -0.2290]]) Multi-output Perceptron Derivative$$\\begin{aligned}&amp;E&#x3D;\\frac{1}{2}(O^1_i-t)^2\\&amp;\\frac{\\delta E}{\\delta W_{jk}}&#x3D;(O_k-t_k)\\frac{\\delta O_k}{\\delta w_{jk}}\\&amp;&#x3D;(O_k-t)\\frac{\\delta O_0}{\\delta w_{j0}}\\&amp;&#x3D;(O_k-t)\\delta(x_0)(1-\\delta(x_0))\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_k-t)O_0(1-O_0)\\frac{\\delta x_0^1}{\\delta w_j^0}\\&amp;&#x3D;(O_k-t)O_0(1-O_0)x_j^0\\end{aligned}$$","categories":[],"tags":[]},{"title":"","slug":"2022-04-15-Win-KeX","date":"2023-08-09T03:00:38.353Z","updated":"2023-08-07T08:49:38.000Z","comments":true,"path":"2023/08/09/2022-04-15-Win-KeX/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-04-15-Win-KeX/","excerpt":"","text":"Win-KeX内容： 概述 安装 先决条件 在WSL2中安装Kali Linux 安装Win-Kex 运行Win-KeX 可选步骤 概述Win-KeX为Linux的Windows子系统（WSL 2）提供了Kali桌面体验，具有以下功能： 窗口模式：在专用窗口中启动Kali Linux桌面 无缝模式：在Windows和Kali应用程序和菜单之间共享Windows桌面 声音支持 无特权和根会话支持 共享剪贴板，可在Kali Linux和Windows应用之间进行剪切和粘贴支持 多会话支持：根窗口和非私有窗口以及无缝会话同时进行 本页详细介绍了在2分钟内安装Win-Kex的步骤。安装所有的安装步骤，直至我们安装Win-的Kex点，也由惊人的5分钟的视频手册的说明NetworkChuck： 5分钟内Windows上的Kali Linux（WSL2 GUI） 注意：您可以跳过xrdp的安装，而是按照本指南的最后一步来安装Win-Kex。 先决条件 运行Windows 10版本2004或更高版本 使用Windows终端机 在WSL2中安装Kali Linux 以管理员身份打开PowerShell并运行： 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux 重新开始 以管理员身份打开PowerShell并运行： 12dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestartdism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart 重新开始 从此处下载并安装WSL2 Linux内核：https :&#x2F;&#x2F;aka.ms&#x2F;wsl2kernel 以管理员身份打开PowerShell并运行： wsl --set-default-version 2 从Microsoft Store安装Kali Linux 注意：要升级现有的WSL1 kali-linux安装，请输入： wsl --set-version kali-linux 2 运行Kali并完成初始设置 安装Win-KeX 通过以下方式安装win-kex： 123kali@kali:~$ sudo apt updatekali@kali:~$kali@kali:~$ sudo apt install -y kali-win-kex 运行Win-KeXWin-KeX支持三种模式： 窗口模式： 要在具有声音支持的窗口模式下启动Win-KeX，请运行 kex --win -s 有关更多信息，请参考Win-KeX Win使用文档。 增强的会话模式： 要在具有声音支持和手臂解决方法的增强会话模式下启动Win-KeX，请运行 kex --esm --ip -s 有关更多信息，请参考Win-KeX ESM使用文档。 无缝模式： 12345To start Win-KeX in Seamless mode with sound support, run`kex --sl -s`Refer to the [Win-KeX SL usage documentation](/docs/wsl/win-kex-sl/) for further information. 可选步骤： 如果有足够的空间，为什么不安装“ Kali with the lot”呢？： sudo apt install -y kali-linux-large 创建Windows终端快捷方式： 在以下选项中选择： 窗口模式下基本的Win-KeX带有声音： 123456&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --wtstart -s&quot;,&#125;, 带有声音的窗口模式下的高级Win-KeX-Kali图标，并在kali主目录中启动： 将kali-menu.jpg图标复制到Windows图片目录，然后将图标和开始目录添加到WT配置： 12345678&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;icon&quot;: &quot;file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.jpg&quot;, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --wtstart -s&quot;, &quot;startingDirectory&quot; : &quot;//wsl$/kali-linux/home/&lt;kali user&gt;&quot;&#125;, 基本Win-KeX在无缝模式下发出声音： 123456&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --sl --wtstart -s&quot;,&#125;, 带有声音的无缝模式下的高级Win-KeX-Kali图标，并在kali主目录中启动： 将kali-menu.jpg图标复制到Windows图片目录，然后将图标和开始目录添加到WT配置： 12345678&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;icon&quot;: &quot;file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.jpg&quot;, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --sl --wtstart -s&quot;, &quot;startingDirectory&quot; : &quot;//wsl$/kali-linux/home/&lt;kali user&gt;&quot;&#125;, 在ESM模式下具有声音的基本Win-KeX： 123456&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ecedc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --esm --wtstart -s&quot;,&#125;, 在带声音的ESM模式下的高级Win-KeX-Kali图标，并在kali主目录中启动： 将kali-menu.jpg图标复制到Windows图片目录，然后将图标和开始目录添加到WT配置： 12345678&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ecedd031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;icon&quot;: &quot;file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.jpg&quot;, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --esm --wtstart -s&quot;, &quot;startingDirectory&quot; : &quot;//wsl$/kali-linux/home/&lt;kali user&gt;&quot;&#125;, 帮助有关更多信息，请通过以下途径寻求帮助： 1kex --help 或通过以下方式查阅手册页： 1man kex 或加入我们的卡利论坛 享受Win-KeX！Win-KeXContent: Overview Installation Prerequisites Install Kali Linux in WSL2 Install Win-Kex Run Win-KeX Optional steps OverviewWin-KeX provides a Kali Desktop Experience for Windows Subsystem for Linux (WSL 2) with the following features: Window mode: start a Kali Linux desktop in a dedicated window Seamless mode: share the Windows desktop between Windows and Kali apps and menus Sound support Unprivileged and Root session support Shared clipboard for cut and paste support between Kali Linux and Windows apps Multi-session support: root window &amp; non-priv window &amp; seamless sessions concurrently This page details the steps to install Win-Kex in under 2 minutes.InstallationAll installation steps, up to the point where we install Win-Kex, are also explained in the 5 minute video guide by the amazing NetworkChuck: Kali Linux on Windows in 5min (WSL2 GUI) Note: You can skip the installation of xrdp and follow the last step of this guide to install Win-Kex instead. Prerequisites Running Windows 10 version 2004 or higher Using Windows Terminal Install Kali Linux in WSL2 Open PowerShell as administrator and run: 1Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Restart Open PowerShell as administrator and run: 12dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestartdism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart Restart Download and install the WSL2 Linux Kernel from here: https://aka.ms/wsl2kernel Open PowerShell as administrator and run: wsl --set-default-version 2 Install Kali Linux from the Microsoft Store Note: to upgrade an existing WSL1 kali-linux installation, type: wsl --set-version kali-linux 2 Run Kali and finish the initial setup Install Win-KeX Install win-kex via: 123kali@kali:~$ sudo apt updatekali@kali:~$kali@kali:~$ sudo apt install -y kali-win-kex Run Win-KeXWin-KeX supports three modes: Window Mode: To start Win-KeX in Window mode with sound support, run kex --win -s Refer to the Win-KeX Win usage documentation for further information. Enhanced Session Mode: To start Win-KeX in Enhanced Session Mode with sound support and arm workaround, run kex --esm --ip -s Refer to the Win-KeX ESM usage documentation for further information. Seamless mode: 12345To start Win-KeX in Seamless mode with sound support, run`kex --sl -s`Refer to the [Win-KeX SL usage documentation](/docs/wsl/win-kex-sl/) for further information. Optional Steps: If you have the space, why not install “Kali with the lot”?: sudo apt install -y kali-linux-large Create a Windows Terminal Shortcut: Choose amongst these options: Basic Win-KeX in window mode with sound: 123456&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --wtstart -s&quot;,&#125;, Advanced Win-KeX in window mode with sound - Kali icon and start in kali home directory: Copy the kali-menu.jpg icon across to your windows picture directory and add the icon and start directory to your WT config: 12345678&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;icon&quot;: &quot;file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.jpg&quot;, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --wtstart -s&quot;, &quot;startingDirectory&quot; : &quot;//wsl$/kali-linux/home/&lt;kali user&gt;&quot;&#125;, Basic Win-KeX in seamless mode with sound: 123456&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --sl --wtstart -s&quot;,&#125;, Advanced Win-KeX in seamless mode with sound - Kali icon and start in kali home directory: Copy the kali-menu.jpg icon across to your windows picture directory and add the icon and start directory to your WT config: 12345678&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ececc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;icon&quot;: &quot;file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.jpg&quot;, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --sl --wtstart -s&quot;, &quot;startingDirectory&quot; : &quot;//wsl$/kali-linux/home/&lt;kali user&gt;&quot;&#125;, Basic Win-KeX in ESM mode with sound: 123456&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ecedc031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --esm --wtstart -s&quot;,&#125;, Advanced Win-KeX in ESM mode with sound - Kali icon and start in kali home directory: Copy the kali-menu.jpg icon across to your windows picture directory and add the icon and start directory to your WT config: 12345678&#123; &quot;guid&quot;: &quot;&#123;55ca431a-3a87-5fb3-83cd-11ecedd031d2&#125;&quot;, &quot;hidden&quot;: false, &quot;icon&quot;: &quot;file:///c:/users/&lt;windows user&gt;/pictures/icons/kali-menu.jpg&quot;, &quot;name&quot;: &quot;Win-KeX&quot;, &quot;commandline&quot;: &quot;wsl -d kali-linux kex --esm --wtstart -s&quot;, &quot;startingDirectory&quot; : &quot;//wsl$/kali-linux/home/&lt;kali user&gt;&quot;&#125;, HelpFor more information, ask for help via: 1kex --help or consult the manpage via: 1man kex or join us in the Kali Forums Enjoy Win-KeX!","categories":[],"tags":[]},{"title":"","slug":"2022-04-15-MarkdownGraph","date":"2023-08-09T03:00:38.320Z","updated":"2023-08-07T08:49:38.000Z","comments":true,"path":"2023/08/09/2022-04-15-MarkdownGraph/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-04-15-MarkdownGraph/","excerpt":"","text":"1、横向流程图源码格式： 12345678​```mermaidgraph LRA[方形] --&gt;B(圆角) B --&gt; C&#123;条件a&#125; C --&gt;|a=1| D[结果1] C --&gt;|a=2| E[结果2] F[横向流程图]​``` 1234567graph LRA[方形] --&gt;B(圆角) B --&gt; C&#123;条件a&#125; C --&gt;|a=1| D[结果1] C --&gt;|a=2| E[结果2] F[横向流程图] 2、竖向流程图源码格式： 12345678​```mermaidgraph TDA[方形] --&gt; B(圆角) B --&gt; C&#123;条件a&#125; C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[竖向流程图]​``` 123456graph TDA[方形] --&gt; B(圆角) B --&gt; C&#123;条件a&#125; C --&gt; |a=1| D[结果1] C --&gt; |a=2| E[结果2] F[竖向流程图] 3、标准流程图源码格式： 1234567891011​```flowst=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st-&gt;op-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op​``` 123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st-&gt;op-&gt;condcond(yes)-&gt;io-&gt;econd(no)-&gt;sub1(right)-&gt;op 4、标准流程图源码格式（横向）： 1234567891011​```flowst=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st(right)-&gt;op(right)-&gt;condcond(yes)-&gt;io(bottom)-&gt;econd(no)-&gt;sub1(right)-&gt;op​``` 123456789st=&gt;start: 开始框op=&gt;operation: 处理框cond=&gt;condition: 判断框(是或否?)sub1=&gt;subroutine: 子流程io=&gt;inputoutput: 输入输出框e=&gt;end: 结束框st(right)-&gt;op(right)-&gt;condcond(yes)-&gt;io(bottom)-&gt;econd(no)-&gt;sub1(right)-&gt;op 5、UML时序图源码样例： 1234567​```sequence对象A-&gt;对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B--&gt;对象A: 我很好(响应)对象A-&gt;对象B: 你真的好吗？​``` 12345对象A-&gt;对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B--&gt;对象A: 我很好(响应)对象A-&gt;对象B: 你真的好吗？ 6、UML时序图源码复杂样例： 12345678910111213​```sequenceTitle: 标题：复杂使用对象A-&gt;对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B--&gt;对象A: 我很好(响应)对象B-&gt;小三: 你好吗小三--&gt;&gt;对象A: 对象B找我了对象A-&gt;对象B: 你真的好吗？Note over 小三,对象B: 我们是朋友participant CNote right of C: 没人陪我玩​``` 1234567891011Title: 标题：复杂使用对象A-&gt;对象B: 对象B你好吗?（请求）Note right of 对象B: 对象B的描述Note left of 对象A: 对象A的描述(提示)对象B--&gt;对象A: 我很好(响应)对象B-&gt;小三: 你好吗小三--&gt;&gt;对象A: 对象B找我了对象A-&gt;对象B: 你真的好吗？Note over 小三,对象B: 我们是朋友participant CNote right of C: 没人陪我玩 7、UML标准时序图样例： 1234567891011121314​```mermaid%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头 sequenceDiagram participant 张三 participant 李四 张三-&gt;王五: 王五你好吗？ loop 健康检查 王五-&gt;王五: 与疾病战斗 end Note right of 王五: 合理 食物 &lt;br/&gt;看医生... 李四--&gt;&gt;张三: 很好! 王五-&gt;李四: 你怎么样? 李四--&gt;王五: 很好!​``` 123456789101112%% 时序图例子,-&gt; 直线，--&gt;虚线，-&gt;&gt;实线箭头 sequenceDiagram participant 张三 participant 李四 张三-&gt;王五: 王五你好吗？ loop 健康检查 王五-&gt;王五: 与疾病战斗 end Note right of 王五: 合理 食物 &lt;br/&gt;看医生... 李四--&gt;&gt;张三: 很好! 王五-&gt;李四: 你怎么样? 李四--&gt;王五: 很好! 8、甘特图样例： 123456789101112131415161718192021​```mermaid%% 语法示例 gantt dateFormat YYYY-MM-DD title 软件开发甘特图 section 设计 需求 :done, des1, 2014-01-06,2014-01-08 原型 :active, des2, 2014-01-09, 3d UI设计 : des3, after des2, 5d 未来任务 : des4, after des3, 5d section 开发 学习准备理解需求 :crit, done, 2014-01-06,24h 设计框架 :crit, done, after des2, 2d 开发 :crit, active, 3d 未来任务 :crit, 5d 耍 :2d section 测试 功能测试 :active, a1, after des3, 3d 压力测试 :after a1 , 20h 测试报告 : 48h​``` 12345678910111213141516171819%% 语法示例 gantt dateFormat YYYY-MM-DD title 软件开发甘特图 section 设计 需求 :done, des1, 2014-01-06,2014-01-08 原型 :active, des2, 2014-01-09, 3d UI设计 : des3, after des2, 5d 未来任务 : des4, after des3, 5d section 开发 学习准备理解需求 :crit, done, 2014-01-06,24h 设计框架 :crit, done, after des2, 2d 开发 :crit, active, 3d 未来任务 :crit, 5d 耍 :2d section 测试 功能测试 :active, a1, after des3, 3d 压力测试 :after a1 , 20h 测试报告 : 48h","categories":[],"tags":[]},{"title":"","slug":"2022-04-15-c变长参数列表","date":"2023-08-09T03:00:38.288Z","updated":"2023-08-07T08:49:39.000Z","comments":true,"path":"2023/08/09/2022-04-15-c变长参数列表/","link":"","permalink":"http://ebxeax.github.io/2023/08/09/2022-04-15-c%E5%8F%98%E9%95%BF%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8/","excerpt":"","text":"1.使用模板中的变长参数函数声明 12345678910111213141516171819202122232425262728#include &lt;iostreamusing namespace std;/*变长参数函数模板声明*/template &lt;typename... Tvoid print(T... val);/*边界条件*/void print(void)&#123;cout&lt;&lt;&quot;here end&quot;&lt;&lt;endl;&#125;/*递归的特例化定义*/template &lt;typename T1, typename... T2void print(T1 start, T2... var)&#123;cout&lt;&lt;&quot;sizeof ... now is: &quot;&lt;&lt;sizeof... (var)&lt;&lt;endl;cout&lt;&lt;start&lt;&lt;endl;print(var...);&#125;int main(void)&#123;print(1,2,3,4);return 0;&#125; 其中的声明其实是没什么用的，只是告诉使用者可以按照这样的格式使用，如果不做这个声明，只保留”边界条件”和”递归的特例化定义”，这样虽然可行，但是未免会造成困惑 执行结果如下： 实际上，这个”变长”付出的代价还是很大的，要递归的实例出n个函数，最终再调用边界条件的函数。过程如下 2.使用va_list()函数实现变长参数列表 以一个矩阵自动识别维度的程序为例 arrayMat.h 1234567891011121314151617181920212223#include&lt;iostream#include&lt;string#include&lt;stdarg.husing namespace std;typedef int dtype;class mat&#123;public: mat(); ~mat(); void set_dim(int dim); void mat::set_mat_shape(int i,...); int get_dim(); int* get_mat_shape(); void print_shape(); dtype* make_mat();private: int dim; int *shape; dtype *enterMat;&#125;; arrayMat.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include&quot;arrayMat.h&quot;mat::mat()&#123;&#125;mat::~mat()&#123;&#125;int mat::get_dim() &#123; return this-dim;&#125;int * mat::get_mat_shape() &#123; return this-shape;&#125;void mat::print_shape()&#123; for (int a = 0; a &lt; this-dim; a++) &#123; std::cout &lt;&lt; shape[a] &lt;&lt; &quot; &quot; ; &#125;&#125;void mat::set_dim(int i) &#123; this-dim = i;&#125;void mat::set_mat_shape(int i, ...) &#123; va_list _var_list; va_start(_var_list, i); int count = 0; int *temp=new int[100]; while (i != -1) &#123; //cout &lt;&lt; i &lt;&lt;&quot; &quot;; temp[count] = i; count++; i = va_arg(_var_list, int); &#125; va_end(_var_list); this-set_dim(count); this-shape = temp; //std::cout &lt;&lt; std::endl; //this-shape = new int [count]; //for (int j = 0; j &lt; count; j++) //shape[j] = temp[j];&#125;//Mat2D A[i][j] = B[i + j * rows] main.cpp 12345678910111213141516171819#include&quot;arrayMat.h&quot;int main() &#123; mat m1,m2; m1.set_mat_shape(1,3,128,128,-1); int *shape = m1.get_mat_shape(); int dim = m1.get_dim(); cout &lt;&lt; &quot;dim: &quot; &lt;&lt; dim&lt;&lt;endl; for (int i = 0; i &lt; dim; i++) cout &lt;&lt;*(shape+i) &lt;&lt;&quot; &quot;; m1.print_shape(); //m1.make_mat(); //m2.set_mat_shape(3,3); //m2.make_mat(); //m2.print_mat(); return 0;&#125; 运行结果：","categories":[],"tags":[]},{"title":"write a C interpreter","slug":"CP_001_introduce-Interpreter","date":"2023-02-13T23:54:44.000Z","updated":"2023-08-07T08:49:41.000Z","comments":true,"path":"2023/02/14/CP_001_introduce-Interpreter/","link":"","permalink":"http://ebxeax.github.io/2023/02/14/CP_001_introduce-Interpreter/","excerpt":"","text":"一般编译器编写分为三个步骤： 1.词法分析器 2.语法分析器 3.目标代码生成 构建编译器四个函数： 1.next() 词法分析，获取下一个标记，自动忽略空白字符 2.program() 语法分析的入口，分析整个程序 3.expression(level) 解析一个表达式 4.eval() 虚拟机入口，解释目标代码 计算机内部工作原理三个部件：CPU、寄存器、内存代码（汇编指令）以二进制的形式保存在内存中，CPU从中一条一条加载指令执行，程序运行状态保存于寄存器 1.代码段（text）用于存放代码（指令） 2.数据段（data）用于存放初始化了的数据，如int i &#x3D; 10;，就需要存放到数据段中 3.未初始化数据段（bss）用于存放未初始化的数据，如 int i[1000];，因为不关心其中的真正数值，所以单独存放可以节省空间，减少程序的体积 4.栈（stack）用于处理函数调用相关的数据，如调用帧（calling frame）或是函数的局部变量等 5.堆（heap）用于为程序动态分配内存 12345678910111213141516+------------------+| stack | | high address| ... v || || || || || ... ^ || heap | |+------------------+| bss segment |+------------------+| data segment |+------------------+| text segment | low address+------------------+","categories":[{"name":"interpreter C","slug":"interpreter-C","permalink":"http://ebxeax.github.io/categories/interpreter-C/"}],"tags":[]}],"categories":[{"name":"interpreter C","slug":"interpreter-C","permalink":"http://ebxeax.github.io/categories/interpreter-C/"}],"tags":[]}